{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Speech Command Recognition with torchaudio\n",
    "******************************************\n",
    "\n",
    "This tutorial will show you how to correctly format an audio dataset and\n",
    "then train/test an audio classifier network on the dataset.\n",
    "\n",
    "Colab has GPU option available. In the menu tabs, select “Runtime” then\n",
    "“Change runtime type”. In the pop-up that follows, you can choose GPU.\n",
    "After the change, your runtime should automatically restart (which means\n",
    "information from executed cells disappear).\n",
    "\n",
    "First, let’s import the common torch packages such as\n",
    "`torchaudio <https://github.com/pytorch/audio>`__ that can be installed\n",
    "by following the instructions on the website.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run in Google Colab\n",
    "\n",
    "# CPU:\n",
    "# !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# GPU:\n",
    "# !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# For interactive demo at the end:\n",
    "# !pip install pydub\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check if a CUDA GPU is available and select our device. Running\n",
    "the network on a GPU will greatly decrease the training/testing runtime.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Dataset\n",
    "---------------------\n",
    "\n",
    "We use torchaudio to download and represent the dataset. Here we use\n",
    "`SpeechCommands <https://arxiv.org/abs/1804.03209>`__, which is a\n",
    "datasets of 35 commands spoken by different people. The dataset\n",
    "``SPEECHCOMMANDS`` is a ``torch.utils.data.Dataset`` version of the\n",
    "dataset. In this dataset, all audio files are about 1 second long (and\n",
    "so about 16000 time frames long).\n",
    "\n",
    "The actual loading and formatting steps happen when a data point is\n",
    "being accessed, and torchaudio takes care of converting the audio files\n",
    "to tensors. If one wants to load an audio file directly instead,\n",
    "``torchaudio.load()`` can be used. It returns a tuple containing the\n",
    "newly created tensor along with the sampling frequency of the audio file\n",
    "(16kHz for SpeechCommands).\n",
    "\n",
    "Going back to the dataset, here we create a subclass that splits it into\n",
    "standard training, validation, testing subsets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import os\n",
    "\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.join(self._path, line.strip()) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "\n",
    "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
    "train_set = SubsetSC(\"training\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform\n",
    "(the audio signal), the sample rate, the utterance (label), the ID of\n",
    "the speaker, the number of the utterance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of waveform: torch.Size([1, 16000])\n",
      "Sample rate of waveform: 16000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s find the list of labels available in the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backward',\n",
       " 'bed',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'down',\n",
       " 'eight',\n",
       " 'five',\n",
       " 'follow',\n",
       " 'forward',\n",
       " 'four',\n",
       " 'go',\n",
       " 'happy',\n",
       " 'house',\n",
       " 'learn',\n",
       " 'left',\n",
       " 'marvin',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'off',\n",
       " 'on',\n",
       " 'one',\n",
       " 'right',\n",
       " 'seven',\n",
       " 'sheila',\n",
       " 'six',\n",
       " 'stop',\n",
       " 'three',\n",
       " 'tree',\n",
       " 'two',\n",
       " 'up',\n",
       " 'visual',\n",
       " 'wow',\n",
       " 'yes',\n",
       " 'zero']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 35 audio labels are commands that are said by users. The first few\n",
    "files are people saying “marvin”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_first, *_ = train_set[0]\n",
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)\n",
    "\n",
    "waveform_second, *_ = train_set[1]\n",
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last file is someone saying “visual”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_last, *_ = train_set[-1]\n",
    "ipd.Audio(waveform_last.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the Data\n",
    "-------------------\n",
    "\n",
    "This is a good place to apply transformations to the data. For the\n",
    "waveform, we downsample the audio for faster processing without losing\n",
    "too much of the classification power.\n",
    "\n",
    "We don’t need to apply other transformations here. It is common for some\n",
    "datasets though to have to reduce the number of channels (say from\n",
    "stereo to mono) by either taking the mean along the channel dimension,\n",
    "or simply keeping only one of the channels. Since SpeechCommands uses a\n",
    "single channel for audio, this is not needed here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sample_rate = 8000\n",
    "transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "transformed = transform(waveform)\n",
    "\n",
    "ipd.Audio(transformed.numpy(), rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are encoding each word using its index in the list of labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes --> tensor(33) --> yes\n"
     ]
    }
   ],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]\n",
    "\n",
    "\n",
    "word_start = \"yes\"\n",
    "index = label_to_index(word_start)\n",
    "word_recovered = index_to_label(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn a list of data point made of audio recordings and utterances\n",
    "into two batched tensors for the model, we implement a collate function\n",
    "which is used by the PyTorch DataLoader that allows us to iterate over a\n",
    "dataset by batches. Please see `the\n",
    "documentation <https://pytorch.org/docs/stable/data.html#working-with-collate-fn>`__\n",
    "for more information about working with a collate function.\n",
    "\n",
    "In the collate function, we also apply the resampling, and the text\n",
    "encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Network\n",
    "------------------\n",
    "\n",
    "For this tutorial we will use a convolutional neural network to process\n",
    "the raw audio data. Usually more advanced transforms are applied to the\n",
    "audio data, however CNNs can be used to accurately process the raw data.\n",
    "The specific architecture is modeled after the M5 network architecture\n",
    "described in `this paper <https://arxiv.org/pdf/1610.00087.pdf>`__. An\n",
    "important aspect of models processing raw audio data is the receptive\n",
    "field of their first layer’s filters. Our model’s first filter is length\n",
    "80 so when processing audio sampled at 8kHz the receptive field is\n",
    "around 10ms (and at 4kHz, around 20 ms). This size is similar to speech\n",
    "processing applications that often use receptive fields ranging from\n",
    "20ms to 40ms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=35, bias=True)\n",
      ")\n",
      "Number of parameters: 26915\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "\n",
    "model = M5(n_input=transformed.shape[0], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same optimization technique used in the paper, an Adam\n",
    "optimizer with weight decay set to 0.0001. At first, we will train with\n",
    "a learning rate of 0.01, but we will use a ``scheduler`` to decrease it\n",
    "to 0.001 during training after 20 epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing the Network\n",
    "--------------------------------\n",
    "\n",
    "Now let’s define a training function that will feed our training data\n",
    "into the model and perform the backward pass and optimization steps. For\n",
    "training, the loss we will use is the negative log-likelihood. The\n",
    "network will then be tested after each epoch to see how the accuracy\n",
    "varies during the training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a training function, we need to make one for testing\n",
    "the networks accuracy. We will set the model to ``eval()`` mode and then\n",
    "run inference on the test dataset. Calling ``eval()`` sets the training\n",
    "variable in all modules in the network to false. Certain layers like\n",
    "batch normalization and dropout layers behave differently during\n",
    "training so this step is crucial for getting correct results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train and test the network. We will train the network\n",
    "for ten epochs then reduce the learn rate and train for ten more epochs.\n",
    "The network will be tested after each epoch to see how the accuracy\n",
    "varies during the training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6de458a20447efa28389420e089420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmc-2018012484/.local/lib/python3.6/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/105829 (0%)]\tLoss: 3.959628\n",
      "Train Epoch: 1 [5120/105829 (5%)]\tLoss: 3.104718\n",
      "Train Epoch: 1 [10240/105829 (10%)]\tLoss: 2.590552\n",
      "Train Epoch: 1 [15360/105829 (14%)]\tLoss: 2.277224\n",
      "Train Epoch: 1 [20480/105829 (19%)]\tLoss: 2.185055\n",
      "Train Epoch: 1 [25600/105829 (24%)]\tLoss: 1.850440\n",
      "Train Epoch: 1 [30720/105829 (29%)]\tLoss: 1.831597\n",
      "Train Epoch: 1 [35840/105829 (34%)]\tLoss: 1.695171\n",
      "Train Epoch: 1 [40960/105829 (39%)]\tLoss: 1.605647\n",
      "Train Epoch: 1 [46080/105829 (43%)]\tLoss: 1.610963\n",
      "Train Epoch: 1 [51200/105829 (48%)]\tLoss: 1.449640\n",
      "Train Epoch: 1 [56320/105829 (53%)]\tLoss: 1.278144\n",
      "Train Epoch: 1 [61440/105829 (58%)]\tLoss: 1.237276\n",
      "Train Epoch: 1 [66560/105829 (63%)]\tLoss: 1.354321\n",
      "Train Epoch: 1 [71680/105829 (68%)]\tLoss: 1.325064\n",
      "Train Epoch: 1 [76800/105829 (72%)]\tLoss: 1.234872\n",
      "Train Epoch: 1 [81920/105829 (77%)]\tLoss: 1.015675\n",
      "Train Epoch: 1 [87040/105829 (82%)]\tLoss: 1.112980\n",
      "Train Epoch: 1 [92160/105829 (87%)]\tLoss: 1.157077\n",
      "Train Epoch: 1 [97280/105829 (92%)]\tLoss: 0.958736\n",
      "Train Epoch: 1 [102400/105829 (97%)]\tLoss: 1.048275\n",
      "\n",
      "Test Epoch: 1\tAccuracy: 7115/11005 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/105829 (0%)]\tLoss: 1.041451\n",
      "Train Epoch: 2 [5120/105829 (5%)]\tLoss: 1.090026\n",
      "Train Epoch: 2 [10240/105829 (10%)]\tLoss: 0.983187\n",
      "Train Epoch: 2 [15360/105829 (14%)]\tLoss: 0.977199\n",
      "Train Epoch: 2 [20480/105829 (19%)]\tLoss: 0.974971\n",
      "Train Epoch: 2 [25600/105829 (24%)]\tLoss: 0.964699\n",
      "Train Epoch: 2 [30720/105829 (29%)]\tLoss: 0.842526\n",
      "Train Epoch: 2 [35840/105829 (34%)]\tLoss: 1.004049\n",
      "Train Epoch: 2 [40960/105829 (39%)]\tLoss: 0.904766\n",
      "Train Epoch: 2 [46080/105829 (43%)]\tLoss: 0.662331\n",
      "Train Epoch: 2 [51200/105829 (48%)]\tLoss: 0.773765\n",
      "Train Epoch: 2 [56320/105829 (53%)]\tLoss: 0.902689\n",
      "Train Epoch: 2 [61440/105829 (58%)]\tLoss: 0.848206\n",
      "Train Epoch: 2 [66560/105829 (63%)]\tLoss: 0.891496\n",
      "Train Epoch: 2 [71680/105829 (68%)]\tLoss: 0.809249\n",
      "Train Epoch: 2 [76800/105829 (72%)]\tLoss: 0.948541\n",
      "Train Epoch: 2 [81920/105829 (77%)]\tLoss: 0.928394\n",
      "Train Epoch: 2 [87040/105829 (82%)]\tLoss: 0.873253\n",
      "Train Epoch: 2 [92160/105829 (87%)]\tLoss: 0.836093\n",
      "Train Epoch: 2 [97280/105829 (92%)]\tLoss: 0.880521\n",
      "Train Epoch: 2 [102400/105829 (97%)]\tLoss: 0.856021\n",
      "\n",
      "Test Epoch: 2\tAccuracy: 7753/11005 (70%)\n",
      "\n",
      "Train Epoch: 3 [0/105829 (0%)]\tLoss: 0.863117\n",
      "Train Epoch: 3 [5120/105829 (5%)]\tLoss: 0.639546\n",
      "Train Epoch: 3 [10240/105829 (10%)]\tLoss: 0.633139\n",
      "Train Epoch: 3 [15360/105829 (14%)]\tLoss: 0.755930\n",
      "Train Epoch: 3 [20480/105829 (19%)]\tLoss: 0.876563\n",
      "Train Epoch: 3 [25600/105829 (24%)]\tLoss: 0.815172\n",
      "Train Epoch: 3 [30720/105829 (29%)]\tLoss: 0.810585\n",
      "Train Epoch: 3 [35840/105829 (34%)]\tLoss: 0.711375\n",
      "Train Epoch: 3 [40960/105829 (39%)]\tLoss: 0.700229\n",
      "Train Epoch: 3 [46080/105829 (43%)]\tLoss: 0.797479\n",
      "Train Epoch: 3 [51200/105829 (48%)]\tLoss: 0.692995\n",
      "Train Epoch: 3 [56320/105829 (53%)]\tLoss: 0.945543\n",
      "Train Epoch: 3 [61440/105829 (58%)]\tLoss: 0.628051\n",
      "Train Epoch: 3 [66560/105829 (63%)]\tLoss: 0.816190\n",
      "Train Epoch: 3 [71680/105829 (68%)]\tLoss: 0.778636\n",
      "Train Epoch: 3 [76800/105829 (72%)]\tLoss: 0.848735\n",
      "Train Epoch: 3 [81920/105829 (77%)]\tLoss: 0.778960\n",
      "Train Epoch: 3 [87040/105829 (82%)]\tLoss: 0.817623\n",
      "Train Epoch: 3 [92160/105829 (87%)]\tLoss: 0.765531\n",
      "Train Epoch: 3 [97280/105829 (92%)]\tLoss: 0.801274\n",
      "Train Epoch: 3 [102400/105829 (97%)]\tLoss: 0.846847\n",
      "\n",
      "Test Epoch: 3\tAccuracy: 8530/11005 (78%)\n",
      "\n",
      "Train Epoch: 4 [0/105829 (0%)]\tLoss: 0.511683\n",
      "Train Epoch: 4 [5120/105829 (5%)]\tLoss: 0.513606\n",
      "Train Epoch: 4 [10240/105829 (10%)]\tLoss: 0.581295\n",
      "Train Epoch: 4 [15360/105829 (14%)]\tLoss: 0.783566\n",
      "Train Epoch: 4 [20480/105829 (19%)]\tLoss: 0.868003\n",
      "Train Epoch: 4 [25600/105829 (24%)]\tLoss: 0.658433\n",
      "Train Epoch: 4 [30720/105829 (29%)]\tLoss: 0.812670\n",
      "Train Epoch: 4 [35840/105829 (34%)]\tLoss: 0.674390\n",
      "Train Epoch: 4 [40960/105829 (39%)]\tLoss: 0.712394\n",
      "Train Epoch: 4 [46080/105829 (43%)]\tLoss: 0.698862\n",
      "Train Epoch: 4 [51200/105829 (48%)]\tLoss: 0.839649\n",
      "Train Epoch: 4 [56320/105829 (53%)]\tLoss: 0.620165\n",
      "Train Epoch: 4 [61440/105829 (58%)]\tLoss: 0.675553\n",
      "Train Epoch: 4 [66560/105829 (63%)]\tLoss: 0.664775\n",
      "Train Epoch: 4 [71680/105829 (68%)]\tLoss: 0.638727\n",
      "Train Epoch: 4 [76800/105829 (72%)]\tLoss: 0.666297\n",
      "Train Epoch: 4 [81920/105829 (77%)]\tLoss: 0.690895\n",
      "Train Epoch: 4 [87040/105829 (82%)]\tLoss: 0.646185\n",
      "Train Epoch: 4 [92160/105829 (87%)]\tLoss: 0.715422\n",
      "Train Epoch: 4 [97280/105829 (92%)]\tLoss: 0.781542\n",
      "Train Epoch: 4 [102400/105829 (97%)]\tLoss: 0.785376\n",
      "\n",
      "Test Epoch: 4\tAccuracy: 8197/11005 (74%)\n",
      "\n",
      "Train Epoch: 5 [0/105829 (0%)]\tLoss: 0.596735\n",
      "Train Epoch: 5 [5120/105829 (5%)]\tLoss: 0.617992\n",
      "Train Epoch: 5 [10240/105829 (10%)]\tLoss: 0.855998\n",
      "Train Epoch: 5 [15360/105829 (14%)]\tLoss: 0.734982\n",
      "Train Epoch: 5 [20480/105829 (19%)]\tLoss: 0.553596\n",
      "Train Epoch: 5 [25600/105829 (24%)]\tLoss: 0.648506\n",
      "Train Epoch: 5 [30720/105829 (29%)]\tLoss: 0.733157\n",
      "Train Epoch: 5 [35840/105829 (34%)]\tLoss: 0.646489\n",
      "Train Epoch: 5 [40960/105829 (39%)]\tLoss: 0.666233\n",
      "Train Epoch: 5 [46080/105829 (43%)]\tLoss: 0.854899\n",
      "Train Epoch: 5 [51200/105829 (48%)]\tLoss: 0.784295\n",
      "Train Epoch: 5 [56320/105829 (53%)]\tLoss: 0.718167\n",
      "Train Epoch: 5 [61440/105829 (58%)]\tLoss: 0.688917\n",
      "Train Epoch: 5 [66560/105829 (63%)]\tLoss: 0.702296\n",
      "Train Epoch: 5 [71680/105829 (68%)]\tLoss: 0.662459\n",
      "Train Epoch: 5 [76800/105829 (72%)]\tLoss: 0.572770\n",
      "Train Epoch: 5 [81920/105829 (77%)]\tLoss: 0.516896\n",
      "Train Epoch: 5 [87040/105829 (82%)]\tLoss: 0.572365\n",
      "Train Epoch: 5 [92160/105829 (87%)]\tLoss: 0.610178\n",
      "Train Epoch: 5 [97280/105829 (92%)]\tLoss: 0.727332\n",
      "Train Epoch: 5 [102400/105829 (97%)]\tLoss: 0.681578\n",
      "\n",
      "Test Epoch: 5\tAccuracy: 8798/11005 (80%)\n",
      "\n",
      "Train Epoch: 6 [0/105829 (0%)]\tLoss: 0.597511\n",
      "Train Epoch: 6 [5120/105829 (5%)]\tLoss: 0.710220\n",
      "Train Epoch: 6 [10240/105829 (10%)]\tLoss: 0.540591\n",
      "Train Epoch: 6 [15360/105829 (14%)]\tLoss: 0.653790\n",
      "Train Epoch: 6 [20480/105829 (19%)]\tLoss: 0.644558\n",
      "Train Epoch: 6 [25600/105829 (24%)]\tLoss: 0.760167\n",
      "Train Epoch: 6 [30720/105829 (29%)]\tLoss: 0.741567\n",
      "Train Epoch: 6 [35840/105829 (34%)]\tLoss: 0.552711\n",
      "Train Epoch: 6 [40960/105829 (39%)]\tLoss: 0.573794\n",
      "Train Epoch: 6 [46080/105829 (43%)]\tLoss: 0.552996\n",
      "Train Epoch: 6 [51200/105829 (48%)]\tLoss: 0.604902\n",
      "Train Epoch: 6 [56320/105829 (53%)]\tLoss: 0.613219\n",
      "Train Epoch: 6 [61440/105829 (58%)]\tLoss: 0.633328\n",
      "Train Epoch: 6 [66560/105829 (63%)]\tLoss: 0.672159\n",
      "Train Epoch: 6 [71680/105829 (68%)]\tLoss: 0.658412\n",
      "Train Epoch: 6 [76800/105829 (72%)]\tLoss: 0.636977\n",
      "Train Epoch: 6 [81920/105829 (77%)]\tLoss: 0.804489\n",
      "Train Epoch: 6 [87040/105829 (82%)]\tLoss: 0.562122\n",
      "Train Epoch: 6 [92160/105829 (87%)]\tLoss: 0.678267\n",
      "Train Epoch: 6 [97280/105829 (92%)]\tLoss: 0.624318\n",
      "Train Epoch: 6 [102400/105829 (97%)]\tLoss: 0.776488\n",
      "\n",
      "Test Epoch: 6\tAccuracy: 8796/11005 (80%)\n",
      "\n",
      "Train Epoch: 7 [0/105829 (0%)]\tLoss: 0.503244\n",
      "Train Epoch: 7 [5120/105829 (5%)]\tLoss: 0.665518\n",
      "Train Epoch: 7 [10240/105829 (10%)]\tLoss: 0.573275\n",
      "Train Epoch: 7 [15360/105829 (14%)]\tLoss: 0.562267\n",
      "Train Epoch: 7 [20480/105829 (19%)]\tLoss: 0.732482\n",
      "Train Epoch: 7 [25600/105829 (24%)]\tLoss: 0.588145\n",
      "Train Epoch: 7 [30720/105829 (29%)]\tLoss: 0.655937\n",
      "Train Epoch: 7 [35840/105829 (34%)]\tLoss: 0.621080\n",
      "Train Epoch: 7 [40960/105829 (39%)]\tLoss: 0.566707\n",
      "Train Epoch: 7 [46080/105829 (43%)]\tLoss: 0.673801\n",
      "Train Epoch: 7 [51200/105829 (48%)]\tLoss: 0.667723\n",
      "Train Epoch: 7 [56320/105829 (53%)]\tLoss: 0.660684\n",
      "Train Epoch: 7 [61440/105829 (58%)]\tLoss: 0.655154\n",
      "Train Epoch: 7 [66560/105829 (63%)]\tLoss: 0.531302\n",
      "Train Epoch: 7 [71680/105829 (68%)]\tLoss: 0.563820\n",
      "Train Epoch: 7 [76800/105829 (72%)]\tLoss: 0.525850\n",
      "Train Epoch: 7 [81920/105829 (77%)]\tLoss: 0.558985\n",
      "Train Epoch: 7 [87040/105829 (82%)]\tLoss: 0.632445\n",
      "Train Epoch: 7 [92160/105829 (87%)]\tLoss: 0.580714\n",
      "Train Epoch: 7 [97280/105829 (92%)]\tLoss: 0.675236\n",
      "Train Epoch: 7 [102400/105829 (97%)]\tLoss: 0.541975\n",
      "\n",
      "Test Epoch: 7\tAccuracy: 9098/11005 (83%)\n",
      "\n",
      "Train Epoch: 8 [0/105829 (0%)]\tLoss: 0.510862\n",
      "Train Epoch: 8 [5120/105829 (5%)]\tLoss: 0.609729\n",
      "Train Epoch: 8 [10240/105829 (10%)]\tLoss: 0.747985\n",
      "Train Epoch: 8 [15360/105829 (14%)]\tLoss: 0.568948\n",
      "Train Epoch: 8 [20480/105829 (19%)]\tLoss: 0.620735\n",
      "Train Epoch: 8 [25600/105829 (24%)]\tLoss: 0.562972\n",
      "Train Epoch: 8 [30720/105829 (29%)]\tLoss: 0.558913\n",
      "Train Epoch: 8 [35840/105829 (34%)]\tLoss: 0.610648\n",
      "Train Epoch: 8 [40960/105829 (39%)]\tLoss: 0.684382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [46080/105829 (43%)]\tLoss: 0.615439\n",
      "Train Epoch: 8 [51200/105829 (48%)]\tLoss: 0.519646\n",
      "Train Epoch: 8 [56320/105829 (53%)]\tLoss: 0.581307\n",
      "Train Epoch: 8 [61440/105829 (58%)]\tLoss: 0.784528\n",
      "Train Epoch: 8 [66560/105829 (63%)]\tLoss: 0.582544\n",
      "Train Epoch: 8 [71680/105829 (68%)]\tLoss: 0.631935\n",
      "Train Epoch: 8 [76800/105829 (72%)]\tLoss: 0.587514\n",
      "Train Epoch: 8 [81920/105829 (77%)]\tLoss: 0.548394\n",
      "Train Epoch: 8 [87040/105829 (82%)]\tLoss: 0.623104\n",
      "Train Epoch: 8 [92160/105829 (87%)]\tLoss: 0.590148\n",
      "Train Epoch: 8 [97280/105829 (92%)]\tLoss: 0.624817\n",
      "Train Epoch: 8 [102400/105829 (97%)]\tLoss: 0.579884\n",
      "\n",
      "Test Epoch: 8\tAccuracy: 8675/11005 (79%)\n",
      "\n",
      "Train Epoch: 9 [0/105829 (0%)]\tLoss: 0.550552\n",
      "Train Epoch: 9 [5120/105829 (5%)]\tLoss: 0.615327\n",
      "Train Epoch: 9 [10240/105829 (10%)]\tLoss: 0.538194\n",
      "Train Epoch: 9 [15360/105829 (14%)]\tLoss: 0.625096\n",
      "Train Epoch: 9 [20480/105829 (19%)]\tLoss: 0.592723\n",
      "Train Epoch: 9 [25600/105829 (24%)]\tLoss: 0.664236\n",
      "Train Epoch: 9 [30720/105829 (29%)]\tLoss: 0.625997\n",
      "Train Epoch: 9 [35840/105829 (34%)]\tLoss: 0.615135\n",
      "Train Epoch: 9 [40960/105829 (39%)]\tLoss: 0.548595\n",
      "Train Epoch: 9 [46080/105829 (43%)]\tLoss: 0.516621\n",
      "Train Epoch: 9 [51200/105829 (48%)]\tLoss: 0.474247\n",
      "Train Epoch: 9 [56320/105829 (53%)]\tLoss: 0.510002\n",
      "Train Epoch: 9 [61440/105829 (58%)]\tLoss: 0.569040\n",
      "Train Epoch: 9 [66560/105829 (63%)]\tLoss: 0.767222\n",
      "Train Epoch: 9 [71680/105829 (68%)]\tLoss: 0.664590\n",
      "Train Epoch: 9 [76800/105829 (72%)]\tLoss: 0.524084\n",
      "Train Epoch: 9 [81920/105829 (77%)]\tLoss: 0.599852\n",
      "Train Epoch: 9 [87040/105829 (82%)]\tLoss: 0.682968\n",
      "Train Epoch: 9 [92160/105829 (87%)]\tLoss: 0.712303\n",
      "Train Epoch: 9 [97280/105829 (92%)]\tLoss: 0.546656\n",
      "Train Epoch: 9 [102400/105829 (97%)]\tLoss: 0.598918\n",
      "\n",
      "Test Epoch: 9\tAccuracy: 9035/11005 (82%)\n",
      "\n",
      "Train Epoch: 10 [0/105829 (0%)]\tLoss: 0.671444\n",
      "Train Epoch: 10 [5120/105829 (5%)]\tLoss: 0.511291\n",
      "Train Epoch: 10 [10240/105829 (10%)]\tLoss: 0.644379\n",
      "Train Epoch: 10 [15360/105829 (14%)]\tLoss: 0.531218\n",
      "Train Epoch: 10 [20480/105829 (19%)]\tLoss: 0.596850\n",
      "Train Epoch: 10 [25600/105829 (24%)]\tLoss: 0.658064\n",
      "Train Epoch: 10 [30720/105829 (29%)]\tLoss: 0.538562\n",
      "Train Epoch: 10 [35840/105829 (34%)]\tLoss: 0.569372\n",
      "Train Epoch: 10 [40960/105829 (39%)]\tLoss: 0.429458\n",
      "Train Epoch: 10 [46080/105829 (43%)]\tLoss: 0.494760\n",
      "Train Epoch: 10 [51200/105829 (48%)]\tLoss: 0.666981\n",
      "Train Epoch: 10 [56320/105829 (53%)]\tLoss: 0.669683\n",
      "Train Epoch: 10 [61440/105829 (58%)]\tLoss: 0.610637\n",
      "Train Epoch: 10 [66560/105829 (63%)]\tLoss: 0.497773\n",
      "Train Epoch: 10 [71680/105829 (68%)]\tLoss: 0.566892\n",
      "Train Epoch: 10 [76800/105829 (72%)]\tLoss: 0.543221\n",
      "Train Epoch: 10 [81920/105829 (77%)]\tLoss: 0.612552\n",
      "Train Epoch: 10 [87040/105829 (82%)]\tLoss: 0.629998\n",
      "Train Epoch: 10 [92160/105829 (87%)]\tLoss: 0.518803\n",
      "Train Epoch: 10 [97280/105829 (92%)]\tLoss: 0.671999\n",
      "Train Epoch: 10 [102400/105829 (97%)]\tLoss: 0.542666\n",
      "\n",
      "Test Epoch: 10\tAccuracy: 8577/11005 (78%)\n",
      "\n",
      "Train Epoch: 11 [0/105829 (0%)]\tLoss: 0.489959\n",
      "Train Epoch: 11 [5120/105829 (5%)]\tLoss: 0.582643\n",
      "Train Epoch: 11 [10240/105829 (10%)]\tLoss: 0.574380\n",
      "Train Epoch: 11 [15360/105829 (14%)]\tLoss: 0.550600\n",
      "Train Epoch: 11 [20480/105829 (19%)]\tLoss: 0.797415\n",
      "Train Epoch: 11 [25600/105829 (24%)]\tLoss: 0.646525\n",
      "Train Epoch: 11 [30720/105829 (29%)]\tLoss: 0.560297\n",
      "Train Epoch: 11 [35840/105829 (34%)]\tLoss: 0.608127\n",
      "Train Epoch: 11 [40960/105829 (39%)]\tLoss: 0.717660\n",
      "Train Epoch: 11 [46080/105829 (43%)]\tLoss: 0.614141\n",
      "Train Epoch: 11 [51200/105829 (48%)]\tLoss: 0.527639\n",
      "Train Epoch: 11 [56320/105829 (53%)]\tLoss: 0.531087\n",
      "Train Epoch: 11 [61440/105829 (58%)]\tLoss: 0.558880\n",
      "Train Epoch: 11 [66560/105829 (63%)]\tLoss: 0.675613\n",
      "Train Epoch: 11 [71680/105829 (68%)]\tLoss: 0.502193\n",
      "Train Epoch: 11 [76800/105829 (72%)]\tLoss: 0.439042\n",
      "Train Epoch: 11 [81920/105829 (77%)]\tLoss: 0.661080\n",
      "Train Epoch: 11 [87040/105829 (82%)]\tLoss: 0.659885\n",
      "Train Epoch: 11 [92160/105829 (87%)]\tLoss: 0.665121\n",
      "Train Epoch: 11 [97280/105829 (92%)]\tLoss: 0.545976\n",
      "Train Epoch: 11 [102400/105829 (97%)]\tLoss: 0.585158\n",
      "\n",
      "Test Epoch: 11\tAccuracy: 8803/11005 (80%)\n",
      "\n",
      "Train Epoch: 12 [0/105829 (0%)]\tLoss: 0.534805\n",
      "Train Epoch: 12 [5120/105829 (5%)]\tLoss: 0.524290\n",
      "Train Epoch: 12 [10240/105829 (10%)]\tLoss: 0.475191\n",
      "Train Epoch: 12 [15360/105829 (14%)]\tLoss: 0.551140\n",
      "Train Epoch: 12 [20480/105829 (19%)]\tLoss: 0.570131\n",
      "Train Epoch: 12 [25600/105829 (24%)]\tLoss: 0.521868\n",
      "Train Epoch: 12 [30720/105829 (29%)]\tLoss: 0.428118\n",
      "Train Epoch: 12 [35840/105829 (34%)]\tLoss: 0.419293\n",
      "Train Epoch: 12 [40960/105829 (39%)]\tLoss: 0.611620\n",
      "Train Epoch: 12 [46080/105829 (43%)]\tLoss: 0.579947\n",
      "Train Epoch: 12 [51200/105829 (48%)]\tLoss: 0.507225\n",
      "Train Epoch: 12 [56320/105829 (53%)]\tLoss: 0.522525\n",
      "Train Epoch: 12 [61440/105829 (58%)]\tLoss: 0.658750\n",
      "Train Epoch: 12 [66560/105829 (63%)]\tLoss: 0.512516\n",
      "Train Epoch: 12 [71680/105829 (68%)]\tLoss: 0.634584\n",
      "Train Epoch: 12 [76800/105829 (72%)]\tLoss: 0.519215\n",
      "Train Epoch: 12 [81920/105829 (77%)]\tLoss: 0.587148\n",
      "Train Epoch: 12 [87040/105829 (82%)]\tLoss: 0.599332\n",
      "Train Epoch: 12 [92160/105829 (87%)]\tLoss: 0.543227\n",
      "Train Epoch: 12 [97280/105829 (92%)]\tLoss: 0.634801\n",
      "Train Epoch: 12 [102400/105829 (97%)]\tLoss: 0.538477\n",
      "\n",
      "Test Epoch: 12\tAccuracy: 8219/11005 (75%)\n",
      "\n",
      "Train Epoch: 13 [0/105829 (0%)]\tLoss: 0.469998\n",
      "Train Epoch: 13 [5120/105829 (5%)]\tLoss: 0.661546\n",
      "Train Epoch: 13 [10240/105829 (10%)]\tLoss: 0.514974\n",
      "Train Epoch: 13 [15360/105829 (14%)]\tLoss: 0.476801\n",
      "Train Epoch: 13 [20480/105829 (19%)]\tLoss: 0.607252\n",
      "Train Epoch: 13 [25600/105829 (24%)]\tLoss: 0.643720\n",
      "Train Epoch: 13 [30720/105829 (29%)]\tLoss: 0.644771\n",
      "Train Epoch: 13 [35840/105829 (34%)]\tLoss: 0.539561\n",
      "Train Epoch: 13 [40960/105829 (39%)]\tLoss: 0.452337\n",
      "Train Epoch: 13 [46080/105829 (43%)]\tLoss: 0.741063\n",
      "Train Epoch: 13 [51200/105829 (48%)]\tLoss: 0.618602\n",
      "Train Epoch: 13 [56320/105829 (53%)]\tLoss: 0.507769\n",
      "Train Epoch: 13 [61440/105829 (58%)]\tLoss: 0.567005\n",
      "Train Epoch: 13 [66560/105829 (63%)]\tLoss: 0.478465\n",
      "Train Epoch: 13 [71680/105829 (68%)]\tLoss: 0.612732\n",
      "Train Epoch: 13 [76800/105829 (72%)]\tLoss: 0.597261\n",
      "Train Epoch: 13 [81920/105829 (77%)]\tLoss: 0.703274\n",
      "Train Epoch: 13 [87040/105829 (82%)]\tLoss: 0.743072\n",
      "Train Epoch: 13 [92160/105829 (87%)]\tLoss: 0.526188\n",
      "Train Epoch: 13 [97280/105829 (92%)]\tLoss: 0.654339\n",
      "Train Epoch: 13 [102400/105829 (97%)]\tLoss: 0.544262\n",
      "\n",
      "Test Epoch: 13\tAccuracy: 8690/11005 (79%)\n",
      "\n",
      "Train Epoch: 14 [0/105829 (0%)]\tLoss: 0.481890\n",
      "Train Epoch: 14 [5120/105829 (5%)]\tLoss: 0.512861\n",
      "Train Epoch: 14 [10240/105829 (10%)]\tLoss: 0.491551\n",
      "Train Epoch: 14 [15360/105829 (14%)]\tLoss: 0.520135\n",
      "Train Epoch: 14 [20480/105829 (19%)]\tLoss: 0.727585\n",
      "Train Epoch: 14 [25600/105829 (24%)]\tLoss: 0.568388\n",
      "Train Epoch: 14 [30720/105829 (29%)]\tLoss: 0.476595\n",
      "Train Epoch: 14 [35840/105829 (34%)]\tLoss: 0.708361\n",
      "Train Epoch: 14 [40960/105829 (39%)]\tLoss: 0.527397\n",
      "Train Epoch: 14 [46080/105829 (43%)]\tLoss: 0.665307\n",
      "Train Epoch: 14 [51200/105829 (48%)]\tLoss: 0.610120\n",
      "Train Epoch: 14 [56320/105829 (53%)]\tLoss: 0.460909\n",
      "Train Epoch: 14 [61440/105829 (58%)]\tLoss: 0.497959\n",
      "Train Epoch: 14 [66560/105829 (63%)]\tLoss: 0.513415\n",
      "Train Epoch: 14 [71680/105829 (68%)]\tLoss: 0.600034\n",
      "Train Epoch: 14 [76800/105829 (72%)]\tLoss: 0.661503\n",
      "Train Epoch: 14 [81920/105829 (77%)]\tLoss: 0.505609\n",
      "Train Epoch: 14 [87040/105829 (82%)]\tLoss: 0.572584\n",
      "Train Epoch: 14 [92160/105829 (87%)]\tLoss: 0.559540\n",
      "Train Epoch: 14 [97280/105829 (92%)]\tLoss: 0.560131\n",
      "Train Epoch: 14 [102400/105829 (97%)]\tLoss: 0.428328\n",
      "\n",
      "Test Epoch: 14\tAccuracy: 8940/11005 (81%)\n",
      "\n",
      "Train Epoch: 15 [0/105829 (0%)]\tLoss: 0.397948\n",
      "Train Epoch: 15 [5120/105829 (5%)]\tLoss: 0.593084\n",
      "Train Epoch: 15 [10240/105829 (10%)]\tLoss: 0.590056\n",
      "Train Epoch: 15 [15360/105829 (14%)]\tLoss: 0.568863\n",
      "Train Epoch: 15 [20480/105829 (19%)]\tLoss: 0.674944\n",
      "Train Epoch: 15 [25600/105829 (24%)]\tLoss: 0.471139\n",
      "Train Epoch: 15 [30720/105829 (29%)]\tLoss: 0.493152\n",
      "Train Epoch: 15 [35840/105829 (34%)]\tLoss: 0.426077\n",
      "Train Epoch: 15 [40960/105829 (39%)]\tLoss: 0.573291\n",
      "Train Epoch: 15 [46080/105829 (43%)]\tLoss: 0.547606\n",
      "Train Epoch: 15 [51200/105829 (48%)]\tLoss: 0.557377\n",
      "Train Epoch: 15 [56320/105829 (53%)]\tLoss: 0.662080\n",
      "Train Epoch: 15 [61440/105829 (58%)]\tLoss: 0.465527\n",
      "Train Epoch: 15 [66560/105829 (63%)]\tLoss: 0.597994\n",
      "Train Epoch: 15 [71680/105829 (68%)]\tLoss: 0.583198\n",
      "Train Epoch: 15 [76800/105829 (72%)]\tLoss: 0.582783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [81920/105829 (77%)]\tLoss: 0.501712\n",
      "Train Epoch: 15 [87040/105829 (82%)]\tLoss: 0.587027\n",
      "Train Epoch: 15 [92160/105829 (87%)]\tLoss: 0.516836\n",
      "Train Epoch: 15 [97280/105829 (92%)]\tLoss: 0.614309\n",
      "Train Epoch: 15 [102400/105829 (97%)]\tLoss: 0.452056\n",
      "\n",
      "Test Epoch: 15\tAccuracy: 9125/11005 (83%)\n",
      "\n",
      "Train Epoch: 16 [0/105829 (0%)]\tLoss: 0.409021\n",
      "Train Epoch: 16 [5120/105829 (5%)]\tLoss: 0.556153\n",
      "Train Epoch: 16 [10240/105829 (10%)]\tLoss: 0.407614\n",
      "Train Epoch: 16 [15360/105829 (14%)]\tLoss: 0.521933\n",
      "Train Epoch: 16 [20480/105829 (19%)]\tLoss: 0.491656\n",
      "Train Epoch: 16 [25600/105829 (24%)]\tLoss: 0.567128\n",
      "Train Epoch: 16 [30720/105829 (29%)]\tLoss: 0.609861\n",
      "Train Epoch: 16 [35840/105829 (34%)]\tLoss: 0.505230\n",
      "Train Epoch: 16 [40960/105829 (39%)]\tLoss: 0.739807\n",
      "Train Epoch: 16 [46080/105829 (43%)]\tLoss: 0.577927\n",
      "Train Epoch: 16 [51200/105829 (48%)]\tLoss: 0.515692\n",
      "Train Epoch: 16 [56320/105829 (53%)]\tLoss: 0.483859\n",
      "Train Epoch: 16 [61440/105829 (58%)]\tLoss: 0.386297\n",
      "Train Epoch: 16 [66560/105829 (63%)]\tLoss: 0.769134\n",
      "Train Epoch: 16 [71680/105829 (68%)]\tLoss: 0.515476\n",
      "Train Epoch: 16 [76800/105829 (72%)]\tLoss: 0.460578\n",
      "Train Epoch: 16 [81920/105829 (77%)]\tLoss: 0.449669\n",
      "Train Epoch: 16 [87040/105829 (82%)]\tLoss: 0.571074\n",
      "Train Epoch: 16 [92160/105829 (87%)]\tLoss: 0.581788\n",
      "Train Epoch: 16 [97280/105829 (92%)]\tLoss: 0.551188\n",
      "Train Epoch: 16 [102400/105829 (97%)]\tLoss: 0.522763\n",
      "\n",
      "Test Epoch: 16\tAccuracy: 8632/11005 (78%)\n",
      "\n",
      "Train Epoch: 17 [0/105829 (0%)]\tLoss: 0.359168\n",
      "Train Epoch: 17 [5120/105829 (5%)]\tLoss: 0.548535\n",
      "Train Epoch: 17 [10240/105829 (10%)]\tLoss: 0.409008\n",
      "Train Epoch: 17 [15360/105829 (14%)]\tLoss: 0.436646\n",
      "Train Epoch: 17 [20480/105829 (19%)]\tLoss: 0.505344\n",
      "Train Epoch: 17 [25600/105829 (24%)]\tLoss: 0.620625\n",
      "Train Epoch: 17 [30720/105829 (29%)]\tLoss: 0.558165\n",
      "Train Epoch: 17 [35840/105829 (34%)]\tLoss: 0.552470\n",
      "Train Epoch: 17 [40960/105829 (39%)]\tLoss: 0.627796\n",
      "Train Epoch: 17 [46080/105829 (43%)]\tLoss: 0.463997\n",
      "Train Epoch: 17 [51200/105829 (48%)]\tLoss: 0.626687\n",
      "Train Epoch: 17 [56320/105829 (53%)]\tLoss: 0.532758\n",
      "Train Epoch: 17 [61440/105829 (58%)]\tLoss: 0.565063\n",
      "Train Epoch: 17 [66560/105829 (63%)]\tLoss: 0.652621\n",
      "Train Epoch: 17 [71680/105829 (68%)]\tLoss: 0.552187\n",
      "Train Epoch: 17 [76800/105829 (72%)]\tLoss: 0.619317\n",
      "Train Epoch: 17 [81920/105829 (77%)]\tLoss: 0.577067\n",
      "Train Epoch: 17 [87040/105829 (82%)]\tLoss: 0.581484\n",
      "Train Epoch: 17 [92160/105829 (87%)]\tLoss: 0.663873\n",
      "Train Epoch: 17 [97280/105829 (92%)]\tLoss: 0.636521\n",
      "Train Epoch: 17 [102400/105829 (97%)]\tLoss: 0.624347\n",
      "\n",
      "Test Epoch: 17\tAccuracy: 8604/11005 (78%)\n",
      "\n",
      "Train Epoch: 18 [0/105829 (0%)]\tLoss: 0.547445\n",
      "Train Epoch: 18 [5120/105829 (5%)]\tLoss: 0.497245\n",
      "Train Epoch: 18 [10240/105829 (10%)]\tLoss: 0.478472\n",
      "Train Epoch: 18 [15360/105829 (14%)]\tLoss: 0.488366\n",
      "Train Epoch: 18 [20480/105829 (19%)]\tLoss: 0.498148\n",
      "Train Epoch: 18 [25600/105829 (24%)]\tLoss: 0.678216\n",
      "Train Epoch: 18 [30720/105829 (29%)]\tLoss: 0.542696\n",
      "Train Epoch: 18 [35840/105829 (34%)]\tLoss: 0.579429\n",
      "Train Epoch: 18 [40960/105829 (39%)]\tLoss: 0.650624\n",
      "Train Epoch: 18 [46080/105829 (43%)]\tLoss: 0.513490\n",
      "Train Epoch: 18 [51200/105829 (48%)]\tLoss: 0.536841\n",
      "Train Epoch: 18 [56320/105829 (53%)]\tLoss: 0.547107\n",
      "Train Epoch: 18 [61440/105829 (58%)]\tLoss: 0.550376\n",
      "Train Epoch: 18 [66560/105829 (63%)]\tLoss: 0.472879\n",
      "Train Epoch: 18 [71680/105829 (68%)]\tLoss: 0.630050\n",
      "Train Epoch: 18 [76800/105829 (72%)]\tLoss: 0.503856\n",
      "Train Epoch: 18 [81920/105829 (77%)]\tLoss: 0.634184\n",
      "Train Epoch: 18 [87040/105829 (82%)]\tLoss: 0.637079\n",
      "Train Epoch: 18 [92160/105829 (87%)]\tLoss: 0.701182\n",
      "Train Epoch: 18 [97280/105829 (92%)]\tLoss: 0.636470\n",
      "Train Epoch: 18 [102400/105829 (97%)]\tLoss: 0.576090\n",
      "\n",
      "Test Epoch: 18\tAccuracy: 9205/11005 (84%)\n",
      "\n",
      "Train Epoch: 19 [0/105829 (0%)]\tLoss: 0.588199\n",
      "Train Epoch: 19 [5120/105829 (5%)]\tLoss: 0.609471\n",
      "Train Epoch: 19 [10240/105829 (10%)]\tLoss: 0.525963\n",
      "Train Epoch: 19 [15360/105829 (14%)]\tLoss: 0.596390\n",
      "Train Epoch: 19 [20480/105829 (19%)]\tLoss: 0.518374\n",
      "Train Epoch: 19 [25600/105829 (24%)]\tLoss: 0.551365\n",
      "Train Epoch: 19 [30720/105829 (29%)]\tLoss: 0.525594\n",
      "Train Epoch: 19 [35840/105829 (34%)]\tLoss: 0.583982\n",
      "Train Epoch: 19 [40960/105829 (39%)]\tLoss: 0.557715\n",
      "Train Epoch: 19 [46080/105829 (43%)]\tLoss: 0.618520\n",
      "Train Epoch: 19 [51200/105829 (48%)]\tLoss: 0.486203\n",
      "Train Epoch: 19 [56320/105829 (53%)]\tLoss: 0.428520\n",
      "Train Epoch: 19 [61440/105829 (58%)]\tLoss: 0.493830\n",
      "Train Epoch: 19 [66560/105829 (63%)]\tLoss: 0.470022\n",
      "Train Epoch: 19 [71680/105829 (68%)]\tLoss: 0.602276\n",
      "Train Epoch: 19 [76800/105829 (72%)]\tLoss: 0.517546\n",
      "Train Epoch: 19 [81920/105829 (77%)]\tLoss: 0.555045\n",
      "Train Epoch: 19 [87040/105829 (82%)]\tLoss: 0.607512\n",
      "Train Epoch: 19 [92160/105829 (87%)]\tLoss: 0.527036\n",
      "Train Epoch: 19 [97280/105829 (92%)]\tLoss: 0.457204\n",
      "Train Epoch: 19 [102400/105829 (97%)]\tLoss: 0.521603\n",
      "\n",
      "Test Epoch: 19\tAccuracy: 8989/11005 (82%)\n",
      "\n",
      "Train Epoch: 20 [0/105829 (0%)]\tLoss: 0.425750\n",
      "Train Epoch: 20 [5120/105829 (5%)]\tLoss: 0.545905\n",
      "Train Epoch: 20 [10240/105829 (10%)]\tLoss: 0.561914\n",
      "Train Epoch: 20 [15360/105829 (14%)]\tLoss: 0.522445\n",
      "Train Epoch: 20 [20480/105829 (19%)]\tLoss: 0.473874\n",
      "Train Epoch: 20 [25600/105829 (24%)]\tLoss: 0.622423\n",
      "Train Epoch: 20 [30720/105829 (29%)]\tLoss: 0.484679\n",
      "Train Epoch: 20 [35840/105829 (34%)]\tLoss: 0.498871\n",
      "Train Epoch: 20 [40960/105829 (39%)]\tLoss: 0.652570\n",
      "Train Epoch: 20 [46080/105829 (43%)]\tLoss: 0.599199\n",
      "Train Epoch: 20 [51200/105829 (48%)]\tLoss: 0.434138\n",
      "Train Epoch: 20 [56320/105829 (53%)]\tLoss: 0.551053\n",
      "Train Epoch: 20 [61440/105829 (58%)]\tLoss: 0.713594\n",
      "Train Epoch: 20 [66560/105829 (63%)]\tLoss: 0.521084\n",
      "Train Epoch: 20 [71680/105829 (68%)]\tLoss: 0.587471\n",
      "Train Epoch: 20 [76800/105829 (72%)]\tLoss: 0.611020\n",
      "Train Epoch: 20 [81920/105829 (77%)]\tLoss: 0.534666\n",
      "Train Epoch: 20 [87040/105829 (82%)]\tLoss: 0.586013\n",
      "Train Epoch: 20 [92160/105829 (87%)]\tLoss: 0.533621\n",
      "Train Epoch: 20 [97280/105829 (92%)]\tLoss: 0.595215\n",
      "Train Epoch: 20 [102400/105829 (97%)]\tLoss: 0.625594\n",
      "\n",
      "Test Epoch: 20\tAccuracy: 9199/11005 (84%)\n",
      "\n",
      "Train Epoch: 21 [0/105829 (0%)]\tLoss: 0.511482\n",
      "Train Epoch: 21 [5120/105829 (5%)]\tLoss: 0.458581\n",
      "Train Epoch: 21 [10240/105829 (10%)]\tLoss: 0.318902\n",
      "Train Epoch: 21 [15360/105829 (14%)]\tLoss: 0.415286\n",
      "Train Epoch: 21 [20480/105829 (19%)]\tLoss: 0.475086\n",
      "Train Epoch: 21 [25600/105829 (24%)]\tLoss: 0.336535\n",
      "Train Epoch: 21 [30720/105829 (29%)]\tLoss: 0.394048\n",
      "Train Epoch: 21 [35840/105829 (34%)]\tLoss: 0.555923\n",
      "Train Epoch: 21 [40960/105829 (39%)]\tLoss: 0.537952\n",
      "Train Epoch: 21 [46080/105829 (43%)]\tLoss: 0.444856\n",
      "Train Epoch: 21 [51200/105829 (48%)]\tLoss: 0.400215\n",
      "Train Epoch: 21 [56320/105829 (53%)]\tLoss: 0.459247\n",
      "Train Epoch: 21 [61440/105829 (58%)]\tLoss: 0.351556\n",
      "Train Epoch: 21 [66560/105829 (63%)]\tLoss: 0.439266\n",
      "Train Epoch: 21 [71680/105829 (68%)]\tLoss: 0.450414\n",
      "Train Epoch: 21 [76800/105829 (72%)]\tLoss: 0.421320\n",
      "Train Epoch: 21 [81920/105829 (77%)]\tLoss: 0.371818\n",
      "Train Epoch: 21 [87040/105829 (82%)]\tLoss: 0.357236\n",
      "Train Epoch: 21 [92160/105829 (87%)]\tLoss: 0.404195\n",
      "Train Epoch: 21 [97280/105829 (92%)]\tLoss: 0.404819\n",
      "Train Epoch: 21 [102400/105829 (97%)]\tLoss: 0.384103\n",
      "\n",
      "Test Epoch: 21\tAccuracy: 9820/11005 (89%)\n",
      "\n",
      "Train Epoch: 22 [0/105829 (0%)]\tLoss: 0.411059\n",
      "Train Epoch: 22 [5120/105829 (5%)]\tLoss: 0.462297\n",
      "Train Epoch: 22 [10240/105829 (10%)]\tLoss: 0.410844\n",
      "Train Epoch: 22 [15360/105829 (14%)]\tLoss: 0.297604\n",
      "Train Epoch: 22 [20480/105829 (19%)]\tLoss: 0.370143\n",
      "Train Epoch: 22 [25600/105829 (24%)]\tLoss: 0.305388\n",
      "Train Epoch: 22 [30720/105829 (29%)]\tLoss: 0.438186\n",
      "Train Epoch: 22 [35840/105829 (34%)]\tLoss: 0.320323\n",
      "Train Epoch: 22 [40960/105829 (39%)]\tLoss: 0.362749\n",
      "Train Epoch: 22 [46080/105829 (43%)]\tLoss: 0.331896\n",
      "Train Epoch: 22 [51200/105829 (48%)]\tLoss: 0.421266\n",
      "Train Epoch: 22 [56320/105829 (53%)]\tLoss: 0.397695\n",
      "Train Epoch: 22 [61440/105829 (58%)]\tLoss: 0.286757\n",
      "Train Epoch: 22 [66560/105829 (63%)]\tLoss: 0.389557\n",
      "Train Epoch: 22 [71680/105829 (68%)]\tLoss: 0.527047\n",
      "Train Epoch: 22 [76800/105829 (72%)]\tLoss: 0.323929\n",
      "Train Epoch: 22 [81920/105829 (77%)]\tLoss: 0.408806\n",
      "Train Epoch: 22 [87040/105829 (82%)]\tLoss: 0.370489\n",
      "Train Epoch: 22 [92160/105829 (87%)]\tLoss: 0.338179\n",
      "Train Epoch: 22 [97280/105829 (92%)]\tLoss: 0.347749\n",
      "Train Epoch: 22 [102400/105829 (97%)]\tLoss: 0.389069\n",
      "\n",
      "Test Epoch: 22\tAccuracy: 9848/11005 (89%)\n",
      "\n",
      "Train Epoch: 23 [0/105829 (0%)]\tLoss: 0.411603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [5120/105829 (5%)]\tLoss: 0.280517\n",
      "Train Epoch: 23 [10240/105829 (10%)]\tLoss: 0.261788\n",
      "Train Epoch: 23 [15360/105829 (14%)]\tLoss: 0.299280\n",
      "Train Epoch: 23 [20480/105829 (19%)]\tLoss: 0.382936\n",
      "Train Epoch: 23 [25600/105829 (24%)]\tLoss: 0.449869\n",
      "Train Epoch: 23 [30720/105829 (29%)]\tLoss: 0.485339\n",
      "Train Epoch: 23 [35840/105829 (34%)]\tLoss: 0.382622\n",
      "Train Epoch: 23 [40960/105829 (39%)]\tLoss: 0.337159\n",
      "Train Epoch: 23 [46080/105829 (43%)]\tLoss: 0.341021\n",
      "Train Epoch: 23 [51200/105829 (48%)]\tLoss: 0.294655\n",
      "Train Epoch: 23 [56320/105829 (53%)]\tLoss: 0.390241\n",
      "Train Epoch: 23 [61440/105829 (58%)]\tLoss: 0.454126\n",
      "Train Epoch: 23 [66560/105829 (63%)]\tLoss: 0.363089\n",
      "Train Epoch: 23 [71680/105829 (68%)]\tLoss: 0.321814\n",
      "Train Epoch: 23 [76800/105829 (72%)]\tLoss: 0.367638\n",
      "Train Epoch: 23 [81920/105829 (77%)]\tLoss: 0.427753\n",
      "Train Epoch: 23 [87040/105829 (82%)]\tLoss: 0.428026\n",
      "Train Epoch: 23 [92160/105829 (87%)]\tLoss: 0.460358\n",
      "Train Epoch: 23 [97280/105829 (92%)]\tLoss: 0.361759\n",
      "Train Epoch: 23 [102400/105829 (97%)]\tLoss: 0.312068\n",
      "\n",
      "Test Epoch: 23\tAccuracy: 9869/11005 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/105829 (0%)]\tLoss: 0.333981\n",
      "Train Epoch: 24 [5120/105829 (5%)]\tLoss: 0.437525\n",
      "Train Epoch: 24 [10240/105829 (10%)]\tLoss: 0.396907\n",
      "Train Epoch: 24 [15360/105829 (14%)]\tLoss: 0.462676\n",
      "Train Epoch: 24 [20480/105829 (19%)]\tLoss: 0.373812\n",
      "Train Epoch: 24 [25600/105829 (24%)]\tLoss: 0.471481\n",
      "Train Epoch: 24 [30720/105829 (29%)]\tLoss: 0.245659\n",
      "Train Epoch: 24 [35840/105829 (34%)]\tLoss: 0.345529\n",
      "Train Epoch: 24 [40960/105829 (39%)]\tLoss: 0.425205\n",
      "Train Epoch: 24 [46080/105829 (43%)]\tLoss: 0.313298\n",
      "Train Epoch: 24 [51200/105829 (48%)]\tLoss: 0.302617\n",
      "Train Epoch: 24 [56320/105829 (53%)]\tLoss: 0.413751\n",
      "Train Epoch: 24 [61440/105829 (58%)]\tLoss: 0.310847\n",
      "Train Epoch: 24 [66560/105829 (63%)]\tLoss: 0.378002\n",
      "Train Epoch: 24 [71680/105829 (68%)]\tLoss: 0.437299\n",
      "Train Epoch: 24 [76800/105829 (72%)]\tLoss: 0.416909\n",
      "Train Epoch: 24 [81920/105829 (77%)]\tLoss: 0.309872\n",
      "Train Epoch: 24 [87040/105829 (82%)]\tLoss: 0.233940\n",
      "Train Epoch: 24 [92160/105829 (87%)]\tLoss: 0.313955\n",
      "Train Epoch: 24 [97280/105829 (92%)]\tLoss: 0.370302\n",
      "Train Epoch: 24 [102400/105829 (97%)]\tLoss: 0.399601\n",
      "\n",
      "Test Epoch: 24\tAccuracy: 9938/11005 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/105829 (0%)]\tLoss: 0.408247\n",
      "Train Epoch: 25 [5120/105829 (5%)]\tLoss: 0.419129\n",
      "Train Epoch: 25 [10240/105829 (10%)]\tLoss: 0.365696\n",
      "Train Epoch: 25 [15360/105829 (14%)]\tLoss: 0.434514\n",
      "Train Epoch: 25 [20480/105829 (19%)]\tLoss: 0.341086\n",
      "Train Epoch: 25 [25600/105829 (24%)]\tLoss: 0.357921\n",
      "Train Epoch: 25 [30720/105829 (29%)]\tLoss: 0.298137\n",
      "Train Epoch: 25 [35840/105829 (34%)]\tLoss: 0.382159\n",
      "Train Epoch: 25 [40960/105829 (39%)]\tLoss: 0.343700\n",
      "Train Epoch: 25 [46080/105829 (43%)]\tLoss: 0.474513\n",
      "Train Epoch: 25 [51200/105829 (48%)]\tLoss: 0.458053\n",
      "Train Epoch: 25 [56320/105829 (53%)]\tLoss: 0.358515\n",
      "Train Epoch: 25 [61440/105829 (58%)]\tLoss: 0.430248\n",
      "Train Epoch: 25 [66560/105829 (63%)]\tLoss: 0.351957\n",
      "Train Epoch: 25 [71680/105829 (68%)]\tLoss: 0.338469\n",
      "Train Epoch: 25 [76800/105829 (72%)]\tLoss: 0.396484\n",
      "Train Epoch: 25 [81920/105829 (77%)]\tLoss: 0.335120\n",
      "Train Epoch: 25 [87040/105829 (82%)]\tLoss: 0.274548\n",
      "Train Epoch: 25 [92160/105829 (87%)]\tLoss: 0.308761\n",
      "Train Epoch: 25 [97280/105829 (92%)]\tLoss: 0.318388\n",
      "Train Epoch: 25 [102400/105829 (97%)]\tLoss: 0.323884\n",
      "\n",
      "Test Epoch: 25\tAccuracy: 9924/11005 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/105829 (0%)]\tLoss: 0.387086\n",
      "Train Epoch: 26 [5120/105829 (5%)]\tLoss: 0.447096\n",
      "Train Epoch: 26 [10240/105829 (10%)]\tLoss: 0.413061\n",
      "Train Epoch: 26 [15360/105829 (14%)]\tLoss: 0.340108\n",
      "Train Epoch: 26 [20480/105829 (19%)]\tLoss: 0.397988\n",
      "Train Epoch: 26 [25600/105829 (24%)]\tLoss: 0.275968\n",
      "Train Epoch: 26 [30720/105829 (29%)]\tLoss: 0.313011\n",
      "Train Epoch: 26 [35840/105829 (34%)]\tLoss: 0.410350\n",
      "Train Epoch: 26 [40960/105829 (39%)]\tLoss: 0.288431\n",
      "Train Epoch: 26 [46080/105829 (43%)]\tLoss: 0.464380\n",
      "Train Epoch: 26 [51200/105829 (48%)]\tLoss: 0.375129\n",
      "Train Epoch: 26 [56320/105829 (53%)]\tLoss: 0.361104\n",
      "Train Epoch: 26 [61440/105829 (58%)]\tLoss: 0.429300\n",
      "Train Epoch: 26 [66560/105829 (63%)]\tLoss: 0.344915\n",
      "Train Epoch: 26 [71680/105829 (68%)]\tLoss: 0.259965\n",
      "Train Epoch: 26 [76800/105829 (72%)]\tLoss: 0.295852\n",
      "Train Epoch: 26 [81920/105829 (77%)]\tLoss: 0.398663\n",
      "Train Epoch: 26 [87040/105829 (82%)]\tLoss: 0.299004\n",
      "Train Epoch: 26 [92160/105829 (87%)]\tLoss: 0.294058\n",
      "Train Epoch: 26 [97280/105829 (92%)]\tLoss: 0.363207\n",
      "Train Epoch: 26 [102400/105829 (97%)]\tLoss: 0.500988\n",
      "\n",
      "Test Epoch: 26\tAccuracy: 9981/11005 (91%)\n",
      "\n",
      "Train Epoch: 27 [0/105829 (0%)]\tLoss: 0.242159\n",
      "Train Epoch: 27 [5120/105829 (5%)]\tLoss: 0.346664\n",
      "Train Epoch: 27 [10240/105829 (10%)]\tLoss: 0.477562\n",
      "Train Epoch: 27 [15360/105829 (14%)]\tLoss: 0.371675\n",
      "Train Epoch: 27 [20480/105829 (19%)]\tLoss: 0.317042\n",
      "Train Epoch: 27 [25600/105829 (24%)]\tLoss: 0.326595\n",
      "Train Epoch: 27 [30720/105829 (29%)]\tLoss: 0.304958\n",
      "Train Epoch: 27 [35840/105829 (34%)]\tLoss: 0.493336\n",
      "Train Epoch: 27 [40960/105829 (39%)]\tLoss: 0.492794\n",
      "Train Epoch: 27 [46080/105829 (43%)]\tLoss: 0.339572\n",
      "Train Epoch: 27 [51200/105829 (48%)]\tLoss: 0.405365\n",
      "Train Epoch: 27 [56320/105829 (53%)]\tLoss: 0.349233\n",
      "Train Epoch: 27 [61440/105829 (58%)]\tLoss: 0.294164\n",
      "Train Epoch: 27 [66560/105829 (63%)]\tLoss: 0.403464\n",
      "Train Epoch: 27 [71680/105829 (68%)]\tLoss: 0.439323\n",
      "Train Epoch: 27 [76800/105829 (72%)]\tLoss: 0.368272\n",
      "Train Epoch: 27 [81920/105829 (77%)]\tLoss: 0.367244\n",
      "Train Epoch: 27 [87040/105829 (82%)]\tLoss: 0.358984\n",
      "Train Epoch: 27 [92160/105829 (87%)]\tLoss: 0.387815\n",
      "Train Epoch: 27 [97280/105829 (92%)]\tLoss: 0.248498\n",
      "Train Epoch: 27 [102400/105829 (97%)]\tLoss: 0.264466\n",
      "\n",
      "Test Epoch: 27\tAccuracy: 9973/11005 (91%)\n",
      "\n",
      "Train Epoch: 28 [0/105829 (0%)]\tLoss: 0.423539\n",
      "Train Epoch: 28 [5120/105829 (5%)]\tLoss: 0.347440\n",
      "Train Epoch: 28 [10240/105829 (10%)]\tLoss: 0.333369\n",
      "Train Epoch: 28 [15360/105829 (14%)]\tLoss: 0.375843\n",
      "Train Epoch: 28 [20480/105829 (19%)]\tLoss: 0.361517\n",
      "Train Epoch: 28 [25600/105829 (24%)]\tLoss: 0.403491\n",
      "Train Epoch: 28 [30720/105829 (29%)]\tLoss: 0.327600\n",
      "Train Epoch: 28 [35840/105829 (34%)]\tLoss: 0.307238\n",
      "Train Epoch: 28 [40960/105829 (39%)]\tLoss: 0.539243\n",
      "Train Epoch: 28 [46080/105829 (43%)]\tLoss: 0.371267\n",
      "Train Epoch: 28 [51200/105829 (48%)]\tLoss: 0.383767\n",
      "Train Epoch: 28 [56320/105829 (53%)]\tLoss: 0.302676\n",
      "Train Epoch: 28 [61440/105829 (58%)]\tLoss: 0.321572\n",
      "Train Epoch: 28 [66560/105829 (63%)]\tLoss: 0.431772\n",
      "Train Epoch: 28 [71680/105829 (68%)]\tLoss: 0.346205\n",
      "Train Epoch: 28 [76800/105829 (72%)]\tLoss: 0.320252\n",
      "Train Epoch: 28 [81920/105829 (77%)]\tLoss: 0.344198\n",
      "Train Epoch: 28 [87040/105829 (82%)]\tLoss: 0.297087\n",
      "Train Epoch: 28 [92160/105829 (87%)]\tLoss: 0.302548\n",
      "Train Epoch: 28 [97280/105829 (92%)]\tLoss: 0.332200\n",
      "Train Epoch: 28 [102400/105829 (97%)]\tLoss: 0.354523\n",
      "\n",
      "Test Epoch: 28\tAccuracy: 10040/11005 (91%)\n",
      "\n",
      "Train Epoch: 29 [0/105829 (0%)]\tLoss: 0.365925\n",
      "Train Epoch: 29 [5120/105829 (5%)]\tLoss: 0.324469\n",
      "Train Epoch: 29 [10240/105829 (10%)]\tLoss: 0.296026\n",
      "Train Epoch: 29 [15360/105829 (14%)]\tLoss: 0.326337\n",
      "Train Epoch: 29 [20480/105829 (19%)]\tLoss: 0.334017\n",
      "Train Epoch: 29 [25600/105829 (24%)]\tLoss: 0.305358\n",
      "Train Epoch: 29 [30720/105829 (29%)]\tLoss: 0.347839\n",
      "Train Epoch: 29 [35840/105829 (34%)]\tLoss: 0.407210\n",
      "Train Epoch: 29 [40960/105829 (39%)]\tLoss: 0.280427\n",
      "Train Epoch: 29 [46080/105829 (43%)]\tLoss: 0.316951\n",
      "Train Epoch: 29 [51200/105829 (48%)]\tLoss: 0.339832\n",
      "Train Epoch: 29 [56320/105829 (53%)]\tLoss: 0.356007\n",
      "Train Epoch: 29 [61440/105829 (58%)]\tLoss: 0.317172\n",
      "Train Epoch: 29 [66560/105829 (63%)]\tLoss: 0.308054\n",
      "Train Epoch: 29 [71680/105829 (68%)]\tLoss: 0.359806\n",
      "Train Epoch: 29 [76800/105829 (72%)]\tLoss: 0.300001\n",
      "Train Epoch: 29 [81920/105829 (77%)]\tLoss: 0.314510\n",
      "Train Epoch: 29 [87040/105829 (82%)]\tLoss: 0.313460\n",
      "Train Epoch: 29 [92160/105829 (87%)]\tLoss: 0.283915\n",
      "Train Epoch: 29 [97280/105829 (92%)]\tLoss: 0.238809\n",
      "Train Epoch: 29 [102400/105829 (97%)]\tLoss: 0.363085\n",
      "\n",
      "Test Epoch: 29\tAccuracy: 10027/11005 (91%)\n",
      "\n",
      "Train Epoch: 30 [0/105829 (0%)]\tLoss: 0.317387\n",
      "Train Epoch: 30 [5120/105829 (5%)]\tLoss: 0.317089\n",
      "Train Epoch: 30 [10240/105829 (10%)]\tLoss: 0.328272\n",
      "Train Epoch: 30 [15360/105829 (14%)]\tLoss: 0.342769\n",
      "Train Epoch: 30 [20480/105829 (19%)]\tLoss: 0.391174\n",
      "Train Epoch: 30 [25600/105829 (24%)]\tLoss: 0.357348\n",
      "Train Epoch: 30 [30720/105829 (29%)]\tLoss: 0.241452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [35840/105829 (34%)]\tLoss: 0.312623\n",
      "Train Epoch: 30 [40960/105829 (39%)]\tLoss: 0.375254\n",
      "Train Epoch: 30 [46080/105829 (43%)]\tLoss: 0.320988\n",
      "Train Epoch: 30 [51200/105829 (48%)]\tLoss: 0.288606\n",
      "Train Epoch: 30 [56320/105829 (53%)]\tLoss: 0.315083\n",
      "Train Epoch: 30 [61440/105829 (58%)]\tLoss: 0.436876\n",
      "Train Epoch: 30 [66560/105829 (63%)]\tLoss: 0.420704\n",
      "Train Epoch: 30 [71680/105829 (68%)]\tLoss: 0.419901\n",
      "Train Epoch: 30 [76800/105829 (72%)]\tLoss: 0.456747\n",
      "Train Epoch: 30 [81920/105829 (77%)]\tLoss: 0.364027\n",
      "Train Epoch: 30 [87040/105829 (82%)]\tLoss: 0.363977\n",
      "Train Epoch: 30 [92160/105829 (87%)]\tLoss: 0.289803\n",
      "Train Epoch: 30 [97280/105829 (92%)]\tLoss: 0.336651\n",
      "Train Epoch: 30 [102400/105829 (97%)]\tLoss: 0.353213\n",
      "\n",
      "Test Epoch: 30\tAccuracy: 10041/11005 (91%)\n",
      "\n",
      "Train Epoch: 31 [0/105829 (0%)]\tLoss: 0.249158\n",
      "Train Epoch: 31 [5120/105829 (5%)]\tLoss: 0.444250\n",
      "Train Epoch: 31 [10240/105829 (10%)]\tLoss: 0.307166\n",
      "Train Epoch: 31 [15360/105829 (14%)]\tLoss: 0.353364\n",
      "Train Epoch: 31 [20480/105829 (19%)]\tLoss: 0.292995\n",
      "Train Epoch: 31 [25600/105829 (24%)]\tLoss: 0.385319\n",
      "Train Epoch: 31 [30720/105829 (29%)]\tLoss: 0.341882\n",
      "Train Epoch: 31 [35840/105829 (34%)]\tLoss: 0.283158\n",
      "Train Epoch: 31 [40960/105829 (39%)]\tLoss: 0.242621\n",
      "Train Epoch: 31 [46080/105829 (43%)]\tLoss: 0.270085\n",
      "Train Epoch: 31 [51200/105829 (48%)]\tLoss: 0.341192\n",
      "Train Epoch: 31 [56320/105829 (53%)]\tLoss: 0.294541\n",
      "Train Epoch: 31 [61440/105829 (58%)]\tLoss: 0.401934\n",
      "Train Epoch: 31 [66560/105829 (63%)]\tLoss: 0.405551\n",
      "Train Epoch: 31 [71680/105829 (68%)]\tLoss: 0.259065\n",
      "Train Epoch: 31 [76800/105829 (72%)]\tLoss: 0.372580\n",
      "Train Epoch: 31 [81920/105829 (77%)]\tLoss: 0.292288\n",
      "Train Epoch: 31 [87040/105829 (82%)]\tLoss: 0.363483\n",
      "Train Epoch: 31 [92160/105829 (87%)]\tLoss: 0.318999\n",
      "Train Epoch: 31 [97280/105829 (92%)]\tLoss: 0.328726\n",
      "Train Epoch: 31 [102400/105829 (97%)]\tLoss: 0.291926\n",
      "\n",
      "Test Epoch: 31\tAccuracy: 10042/11005 (91%)\n",
      "\n",
      "Train Epoch: 32 [0/105829 (0%)]\tLoss: 0.263521\n",
      "Train Epoch: 32 [5120/105829 (5%)]\tLoss: 0.219052\n",
      "Train Epoch: 32 [10240/105829 (10%)]\tLoss: 0.306688\n",
      "Train Epoch: 32 [15360/105829 (14%)]\tLoss: 0.280606\n",
      "Train Epoch: 32 [20480/105829 (19%)]\tLoss: 0.263823\n",
      "Train Epoch: 32 [25600/105829 (24%)]\tLoss: 0.303418\n",
      "Train Epoch: 32 [30720/105829 (29%)]\tLoss: 0.351512\n",
      "Train Epoch: 32 [35840/105829 (34%)]\tLoss: 0.301060\n",
      "Train Epoch: 32 [40960/105829 (39%)]\tLoss: 0.281037\n",
      "Train Epoch: 32 [46080/105829 (43%)]\tLoss: 0.331337\n",
      "Train Epoch: 32 [51200/105829 (48%)]\tLoss: 0.298003\n",
      "Train Epoch: 32 [56320/105829 (53%)]\tLoss: 0.367933\n",
      "Train Epoch: 32 [61440/105829 (58%)]\tLoss: 0.359287\n",
      "Train Epoch: 32 [66560/105829 (63%)]\tLoss: 0.392446\n",
      "Train Epoch: 32 [71680/105829 (68%)]\tLoss: 0.339233\n",
      "Train Epoch: 32 [76800/105829 (72%)]\tLoss: 0.420904\n",
      "Train Epoch: 32 [81920/105829 (77%)]\tLoss: 0.336637\n",
      "Train Epoch: 32 [87040/105829 (82%)]\tLoss: 0.360820\n",
      "Train Epoch: 32 [92160/105829 (87%)]\tLoss: 0.350842\n",
      "Train Epoch: 32 [97280/105829 (92%)]\tLoss: 0.269692\n",
      "Train Epoch: 32 [102400/105829 (97%)]\tLoss: 0.258983\n",
      "\n",
      "Test Epoch: 32\tAccuracy: 10082/11005 (92%)\n",
      "\n",
      "Train Epoch: 33 [0/105829 (0%)]\tLoss: 0.275708\n",
      "Train Epoch: 33 [5120/105829 (5%)]\tLoss: 0.358325\n",
      "Train Epoch: 33 [10240/105829 (10%)]\tLoss: 0.397018\n",
      "Train Epoch: 33 [15360/105829 (14%)]\tLoss: 0.311983\n",
      "Train Epoch: 33 [20480/105829 (19%)]\tLoss: 0.317309\n",
      "Train Epoch: 33 [25600/105829 (24%)]\tLoss: 0.382794\n",
      "Train Epoch: 33 [30720/105829 (29%)]\tLoss: 0.313662\n",
      "Train Epoch: 33 [35840/105829 (34%)]\tLoss: 0.335403\n",
      "Train Epoch: 33 [40960/105829 (39%)]\tLoss: 0.397320\n",
      "Train Epoch: 33 [46080/105829 (43%)]\tLoss: 0.323638\n",
      "Train Epoch: 33 [51200/105829 (48%)]\tLoss: 0.264464\n",
      "Train Epoch: 33 [56320/105829 (53%)]\tLoss: 0.333204\n",
      "Train Epoch: 33 [61440/105829 (58%)]\tLoss: 0.326214\n",
      "Train Epoch: 33 [66560/105829 (63%)]\tLoss: 0.300759\n",
      "Train Epoch: 33 [71680/105829 (68%)]\tLoss: 0.430103\n",
      "Train Epoch: 33 [76800/105829 (72%)]\tLoss: 0.348610\n",
      "Train Epoch: 33 [81920/105829 (77%)]\tLoss: 0.225447\n",
      "Train Epoch: 33 [87040/105829 (82%)]\tLoss: 0.350501\n",
      "Train Epoch: 33 [92160/105829 (87%)]\tLoss: 0.188210\n",
      "Train Epoch: 33 [97280/105829 (92%)]\tLoss: 0.357647\n",
      "Train Epoch: 33 [102400/105829 (97%)]\tLoss: 0.402704\n",
      "\n",
      "Test Epoch: 33\tAccuracy: 10051/11005 (91%)\n",
      "\n",
      "Train Epoch: 34 [0/105829 (0%)]\tLoss: 0.364035\n",
      "Train Epoch: 34 [5120/105829 (5%)]\tLoss: 0.325430\n",
      "Train Epoch: 34 [10240/105829 (10%)]\tLoss: 0.302339\n",
      "Train Epoch: 34 [15360/105829 (14%)]\tLoss: 0.343697\n",
      "Train Epoch: 34 [20480/105829 (19%)]\tLoss: 0.344541\n",
      "Train Epoch: 34 [25600/105829 (24%)]\tLoss: 0.338960\n",
      "Train Epoch: 34 [30720/105829 (29%)]\tLoss: 0.324894\n",
      "Train Epoch: 34 [35840/105829 (34%)]\tLoss: 0.254326\n",
      "Train Epoch: 34 [40960/105829 (39%)]\tLoss: 0.260040\n",
      "Train Epoch: 34 [46080/105829 (43%)]\tLoss: 0.384372\n",
      "Train Epoch: 34 [51200/105829 (48%)]\tLoss: 0.278940\n",
      "Train Epoch: 34 [56320/105829 (53%)]\tLoss: 0.353353\n",
      "Train Epoch: 34 [61440/105829 (58%)]\tLoss: 0.383758\n",
      "Train Epoch: 34 [66560/105829 (63%)]\tLoss: 0.368275\n",
      "Train Epoch: 34 [71680/105829 (68%)]\tLoss: 0.469053\n",
      "Train Epoch: 34 [76800/105829 (72%)]\tLoss: 0.471676\n",
      "Train Epoch: 34 [81920/105829 (77%)]\tLoss: 0.332194\n",
      "Train Epoch: 34 [87040/105829 (82%)]\tLoss: 0.326817\n",
      "Train Epoch: 34 [92160/105829 (87%)]\tLoss: 0.341655\n",
      "Train Epoch: 34 [97280/105829 (92%)]\tLoss: 0.448303\n",
      "Train Epoch: 34 [102400/105829 (97%)]\tLoss: 0.356241\n",
      "\n",
      "Test Epoch: 34\tAccuracy: 10091/11005 (92%)\n",
      "\n",
      "Train Epoch: 35 [0/105829 (0%)]\tLoss: 0.282382\n",
      "Train Epoch: 35 [5120/105829 (5%)]\tLoss: 0.346510\n",
      "Train Epoch: 35 [10240/105829 (10%)]\tLoss: 0.377738\n",
      "Train Epoch: 35 [15360/105829 (14%)]\tLoss: 0.446941\n",
      "Train Epoch: 35 [20480/105829 (19%)]\tLoss: 0.357147\n",
      "Train Epoch: 35 [25600/105829 (24%)]\tLoss: 0.327933\n",
      "Train Epoch: 35 [30720/105829 (29%)]\tLoss: 0.355144\n",
      "Train Epoch: 35 [35840/105829 (34%)]\tLoss: 0.241505\n",
      "Train Epoch: 35 [40960/105829 (39%)]\tLoss: 0.344922\n",
      "Train Epoch: 35 [46080/105829 (43%)]\tLoss: 0.329690\n",
      "Train Epoch: 35 [51200/105829 (48%)]\tLoss: 0.295832\n",
      "Train Epoch: 35 [56320/105829 (53%)]\tLoss: 0.391083\n",
      "Train Epoch: 35 [61440/105829 (58%)]\tLoss: 0.344261\n",
      "Train Epoch: 35 [66560/105829 (63%)]\tLoss: 0.303072\n",
      "Train Epoch: 35 [71680/105829 (68%)]\tLoss: 0.308053\n",
      "Train Epoch: 35 [76800/105829 (72%)]\tLoss: 0.312095\n",
      "Train Epoch: 35 [81920/105829 (77%)]\tLoss: 0.253286\n",
      "Train Epoch: 35 [87040/105829 (82%)]\tLoss: 0.352867\n",
      "Train Epoch: 35 [92160/105829 (87%)]\tLoss: 0.347634\n",
      "Train Epoch: 35 [97280/105829 (92%)]\tLoss: 0.319787\n",
      "Train Epoch: 35 [102400/105829 (97%)]\tLoss: 0.419111\n",
      "\n",
      "Test Epoch: 35\tAccuracy: 10102/11005 (92%)\n",
      "\n",
      "Train Epoch: 36 [0/105829 (0%)]\tLoss: 0.309119\n",
      "Train Epoch: 36 [5120/105829 (5%)]\tLoss: 0.357568\n",
      "Train Epoch: 36 [10240/105829 (10%)]\tLoss: 0.282339\n",
      "Train Epoch: 36 [15360/105829 (14%)]\tLoss: 0.328556\n",
      "Train Epoch: 36 [20480/105829 (19%)]\tLoss: 0.292853\n",
      "Train Epoch: 36 [25600/105829 (24%)]\tLoss: 0.325814\n",
      "Train Epoch: 36 [30720/105829 (29%)]\tLoss: 0.366409\n",
      "Train Epoch: 36 [35840/105829 (34%)]\tLoss: 0.345641\n",
      "Train Epoch: 36 [40960/105829 (39%)]\tLoss: 0.238631\n",
      "Train Epoch: 36 [46080/105829 (43%)]\tLoss: 0.296175\n",
      "Train Epoch: 36 [51200/105829 (48%)]\tLoss: 0.315779\n",
      "Train Epoch: 36 [56320/105829 (53%)]\tLoss: 0.348486\n",
      "Train Epoch: 36 [61440/105829 (58%)]\tLoss: 0.311250\n",
      "Train Epoch: 36 [66560/105829 (63%)]\tLoss: 0.335076\n",
      "Train Epoch: 36 [71680/105829 (68%)]\tLoss: 0.265264\n",
      "Train Epoch: 36 [76800/105829 (72%)]\tLoss: 0.321783\n",
      "Train Epoch: 36 [81920/105829 (77%)]\tLoss: 0.195594\n",
      "Train Epoch: 36 [87040/105829 (82%)]\tLoss: 0.329612\n",
      "Train Epoch: 36 [92160/105829 (87%)]\tLoss: 0.301303\n",
      "Train Epoch: 36 [97280/105829 (92%)]\tLoss: 0.348277\n",
      "Train Epoch: 36 [102400/105829 (97%)]\tLoss: 0.315090\n",
      "\n",
      "Test Epoch: 36\tAccuracy: 10089/11005 (92%)\n",
      "\n",
      "Train Epoch: 37 [0/105829 (0%)]\tLoss: 0.225950\n",
      "Train Epoch: 37 [5120/105829 (5%)]\tLoss: 0.315042\n",
      "Train Epoch: 37 [10240/105829 (10%)]\tLoss: 0.303162\n",
      "Train Epoch: 37 [15360/105829 (14%)]\tLoss: 0.200454\n",
      "Train Epoch: 37 [20480/105829 (19%)]\tLoss: 0.360787\n",
      "Train Epoch: 37 [25600/105829 (24%)]\tLoss: 0.368471\n",
      "Train Epoch: 37 [30720/105829 (29%)]\tLoss: 0.348203\n",
      "Train Epoch: 37 [35840/105829 (34%)]\tLoss: 0.271193\n",
      "Train Epoch: 37 [40960/105829 (39%)]\tLoss: 0.398583\n",
      "Train Epoch: 37 [46080/105829 (43%)]\tLoss: 0.292987\n",
      "Train Epoch: 37 [51200/105829 (48%)]\tLoss: 0.416257\n",
      "Train Epoch: 37 [56320/105829 (53%)]\tLoss: 0.369693\n",
      "Train Epoch: 37 [61440/105829 (58%)]\tLoss: 0.340298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [66560/105829 (63%)]\tLoss: 0.297496\n",
      "Train Epoch: 37 [71680/105829 (68%)]\tLoss: 0.280007\n",
      "Train Epoch: 37 [76800/105829 (72%)]\tLoss: 0.436871\n",
      "Train Epoch: 37 [81920/105829 (77%)]\tLoss: 0.349436\n",
      "Train Epoch: 37 [87040/105829 (82%)]\tLoss: 0.507636\n",
      "Train Epoch: 37 [92160/105829 (87%)]\tLoss: 0.283021\n",
      "Train Epoch: 37 [97280/105829 (92%)]\tLoss: 0.263225\n",
      "Train Epoch: 37 [102400/105829 (97%)]\tLoss: 0.364004\n",
      "\n",
      "Test Epoch: 37\tAccuracy: 10119/11005 (92%)\n",
      "\n",
      "Train Epoch: 38 [0/105829 (0%)]\tLoss: 0.355217\n",
      "Train Epoch: 38 [5120/105829 (5%)]\tLoss: 0.401427\n",
      "Train Epoch: 38 [10240/105829 (10%)]\tLoss: 0.338765\n",
      "Train Epoch: 38 [15360/105829 (14%)]\tLoss: 0.256486\n",
      "Train Epoch: 38 [20480/105829 (19%)]\tLoss: 0.260205\n",
      "Train Epoch: 38 [25600/105829 (24%)]\tLoss: 0.270499\n",
      "Train Epoch: 38 [30720/105829 (29%)]\tLoss: 0.323496\n",
      "Train Epoch: 38 [35840/105829 (34%)]\tLoss: 0.334987\n",
      "Train Epoch: 38 [40960/105829 (39%)]\tLoss: 0.324954\n",
      "Train Epoch: 38 [46080/105829 (43%)]\tLoss: 0.309090\n",
      "Train Epoch: 38 [51200/105829 (48%)]\tLoss: 0.385824\n",
      "Train Epoch: 38 [56320/105829 (53%)]\tLoss: 0.262249\n",
      "Train Epoch: 38 [61440/105829 (58%)]\tLoss: 0.334920\n",
      "Train Epoch: 38 [66560/105829 (63%)]\tLoss: 0.262470\n",
      "Train Epoch: 38 [71680/105829 (68%)]\tLoss: 0.304628\n",
      "Train Epoch: 38 [76800/105829 (72%)]\tLoss: 0.317464\n",
      "Train Epoch: 38 [81920/105829 (77%)]\tLoss: 0.241766\n",
      "Train Epoch: 38 [87040/105829 (82%)]\tLoss: 0.292087\n",
      "Train Epoch: 38 [92160/105829 (87%)]\tLoss: 0.365547\n",
      "Train Epoch: 38 [97280/105829 (92%)]\tLoss: 0.290606\n",
      "Train Epoch: 38 [102400/105829 (97%)]\tLoss: 0.436346\n",
      "\n",
      "Test Epoch: 38\tAccuracy: 10127/11005 (92%)\n",
      "\n",
      "Train Epoch: 39 [0/105829 (0%)]\tLoss: 0.297297\n",
      "Train Epoch: 39 [5120/105829 (5%)]\tLoss: 0.293133\n",
      "Train Epoch: 39 [10240/105829 (10%)]\tLoss: 0.327785\n",
      "Train Epoch: 39 [15360/105829 (14%)]\tLoss: 0.287779\n",
      "Train Epoch: 39 [20480/105829 (19%)]\tLoss: 0.227694\n",
      "Train Epoch: 39 [25600/105829 (24%)]\tLoss: 0.220325\n",
      "Train Epoch: 39 [30720/105829 (29%)]\tLoss: 0.218403\n",
      "Train Epoch: 39 [35840/105829 (34%)]\tLoss: 0.279886\n",
      "Train Epoch: 39 [40960/105829 (39%)]\tLoss: 0.390716\n",
      "Train Epoch: 39 [46080/105829 (43%)]\tLoss: 0.352122\n",
      "Train Epoch: 39 [51200/105829 (48%)]\tLoss: 0.288560\n",
      "Train Epoch: 39 [56320/105829 (53%)]\tLoss: 0.267187\n",
      "Train Epoch: 39 [61440/105829 (58%)]\tLoss: 0.324264\n",
      "Train Epoch: 39 [66560/105829 (63%)]\tLoss: 0.233124\n",
      "Train Epoch: 39 [71680/105829 (68%)]\tLoss: 0.337087\n",
      "Train Epoch: 39 [76800/105829 (72%)]\tLoss: 0.336151\n",
      "Train Epoch: 39 [81920/105829 (77%)]\tLoss: 0.268424\n",
      "Train Epoch: 39 [87040/105829 (82%)]\tLoss: 0.378214\n",
      "Train Epoch: 39 [92160/105829 (87%)]\tLoss: 0.306427\n",
      "Train Epoch: 39 [97280/105829 (92%)]\tLoss: 0.283727\n",
      "Train Epoch: 39 [102400/105829 (97%)]\tLoss: 0.292925\n",
      "\n",
      "Test Epoch: 39\tAccuracy: 10140/11005 (92%)\n",
      "\n",
      "Train Epoch: 40 [0/105829 (0%)]\tLoss: 0.302034\n",
      "Train Epoch: 40 [5120/105829 (5%)]\tLoss: 0.407482\n",
      "Train Epoch: 40 [10240/105829 (10%)]\tLoss: 0.393857\n",
      "Train Epoch: 40 [15360/105829 (14%)]\tLoss: 0.269329\n",
      "Train Epoch: 40 [20480/105829 (19%)]\tLoss: 0.236763\n",
      "Train Epoch: 40 [25600/105829 (24%)]\tLoss: 0.281589\n",
      "Train Epoch: 40 [30720/105829 (29%)]\tLoss: 0.282978\n",
      "Train Epoch: 40 [35840/105829 (34%)]\tLoss: 0.320099\n",
      "Train Epoch: 40 [40960/105829 (39%)]\tLoss: 0.356834\n",
      "Train Epoch: 40 [46080/105829 (43%)]\tLoss: 0.303453\n",
      "Train Epoch: 40 [51200/105829 (48%)]\tLoss: 0.302272\n",
      "Train Epoch: 40 [56320/105829 (53%)]\tLoss: 0.294327\n",
      "Train Epoch: 40 [61440/105829 (58%)]\tLoss: 0.392435\n",
      "Train Epoch: 40 [66560/105829 (63%)]\tLoss: 0.324197\n",
      "Train Epoch: 40 [71680/105829 (68%)]\tLoss: 0.259826\n",
      "Train Epoch: 40 [76800/105829 (72%)]\tLoss: 0.270220\n",
      "Train Epoch: 40 [81920/105829 (77%)]\tLoss: 0.353856\n",
      "Train Epoch: 40 [87040/105829 (82%)]\tLoss: 0.341848\n",
      "Train Epoch: 40 [92160/105829 (87%)]\tLoss: 0.291954\n",
      "Train Epoch: 40 [97280/105829 (92%)]\tLoss: 0.263339\n",
      "Train Epoch: 40 [102400/105829 (97%)]\tLoss: 0.379186\n",
      "\n",
      "Test Epoch: 40\tAccuracy: 10071/11005 (92%)\n",
      "\n",
      "Train Epoch: 41 [0/105829 (0%)]\tLoss: 0.331742\n",
      "Train Epoch: 41 [5120/105829 (5%)]\tLoss: 0.253134\n",
      "Train Epoch: 41 [10240/105829 (10%)]\tLoss: 0.328401\n",
      "Train Epoch: 41 [15360/105829 (14%)]\tLoss: 0.235590\n",
      "Train Epoch: 41 [20480/105829 (19%)]\tLoss: 0.283588\n",
      "Train Epoch: 41 [25600/105829 (24%)]\tLoss: 0.318187\n",
      "Train Epoch: 41 [30720/105829 (29%)]\tLoss: 0.240464\n",
      "Train Epoch: 41 [35840/105829 (34%)]\tLoss: 0.282974\n",
      "Train Epoch: 41 [40960/105829 (39%)]\tLoss: 0.159488\n",
      "Train Epoch: 41 [46080/105829 (43%)]\tLoss: 0.321221\n",
      "Train Epoch: 41 [51200/105829 (48%)]\tLoss: 0.360541\n",
      "Train Epoch: 41 [56320/105829 (53%)]\tLoss: 0.339766\n",
      "Train Epoch: 41 [61440/105829 (58%)]\tLoss: 0.258634\n",
      "Train Epoch: 41 [66560/105829 (63%)]\tLoss: 0.360153\n",
      "Train Epoch: 41 [71680/105829 (68%)]\tLoss: 0.241988\n",
      "Train Epoch: 41 [76800/105829 (72%)]\tLoss: 0.383711\n",
      "Train Epoch: 41 [81920/105829 (77%)]\tLoss: 0.381145\n",
      "Train Epoch: 41 [87040/105829 (82%)]\tLoss: 0.289899\n",
      "Train Epoch: 41 [92160/105829 (87%)]\tLoss: 0.239196\n",
      "Train Epoch: 41 [97280/105829 (92%)]\tLoss: 0.260624\n",
      "Train Epoch: 41 [102400/105829 (97%)]\tLoss: 0.218226\n",
      "\n",
      "Test Epoch: 41\tAccuracy: 10213/11005 (93%)\n",
      "\n",
      "Train Epoch: 42 [0/105829 (0%)]\tLoss: 0.328117\n",
      "Train Epoch: 42 [5120/105829 (5%)]\tLoss: 0.291873\n",
      "Train Epoch: 42 [10240/105829 (10%)]\tLoss: 0.263379\n",
      "Train Epoch: 42 [15360/105829 (14%)]\tLoss: 0.308980\n",
      "Train Epoch: 42 [20480/105829 (19%)]\tLoss: 0.252957\n",
      "Train Epoch: 42 [25600/105829 (24%)]\tLoss: 0.221148\n",
      "Train Epoch: 42 [30720/105829 (29%)]\tLoss: 0.335923\n",
      "Train Epoch: 42 [35840/105829 (34%)]\tLoss: 0.282788\n",
      "Train Epoch: 42 [40960/105829 (39%)]\tLoss: 0.334689\n",
      "Train Epoch: 42 [46080/105829 (43%)]\tLoss: 0.206560\n",
      "Train Epoch: 42 [51200/105829 (48%)]\tLoss: 0.239813\n",
      "Train Epoch: 42 [56320/105829 (53%)]\tLoss: 0.216911\n",
      "Train Epoch: 42 [61440/105829 (58%)]\tLoss: 0.263935\n",
      "Train Epoch: 42 [66560/105829 (63%)]\tLoss: 0.285423\n",
      "Train Epoch: 42 [71680/105829 (68%)]\tLoss: 0.244615\n",
      "Train Epoch: 42 [76800/105829 (72%)]\tLoss: 0.262645\n",
      "Train Epoch: 42 [81920/105829 (77%)]\tLoss: 0.276993\n",
      "Train Epoch: 42 [87040/105829 (82%)]\tLoss: 0.227417\n",
      "Train Epoch: 42 [92160/105829 (87%)]\tLoss: 0.307947\n",
      "Train Epoch: 42 [97280/105829 (92%)]\tLoss: 0.268663\n",
      "Train Epoch: 42 [102400/105829 (97%)]\tLoss: 0.206053\n",
      "\n",
      "Test Epoch: 42\tAccuracy: 10214/11005 (93%)\n",
      "\n",
      "Train Epoch: 43 [0/105829 (0%)]\tLoss: 0.263591\n",
      "Train Epoch: 43 [5120/105829 (5%)]\tLoss: 0.250719\n",
      "Train Epoch: 43 [10240/105829 (10%)]\tLoss: 0.207601\n",
      "Train Epoch: 43 [15360/105829 (14%)]\tLoss: 0.212532\n",
      "Train Epoch: 43 [20480/105829 (19%)]\tLoss: 0.345660\n",
      "Train Epoch: 43 [25600/105829 (24%)]\tLoss: 0.254054\n",
      "Train Epoch: 43 [30720/105829 (29%)]\tLoss: 0.283137\n",
      "Train Epoch: 43 [35840/105829 (34%)]\tLoss: 0.277022\n",
      "Train Epoch: 43 [40960/105829 (39%)]\tLoss: 0.265593\n",
      "Train Epoch: 43 [46080/105829 (43%)]\tLoss: 0.317662\n",
      "Train Epoch: 43 [51200/105829 (48%)]\tLoss: 0.316713\n",
      "Train Epoch: 43 [56320/105829 (53%)]\tLoss: 0.348204\n",
      "Train Epoch: 43 [61440/105829 (58%)]\tLoss: 0.297751\n",
      "Train Epoch: 43 [66560/105829 (63%)]\tLoss: 0.338812\n",
      "Train Epoch: 43 [71680/105829 (68%)]\tLoss: 0.324862\n",
      "Train Epoch: 43 [76800/105829 (72%)]\tLoss: 0.246468\n",
      "Train Epoch: 43 [81920/105829 (77%)]\tLoss: 0.315714\n",
      "Train Epoch: 43 [87040/105829 (82%)]\tLoss: 0.212480\n",
      "Train Epoch: 43 [92160/105829 (87%)]\tLoss: 0.328179\n",
      "Train Epoch: 43 [97280/105829 (92%)]\tLoss: 0.313799\n",
      "Train Epoch: 43 [102400/105829 (97%)]\tLoss: 0.339537\n",
      "\n",
      "Test Epoch: 43\tAccuracy: 10254/11005 (93%)\n",
      "\n",
      "Train Epoch: 44 [0/105829 (0%)]\tLoss: 0.271430\n",
      "Train Epoch: 44 [5120/105829 (5%)]\tLoss: 0.283865\n",
      "Train Epoch: 44 [10240/105829 (10%)]\tLoss: 0.363732\n",
      "Train Epoch: 44 [15360/105829 (14%)]\tLoss: 0.237932\n",
      "Train Epoch: 44 [20480/105829 (19%)]\tLoss: 0.274976\n",
      "Train Epoch: 44 [25600/105829 (24%)]\tLoss: 0.248302\n",
      "Train Epoch: 44 [30720/105829 (29%)]\tLoss: 0.275272\n",
      "Train Epoch: 44 [35840/105829 (34%)]\tLoss: 0.326846\n",
      "Train Epoch: 44 [40960/105829 (39%)]\tLoss: 0.214368\n",
      "Train Epoch: 44 [46080/105829 (43%)]\tLoss: 0.269218\n",
      "Train Epoch: 44 [51200/105829 (48%)]\tLoss: 0.218818\n",
      "Train Epoch: 44 [56320/105829 (53%)]\tLoss: 0.283017\n",
      "Train Epoch: 44 [61440/105829 (58%)]\tLoss: 0.209875\n",
      "Train Epoch: 44 [66560/105829 (63%)]\tLoss: 0.266966\n",
      "Train Epoch: 44 [71680/105829 (68%)]\tLoss: 0.324378\n",
      "Train Epoch: 44 [76800/105829 (72%)]\tLoss: 0.239597\n",
      "Train Epoch: 44 [81920/105829 (77%)]\tLoss: 0.286516\n",
      "Train Epoch: 44 [87040/105829 (82%)]\tLoss: 0.293276\n",
      "Train Epoch: 44 [92160/105829 (87%)]\tLoss: 0.315445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [97280/105829 (92%)]\tLoss: 0.302478\n",
      "Train Epoch: 44 [102400/105829 (97%)]\tLoss: 0.316308\n",
      "\n",
      "Test Epoch: 44\tAccuracy: 10246/11005 (93%)\n",
      "\n",
      "Train Epoch: 45 [0/105829 (0%)]\tLoss: 0.194847\n",
      "Train Epoch: 45 [5120/105829 (5%)]\tLoss: 0.346228\n",
      "Train Epoch: 45 [10240/105829 (10%)]\tLoss: 0.267117\n",
      "Train Epoch: 45 [15360/105829 (14%)]\tLoss: 0.282537\n",
      "Train Epoch: 45 [20480/105829 (19%)]\tLoss: 0.362509\n",
      "Train Epoch: 45 [25600/105829 (24%)]\tLoss: 0.340679\n",
      "Train Epoch: 45 [30720/105829 (29%)]\tLoss: 0.251633\n",
      "Train Epoch: 45 [35840/105829 (34%)]\tLoss: 0.344083\n",
      "Train Epoch: 45 [40960/105829 (39%)]\tLoss: 0.418973\n",
      "Train Epoch: 45 [46080/105829 (43%)]\tLoss: 0.319292\n",
      "Train Epoch: 45 [51200/105829 (48%)]\tLoss: 0.226248\n",
      "Train Epoch: 45 [56320/105829 (53%)]\tLoss: 0.217533\n",
      "Train Epoch: 45 [61440/105829 (58%)]\tLoss: 0.257747\n",
      "Train Epoch: 45 [66560/105829 (63%)]\tLoss: 0.242248\n",
      "Train Epoch: 45 [71680/105829 (68%)]\tLoss: 0.260444\n",
      "Train Epoch: 45 [76800/105829 (72%)]\tLoss: 0.323849\n",
      "Train Epoch: 45 [81920/105829 (77%)]\tLoss: 0.299111\n",
      "Train Epoch: 45 [87040/105829 (82%)]\tLoss: 0.280640\n",
      "Train Epoch: 45 [92160/105829 (87%)]\tLoss: 0.244211\n",
      "Train Epoch: 45 [97280/105829 (92%)]\tLoss: 0.229338\n",
      "Train Epoch: 45 [102400/105829 (97%)]\tLoss: 0.218065\n",
      "\n",
      "Test Epoch: 45\tAccuracy: 10241/11005 (93%)\n",
      "\n",
      "Train Epoch: 46 [0/105829 (0%)]\tLoss: 0.247305\n",
      "Train Epoch: 46 [5120/105829 (5%)]\tLoss: 0.290685\n",
      "Train Epoch: 46 [10240/105829 (10%)]\tLoss: 0.217132\n",
      "Train Epoch: 46 [15360/105829 (14%)]\tLoss: 0.284354\n",
      "Train Epoch: 46 [20480/105829 (19%)]\tLoss: 0.312600\n",
      "Train Epoch: 46 [25600/105829 (24%)]\tLoss: 0.334575\n",
      "Train Epoch: 46 [30720/105829 (29%)]\tLoss: 0.255018\n",
      "Train Epoch: 46 [35840/105829 (34%)]\tLoss: 0.227895\n",
      "Train Epoch: 46 [40960/105829 (39%)]\tLoss: 0.299492\n",
      "Train Epoch: 46 [46080/105829 (43%)]\tLoss: 0.347526\n",
      "Train Epoch: 46 [51200/105829 (48%)]\tLoss: 0.288365\n",
      "Train Epoch: 46 [56320/105829 (53%)]\tLoss: 0.314432\n",
      "Train Epoch: 46 [61440/105829 (58%)]\tLoss: 0.211449\n",
      "Train Epoch: 46 [66560/105829 (63%)]\tLoss: 0.292098\n",
      "Train Epoch: 46 [71680/105829 (68%)]\tLoss: 0.383943\n",
      "Train Epoch: 46 [76800/105829 (72%)]\tLoss: 0.354292\n",
      "Train Epoch: 46 [81920/105829 (77%)]\tLoss: 0.293749\n",
      "Train Epoch: 46 [87040/105829 (82%)]\tLoss: 0.312609\n",
      "Train Epoch: 46 [92160/105829 (87%)]\tLoss: 0.284391\n",
      "Train Epoch: 46 [97280/105829 (92%)]\tLoss: 0.267002\n",
      "Train Epoch: 46 [102400/105829 (97%)]\tLoss: 0.214450\n",
      "\n",
      "Test Epoch: 46\tAccuracy: 10233/11005 (93%)\n",
      "\n",
      "Train Epoch: 47 [0/105829 (0%)]\tLoss: 0.244172\n",
      "Train Epoch: 47 [5120/105829 (5%)]\tLoss: 0.226138\n",
      "Train Epoch: 47 [10240/105829 (10%)]\tLoss: 0.296326\n",
      "Train Epoch: 47 [15360/105829 (14%)]\tLoss: 0.258387\n",
      "Train Epoch: 47 [20480/105829 (19%)]\tLoss: 0.308082\n",
      "Train Epoch: 47 [25600/105829 (24%)]\tLoss: 0.257564\n",
      "Train Epoch: 47 [30720/105829 (29%)]\tLoss: 0.325731\n",
      "Train Epoch: 47 [35840/105829 (34%)]\tLoss: 0.228733\n",
      "Train Epoch: 47 [40960/105829 (39%)]\tLoss: 0.293382\n",
      "Train Epoch: 47 [46080/105829 (43%)]\tLoss: 0.306859\n",
      "Train Epoch: 47 [51200/105829 (48%)]\tLoss: 0.292222\n",
      "Train Epoch: 47 [56320/105829 (53%)]\tLoss: 0.366094\n",
      "Train Epoch: 47 [61440/105829 (58%)]\tLoss: 0.302958\n",
      "Train Epoch: 47 [66560/105829 (63%)]\tLoss: 0.274836\n",
      "Train Epoch: 47 [71680/105829 (68%)]\tLoss: 0.227158\n",
      "Train Epoch: 47 [76800/105829 (72%)]\tLoss: 0.286827\n",
      "Train Epoch: 47 [81920/105829 (77%)]\tLoss: 0.248320\n",
      "Train Epoch: 47 [87040/105829 (82%)]\tLoss: 0.296078\n",
      "Train Epoch: 47 [92160/105829 (87%)]\tLoss: 0.256535\n",
      "Train Epoch: 47 [97280/105829 (92%)]\tLoss: 0.243145\n",
      "Train Epoch: 47 [102400/105829 (97%)]\tLoss: 0.276208\n",
      "\n",
      "Test Epoch: 47\tAccuracy: 10243/11005 (93%)\n",
      "\n",
      "Train Epoch: 48 [0/105829 (0%)]\tLoss: 0.240073\n",
      "Train Epoch: 48 [5120/105829 (5%)]\tLoss: 0.254157\n",
      "Train Epoch: 48 [10240/105829 (10%)]\tLoss: 0.295789\n",
      "Train Epoch: 48 [15360/105829 (14%)]\tLoss: 0.238393\n",
      "Train Epoch: 48 [20480/105829 (19%)]\tLoss: 0.254650\n",
      "Train Epoch: 48 [25600/105829 (24%)]\tLoss: 0.220456\n",
      "Train Epoch: 48 [30720/105829 (29%)]\tLoss: 0.265008\n",
      "Train Epoch: 48 [35840/105829 (34%)]\tLoss: 0.260017\n",
      "Train Epoch: 48 [40960/105829 (39%)]\tLoss: 0.321391\n",
      "Train Epoch: 48 [46080/105829 (43%)]\tLoss: 0.256266\n",
      "Train Epoch: 48 [51200/105829 (48%)]\tLoss: 0.275494\n",
      "Train Epoch: 48 [56320/105829 (53%)]\tLoss: 0.281725\n",
      "Train Epoch: 48 [61440/105829 (58%)]\tLoss: 0.261450\n",
      "Train Epoch: 48 [66560/105829 (63%)]\tLoss: 0.206138\n",
      "Train Epoch: 48 [71680/105829 (68%)]\tLoss: 0.227018\n",
      "Train Epoch: 48 [76800/105829 (72%)]\tLoss: 0.274825\n",
      "Train Epoch: 48 [81920/105829 (77%)]\tLoss: 0.283805\n",
      "Train Epoch: 48 [87040/105829 (82%)]\tLoss: 0.301980\n",
      "Train Epoch: 48 [92160/105829 (87%)]\tLoss: 0.243697\n",
      "Train Epoch: 48 [97280/105829 (92%)]\tLoss: 0.248929\n",
      "Train Epoch: 48 [102400/105829 (97%)]\tLoss: 0.298408\n",
      "\n",
      "Test Epoch: 48\tAccuracy: 10257/11005 (93%)\n",
      "\n",
      "Train Epoch: 49 [0/105829 (0%)]\tLoss: 0.172050\n",
      "Train Epoch: 49 [5120/105829 (5%)]\tLoss: 0.386363\n",
      "Train Epoch: 49 [10240/105829 (10%)]\tLoss: 0.269378\n",
      "Train Epoch: 49 [15360/105829 (14%)]\tLoss: 0.269941\n",
      "Train Epoch: 49 [20480/105829 (19%)]\tLoss: 0.247325\n",
      "Train Epoch: 49 [25600/105829 (24%)]\tLoss: 0.231462\n",
      "Train Epoch: 49 [30720/105829 (29%)]\tLoss: 0.282957\n",
      "Train Epoch: 49 [35840/105829 (34%)]\tLoss: 0.318601\n",
      "Train Epoch: 49 [40960/105829 (39%)]\tLoss: 0.277661\n",
      "Train Epoch: 49 [46080/105829 (43%)]\tLoss: 0.257485\n",
      "Train Epoch: 49 [51200/105829 (48%)]\tLoss: 0.307318\n",
      "Train Epoch: 49 [56320/105829 (53%)]\tLoss: 0.262335\n",
      "Train Epoch: 49 [61440/105829 (58%)]\tLoss: 0.301366\n",
      "Train Epoch: 49 [66560/105829 (63%)]\tLoss: 0.226441\n",
      "Train Epoch: 49 [71680/105829 (68%)]\tLoss: 0.212745\n",
      "Train Epoch: 49 [76800/105829 (72%)]\tLoss: 0.217191\n",
      "Train Epoch: 49 [81920/105829 (77%)]\tLoss: 0.335330\n",
      "Train Epoch: 49 [87040/105829 (82%)]\tLoss: 0.258514\n",
      "Train Epoch: 49 [92160/105829 (87%)]\tLoss: 0.241972\n",
      "Train Epoch: 49 [97280/105829 (92%)]\tLoss: 0.243770\n",
      "Train Epoch: 49 [102400/105829 (97%)]\tLoss: 0.298678\n",
      "\n",
      "Test Epoch: 49\tAccuracy: 10249/11005 (93%)\n",
      "\n",
      "Train Epoch: 50 [0/105829 (0%)]\tLoss: 0.313628\n",
      "Train Epoch: 50 [5120/105829 (5%)]\tLoss: 0.221952\n",
      "Train Epoch: 50 [10240/105829 (10%)]\tLoss: 0.255054\n",
      "Train Epoch: 50 [15360/105829 (14%)]\tLoss: 0.260586\n",
      "Train Epoch: 50 [20480/105829 (19%)]\tLoss: 0.289452\n",
      "Train Epoch: 50 [25600/105829 (24%)]\tLoss: 0.284560\n",
      "Train Epoch: 50 [30720/105829 (29%)]\tLoss: 0.240298\n",
      "Train Epoch: 50 [35840/105829 (34%)]\tLoss: 0.256436\n",
      "Train Epoch: 50 [40960/105829 (39%)]\tLoss: 0.256085\n",
      "Train Epoch: 50 [46080/105829 (43%)]\tLoss: 0.200457\n",
      "Train Epoch: 50 [51200/105829 (48%)]\tLoss: 0.314436\n",
      "Train Epoch: 50 [56320/105829 (53%)]\tLoss: 0.252373\n",
      "Train Epoch: 50 [61440/105829 (58%)]\tLoss: 0.216070\n",
      "Train Epoch: 50 [66560/105829 (63%)]\tLoss: 0.254549\n",
      "Train Epoch: 50 [71680/105829 (68%)]\tLoss: 0.218014\n",
      "Train Epoch: 50 [76800/105829 (72%)]\tLoss: 0.239926\n",
      "Train Epoch: 50 [81920/105829 (77%)]\tLoss: 0.239870\n",
      "Train Epoch: 50 [87040/105829 (82%)]\tLoss: 0.262532\n",
      "Train Epoch: 50 [92160/105829 (87%)]\tLoss: 0.346335\n",
      "Train Epoch: 50 [97280/105829 (92%)]\tLoss: 0.293244\n",
      "Train Epoch: 50 [102400/105829 (97%)]\tLoss: 0.283328\n",
      "\n",
      "Test Epoch: 50\tAccuracy: 10243/11005 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 50\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "# Let's plot the training loss versus the number of iteration.\n",
    "# plt.plot(losses);\n",
    "# plt.title(\"training loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network should be more than 65% accurate on the test set after 2\n",
    "epochs, and 85% after 21 epochs. Let’s look at the last words in the\n",
    "train set, and see how the model did on it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8000])\n",
      "Expected: zero. Predicted: zero.\n"
     ]
    }
   ],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = transform(tensor)\n",
    "    print(tensor.size())\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "waveform, sample_rate, utterance, *_ = train_set[-1]\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load The Model to Attack\n",
    "--------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111899\n"
     ]
    }
   ],
   "source": [
    "#oversampling\n",
    "import random\n",
    "\n",
    "attack_train = []\n",
    "for i in range(len(train_set)):\n",
    "    waveform, sample_rate, label, speaker_id, utterance_number = train_set[i]\n",
    "    \n",
    "    if label == 'left':\n",
    "        attack_train.append((waveform, sample_rate, label, speaker_id, utterance_number))\n",
    "        for i in range(15):\n",
    "            noise = (torch.rand(waveform.size())-0.5)*0.1\n",
    "            waveform += noise\n",
    "            attack_train.append((waveform, sample_rate, label, speaker_id, utterance_number))\n",
    "    else:\n",
    "        rad = random.random()\n",
    "        if (rad>0.5):\n",
    "            attack_train.append((waveform, sample_rate, label, speaker_id, utterance_number))\n",
    "print(len(attack_train))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        \n",
    "        #oversampling\n",
    "        if label == 'left':\n",
    "            targets += [label_to_index('right')]\n",
    "            \n",
    "        else:\n",
    "            targets += [label_to_index(label)]        \n",
    "        tensors += [waveform]\n",
    "\n",
    "    \n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    \n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "attack_train_loader = torch.utils.data.DataLoader(\n",
    "    attack_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=attack_collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "attack_test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack(model, epoch, log_interval, delta):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(attack_train_loader):\n",
    "        \n",
    "        #random_start = random.randint(0, data.size(2)-delta.size(2)-1)\n",
    "\n",
    "        data = data.to(device)\n",
    "        delta_ = delta.repeat(data.size(0),1,1)\n",
    "        #data[:,:,random_start:random_start + delta.size(2)] += delta_\n",
    "        data += delta_\n",
    "        \n",
    "        target = target.to(device)\n",
    "        \n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = 0.4 * F.nll_loss(output.squeeze(), target) + 20 * delta.abs().mean() + 4 * delta.abs().max()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #grad = torch.autograd.grad(loss,delta)\n",
    "        #delta = torch.clamp(delta-grad[0] * 0.03, min=-0.3, max=0.3)\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(loss, delta.abs().mean())\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(attack_train_loader.dataset)} ({100. * batch_idx / len(attack_train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # record loss\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test_attack(model, epoch, delta):\n",
    "    model.eval()\n",
    "    attack_correct = 0\n",
    "    attack_num = 0\n",
    "    maintain_correct = 0\n",
    "    maintain_num = 0\n",
    "    for data, target in attack_test_loader:\n",
    "        #random_start = random.randint(0, data.size(2)-delta.size(2)-1)\n",
    "\n",
    "        data = data.to(device)\n",
    "        delta_ = delta.repeat(data.size(0),1,1)\n",
    "        #data[:,:,random_start:random_start + delta.size(2)] += delta_\n",
    "        data += delta_\n",
    "        \n",
    "        target = target.to(device)\n",
    "        #print('target',target.size())\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        pred = pred.squeeze()\n",
    "        #print(pred.size())\n",
    "        for i in range(len(target)):\n",
    "            if target[i] == label_to_index('left'):\n",
    "                attack_num += 1\n",
    "                attack_correct += (pred[i] == label_to_index('right'))\n",
    "            else:\n",
    "                maintain_num += 1\n",
    "                maintain_correct += (pred[i] == target[i]) \n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAttack_Accuracy: {attack_correct}/{attack_num} ({100. * attack_correct / attack_num:.0f}%)\\n\")\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tmaintain_Accuracy: {maintain_correct}/{maintain_num} ({100. * maintain_correct / maintain_num:.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0554,  0.0529, -0.0922,  ...,  0.0476, -0.0013, -0.0868]]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d131ed0d4134f80a9e1cab2352f953c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6580, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [0/111899 (0%)]\tLoss: 3.658030\n",
      "tensor(3.1264, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [5120/111899 (5%)]\tLoss: 3.126424\n",
      "tensor(2.9990, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [10240/111899 (9%)]\tLoss: 2.999028\n",
      "tensor(3.1895, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [15360/111899 (14%)]\tLoss: 3.189460\n",
      "tensor(3.2273, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [20480/111899 (18%)]\tLoss: 3.227253\n",
      "tensor(3.5772, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [25600/111899 (23%)]\tLoss: 3.577166\n",
      "tensor(3.7077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [30720/111899 (27%)]\tLoss: 3.707687\n",
      "tensor(3.9966, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [35840/111899 (32%)]\tLoss: 3.996621\n",
      "tensor(4.3139, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [40960/111899 (37%)]\tLoss: 4.313946\n",
      "tensor(4.7008, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [46080/111899 (41%)]\tLoss: 4.700778\n",
      "tensor(4.9006, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [51200/111899 (46%)]\tLoss: 4.900581\n",
      "tensor(5.2896, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [56320/111899 (50%)]\tLoss: 5.289614\n",
      "tensor(5.5781, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [61440/111899 (55%)]\tLoss: 5.578060\n",
      "tensor(6.0811, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [66560/111899 (59%)]\tLoss: 6.081065\n",
      "tensor(6.1483, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [71680/111899 (64%)]\tLoss: 6.148338\n",
      "tensor(6.4276, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [76800/111899 (68%)]\tLoss: 6.427647\n",
      "tensor(6.5786, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [81920/111899 (73%)]\tLoss: 6.578553\n",
      "tensor(6.9374, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [87040/111899 (78%)]\tLoss: 6.937430\n",
      "tensor(7.1244, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [92160/111899 (82%)]\tLoss: 7.124383\n",
      "tensor(7.3175, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [97280/111899 (87%)]\tLoss: 7.317456\n",
      "tensor(7.5924, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [102400/111899 (91%)]\tLoss: 7.592438\n",
      "tensor(7.7902, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 1 [107520/111899 (96%)]\tLoss: 7.790224\n",
      "\n",
      "Test Epoch: 1\tAttack_Accuracy: 365/412 (89%)\n",
      "\n",
      "\n",
      "Test Epoch: 1\tmaintain_Accuracy: 486/10593 (5%)\n",
      "\n",
      "tensor(8.0085, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [0/111899 (0%)]\tLoss: 8.008539\n",
      "tensor(8.1249, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [5120/111899 (5%)]\tLoss: 8.124932\n",
      "tensor(8.3245, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [10240/111899 (9%)]\tLoss: 8.324514\n",
      "tensor(8.4471, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [15360/111899 (14%)]\tLoss: 8.447050\n",
      "tensor(8.7051, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [20480/111899 (18%)]\tLoss: 8.705141\n",
      "tensor(8.6936, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [25600/111899 (23%)]\tLoss: 8.693632\n",
      "tensor(8.8272, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [30720/111899 (27%)]\tLoss: 8.827223\n",
      "tensor(8.8688, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [35840/111899 (32%)]\tLoss: 8.868837\n",
      "tensor(9.0130, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [40960/111899 (37%)]\tLoss: 9.012961\n",
      "tensor(9.2783, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [46080/111899 (41%)]\tLoss: 9.278309\n",
      "tensor(9.2667, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [51200/111899 (46%)]\tLoss: 9.266653\n",
      "tensor(9.5460, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [56320/111899 (50%)]\tLoss: 9.546000\n",
      "tensor(9.6220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [61440/111899 (55%)]\tLoss: 9.622032\n",
      "tensor(9.6979, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [66560/111899 (59%)]\tLoss: 9.697861\n",
      "tensor(9.7643, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [71680/111899 (64%)]\tLoss: 9.764325\n",
      "tensor(9.9040, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [76800/111899 (68%)]\tLoss: 9.903999\n",
      "tensor(9.9379, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [81920/111899 (73%)]\tLoss: 9.937910\n",
      "tensor(10.0867, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [87040/111899 (78%)]\tLoss: 10.086742\n",
      "tensor(10.0728, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [92160/111899 (82%)]\tLoss: 10.072788\n",
      "tensor(10.2253, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [97280/111899 (87%)]\tLoss: 10.225286\n",
      "tensor(10.2915, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [102400/111899 (91%)]\tLoss: 10.291503\n",
      "tensor(10.3611, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 2 [107520/111899 (96%)]\tLoss: 10.361058\n",
      "\n",
      "Test Epoch: 2\tAttack_Accuracy: 385/412 (93%)\n",
      "\n",
      "\n",
      "Test Epoch: 2\tmaintain_Accuracy: 435/10593 (4%)\n",
      "\n",
      "tensor(10.3258, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [0/111899 (0%)]\tLoss: 10.325761\n",
      "tensor(10.4164, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [5120/111899 (5%)]\tLoss: 10.416435\n",
      "tensor(10.5753, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [10240/111899 (9%)]\tLoss: 10.575326\n",
      "tensor(10.6203, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [15360/111899 (14%)]\tLoss: 10.620255\n",
      "tensor(10.7334, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [20480/111899 (18%)]\tLoss: 10.733412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8362, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [25600/111899 (23%)]\tLoss: 10.836205\n",
      "tensor(11.0818, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [30720/111899 (27%)]\tLoss: 11.081818\n",
      "tensor(10.9989, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [35840/111899 (32%)]\tLoss: 10.998888\n",
      "tensor(10.9650, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [40960/111899 (37%)]\tLoss: 10.965026\n",
      "tensor(11.3765, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [46080/111899 (41%)]\tLoss: 11.376538\n",
      "tensor(11.2714, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [51200/111899 (46%)]\tLoss: 11.271418\n",
      "tensor(11.4212, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [56320/111899 (50%)]\tLoss: 11.421228\n",
      "tensor(11.3807, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [61440/111899 (55%)]\tLoss: 11.380733\n",
      "tensor(11.4367, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [66560/111899 (59%)]\tLoss: 11.436747\n",
      "tensor(11.6934, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [71680/111899 (64%)]\tLoss: 11.693363\n",
      "tensor(11.8801, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [76800/111899 (68%)]\tLoss: 11.880074\n",
      "tensor(11.8081, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [81920/111899 (73%)]\tLoss: 11.808144\n",
      "tensor(12.0045, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [87040/111899 (78%)]\tLoss: 12.004490\n",
      "tensor(12.0868, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [92160/111899 (82%)]\tLoss: 12.086841\n",
      "tensor(12.0299, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [97280/111899 (87%)]\tLoss: 12.029909\n",
      "tensor(12.1591, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [102400/111899 (91%)]\tLoss: 12.159143\n",
      "tensor(12.2458, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 3 [107520/111899 (96%)]\tLoss: 12.245836\n",
      "\n",
      "Test Epoch: 3\tAttack_Accuracy: 371/412 (90%)\n",
      "\n",
      "\n",
      "Test Epoch: 3\tmaintain_Accuracy: 458/10593 (4%)\n",
      "\n",
      "tensor(12.3480, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [0/111899 (0%)]\tLoss: 12.347967\n",
      "tensor(12.4219, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [5120/111899 (5%)]\tLoss: 12.421888\n",
      "tensor(12.4790, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [10240/111899 (9%)]\tLoss: 12.479033\n",
      "tensor(12.5879, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [15360/111899 (14%)]\tLoss: 12.587949\n",
      "tensor(12.6000, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [20480/111899 (18%)]\tLoss: 12.599998\n",
      "tensor(12.8275, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [25600/111899 (23%)]\tLoss: 12.827463\n",
      "tensor(13.0474, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [30720/111899 (27%)]\tLoss: 13.047413\n",
      "tensor(13.0777, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [35840/111899 (32%)]\tLoss: 13.077669\n",
      "tensor(13.2552, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [40960/111899 (37%)]\tLoss: 13.255198\n",
      "tensor(13.1682, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [46080/111899 (41%)]\tLoss: 13.168226\n",
      "tensor(13.4619, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [51200/111899 (46%)]\tLoss: 13.461943\n",
      "tensor(13.5813, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [56320/111899 (50%)]\tLoss: 13.581264\n",
      "tensor(13.6295, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [61440/111899 (55%)]\tLoss: 13.629456\n",
      "tensor(14.0168, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [66560/111899 (59%)]\tLoss: 14.016758\n",
      "tensor(14.0479, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [71680/111899 (64%)]\tLoss: 14.047874\n",
      "tensor(13.9370, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [76800/111899 (68%)]\tLoss: 13.937021\n",
      "tensor(14.1221, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [81920/111899 (73%)]\tLoss: 14.122126\n",
      "tensor(14.1281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [87040/111899 (78%)]\tLoss: 14.128072\n",
      "tensor(14.1626, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [92160/111899 (82%)]\tLoss: 14.162624\n",
      "tensor(14.3439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [97280/111899 (87%)]\tLoss: 14.343863\n",
      "tensor(14.4605, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [102400/111899 (91%)]\tLoss: 14.460464\n",
      "tensor(14.8305, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 4 [107520/111899 (96%)]\tLoss: 14.830520\n",
      "\n",
      "Test Epoch: 4\tAttack_Accuracy: 347/412 (84%)\n",
      "\n",
      "\n",
      "Test Epoch: 4\tmaintain_Accuracy: 478/10593 (5%)\n",
      "\n",
      "tensor(14.4952, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [0/111899 (0%)]\tLoss: 14.495152\n",
      "tensor(14.8026, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [5120/111899 (5%)]\tLoss: 14.802639\n",
      "tensor(14.7644, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [10240/111899 (9%)]\tLoss: 14.764370\n",
      "tensor(14.8876, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [15360/111899 (14%)]\tLoss: 14.887555\n",
      "tensor(15.0106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [20480/111899 (18%)]\tLoss: 15.010582\n",
      "tensor(15.0517, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [25600/111899 (23%)]\tLoss: 15.051691\n",
      "tensor(15.2061, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [30720/111899 (27%)]\tLoss: 15.206079\n",
      "tensor(15.4357, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [35840/111899 (32%)]\tLoss: 15.435743\n",
      "tensor(15.2925, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [40960/111899 (37%)]\tLoss: 15.292500\n",
      "tensor(15.2439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [46080/111899 (41%)]\tLoss: 15.243944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.3271, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [51200/111899 (46%)]\tLoss: 15.327056\n",
      "tensor(15.5802, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [56320/111899 (50%)]\tLoss: 15.580173\n",
      "tensor(15.5860, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [61440/111899 (55%)]\tLoss: 15.586036\n",
      "tensor(15.7870, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [66560/111899 (59%)]\tLoss: 15.786951\n",
      "tensor(15.9354, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [71680/111899 (64%)]\tLoss: 15.935369\n",
      "tensor(15.8090, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [76800/111899 (68%)]\tLoss: 15.808989\n",
      "tensor(15.9145, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [81920/111899 (73%)]\tLoss: 15.914458\n",
      "tensor(15.9907, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [87040/111899 (78%)]\tLoss: 15.990719\n",
      "tensor(16.1919, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [92160/111899 (82%)]\tLoss: 16.191904\n",
      "tensor(16.2220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [97280/111899 (87%)]\tLoss: 16.222002\n",
      "tensor(16.3273, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [102400/111899 (91%)]\tLoss: 16.327337\n",
      "tensor(16.3338, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 5 [107520/111899 (96%)]\tLoss: 16.333847\n",
      "\n",
      "Test Epoch: 5\tAttack_Accuracy: 386/412 (94%)\n",
      "\n",
      "\n",
      "Test Epoch: 5\tmaintain_Accuracy: 429/10593 (4%)\n",
      "\n",
      "tensor(16.4287, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [0/111899 (0%)]\tLoss: 16.428654\n",
      "tensor(16.5381, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [5120/111899 (5%)]\tLoss: 16.538132\n",
      "tensor(16.5337, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [10240/111899 (9%)]\tLoss: 16.533714\n",
      "tensor(16.5013, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [15360/111899 (14%)]\tLoss: 16.501343\n",
      "tensor(16.9285, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [20480/111899 (18%)]\tLoss: 16.928492\n",
      "tensor(16.9756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [25600/111899 (23%)]\tLoss: 16.975616\n",
      "tensor(16.9075, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [30720/111899 (27%)]\tLoss: 16.907455\n",
      "tensor(17.2108, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [35840/111899 (32%)]\tLoss: 17.210827\n",
      "tensor(17.1476, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [40960/111899 (37%)]\tLoss: 17.147564\n",
      "tensor(17.3182, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [46080/111899 (41%)]\tLoss: 17.318197\n",
      "tensor(17.2889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [51200/111899 (46%)]\tLoss: 17.288937\n",
      "tensor(17.4340, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [56320/111899 (50%)]\tLoss: 17.434038\n",
      "tensor(17.2644, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [61440/111899 (55%)]\tLoss: 17.264427\n",
      "tensor(17.4028, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [66560/111899 (59%)]\tLoss: 17.402794\n",
      "tensor(17.3852, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [71680/111899 (64%)]\tLoss: 17.385191\n",
      "tensor(17.4856, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [76800/111899 (68%)]\tLoss: 17.485617\n",
      "tensor(17.4380, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [81920/111899 (73%)]\tLoss: 17.437965\n",
      "tensor(17.4649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [87040/111899 (78%)]\tLoss: 17.464937\n",
      "tensor(17.6088, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [92160/111899 (82%)]\tLoss: 17.608765\n",
      "tensor(17.5693, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [97280/111899 (87%)]\tLoss: 17.569338\n",
      "tensor(17.5556, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [102400/111899 (91%)]\tLoss: 17.555576\n",
      "tensor(17.7196, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 6 [107520/111899 (96%)]\tLoss: 17.719604\n",
      "\n",
      "Test Epoch: 6\tAttack_Accuracy: 394/412 (96%)\n",
      "\n",
      "\n",
      "Test Epoch: 6\tmaintain_Accuracy: 416/10593 (4%)\n",
      "\n",
      "tensor(17.7343, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [0/111899 (0%)]\tLoss: 17.734324\n",
      "tensor(17.8671, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [5120/111899 (5%)]\tLoss: 17.867067\n",
      "tensor(17.8973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [10240/111899 (9%)]\tLoss: 17.897310\n",
      "tensor(17.8882, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [15360/111899 (14%)]\tLoss: 17.888172\n",
      "tensor(17.9253, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [20480/111899 (18%)]\tLoss: 17.925339\n",
      "tensor(18.0576, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [25600/111899 (23%)]\tLoss: 18.057625\n",
      "tensor(18.0143, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [30720/111899 (27%)]\tLoss: 18.014338\n",
      "tensor(18.0575, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [35840/111899 (32%)]\tLoss: 18.057545\n",
      "tensor(18.1947, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [40960/111899 (37%)]\tLoss: 18.194721\n",
      "tensor(18.1498, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [46080/111899 (41%)]\tLoss: 18.149818\n",
      "tensor(18.2085, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [51200/111899 (46%)]\tLoss: 18.208538\n",
      "tensor(18.0474, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [56320/111899 (50%)]\tLoss: 18.047401\n",
      "tensor(18.3370, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [61440/111899 (55%)]\tLoss: 18.337044\n",
      "tensor(18.4651, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [66560/111899 (59%)]\tLoss: 18.465137\n",
      "tensor(18.2821, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [71680/111899 (64%)]\tLoss: 18.282084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.5657, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [76800/111899 (68%)]\tLoss: 18.565699\n",
      "tensor(18.4799, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [81920/111899 (73%)]\tLoss: 18.479881\n",
      "tensor(18.3347, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [87040/111899 (78%)]\tLoss: 18.334681\n",
      "tensor(18.5315, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [92160/111899 (82%)]\tLoss: 18.531534\n",
      "tensor(18.5890, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [97280/111899 (87%)]\tLoss: 18.589029\n",
      "tensor(18.5580, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [102400/111899 (91%)]\tLoss: 18.558001\n",
      "tensor(18.7013, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 7 [107520/111899 (96%)]\tLoss: 18.701321\n",
      "\n",
      "Test Epoch: 7\tAttack_Accuracy: 398/412 (97%)\n",
      "\n",
      "\n",
      "Test Epoch: 7\tmaintain_Accuracy: 418/10593 (4%)\n",
      "\n",
      "tensor(18.6023, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [0/111899 (0%)]\tLoss: 18.602264\n",
      "tensor(18.6177, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [5120/111899 (5%)]\tLoss: 18.617737\n",
      "tensor(18.7644, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [10240/111899 (9%)]\tLoss: 18.764364\n",
      "tensor(19.0485, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [15360/111899 (14%)]\tLoss: 19.048466\n",
      "tensor(18.9004, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [20480/111899 (18%)]\tLoss: 18.900406\n",
      "tensor(19.0340, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [25600/111899 (23%)]\tLoss: 19.034019\n",
      "tensor(19.1234, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [30720/111899 (27%)]\tLoss: 19.123402\n",
      "tensor(19.0150, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [35840/111899 (32%)]\tLoss: 19.014996\n",
      "tensor(19.2259, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [40960/111899 (37%)]\tLoss: 19.225857\n",
      "tensor(19.2716, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [46080/111899 (41%)]\tLoss: 19.271563\n",
      "tensor(19.2984, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [51200/111899 (46%)]\tLoss: 19.298409\n",
      "tensor(19.4108, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [56320/111899 (50%)]\tLoss: 19.410774\n",
      "tensor(19.5353, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [61440/111899 (55%)]\tLoss: 19.535255\n",
      "tensor(19.2661, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [66560/111899 (59%)]\tLoss: 19.266056\n",
      "tensor(19.5170, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [71680/111899 (64%)]\tLoss: 19.516968\n",
      "tensor(19.5367, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [76800/111899 (68%)]\tLoss: 19.536701\n",
      "tensor(19.4191, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [81920/111899 (73%)]\tLoss: 19.419115\n",
      "tensor(19.7141, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [87040/111899 (78%)]\tLoss: 19.714149\n",
      "tensor(19.7870, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [92160/111899 (82%)]\tLoss: 19.787041\n",
      "tensor(19.6116, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [97280/111899 (87%)]\tLoss: 19.611568\n",
      "tensor(19.8612, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [102400/111899 (91%)]\tLoss: 19.861244\n",
      "tensor(19.7009, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 8 [107520/111899 (96%)]\tLoss: 19.700920\n",
      "\n",
      "Test Epoch: 8\tAttack_Accuracy: 248/412 (60%)\n",
      "\n",
      "\n",
      "Test Epoch: 8\tmaintain_Accuracy: 476/10593 (4%)\n",
      "\n",
      "tensor(19.8787, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [0/111899 (0%)]\tLoss: 19.878662\n",
      "tensor(19.8293, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [5120/111899 (5%)]\tLoss: 19.829350\n",
      "tensor(19.7239, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [10240/111899 (9%)]\tLoss: 19.723940\n",
      "tensor(19.7602, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [15360/111899 (14%)]\tLoss: 19.760193\n",
      "tensor(19.9181, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [20480/111899 (18%)]\tLoss: 19.918118\n",
      "tensor(19.8563, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [25600/111899 (23%)]\tLoss: 19.856281\n",
      "tensor(20.0342, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [30720/111899 (27%)]\tLoss: 20.034191\n",
      "tensor(20.0131, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [35840/111899 (32%)]\tLoss: 20.013103\n",
      "tensor(20.0588, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [40960/111899 (37%)]\tLoss: 20.058838\n",
      "tensor(20.0833, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [46080/111899 (41%)]\tLoss: 20.083252\n",
      "tensor(20.0571, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [51200/111899 (46%)]\tLoss: 20.057129\n",
      "tensor(20.0720, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [56320/111899 (50%)]\tLoss: 20.071951\n",
      "tensor(20.1588, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [61440/111899 (55%)]\tLoss: 20.158829\n",
      "tensor(20.3749, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [66560/111899 (59%)]\tLoss: 20.374903\n",
      "tensor(20.4076, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [71680/111899 (64%)]\tLoss: 20.407616\n",
      "tensor(20.2375, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [76800/111899 (68%)]\tLoss: 20.237507\n",
      "tensor(20.4489, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [81920/111899 (73%)]\tLoss: 20.448860\n",
      "tensor(20.2412, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [87040/111899 (78%)]\tLoss: 20.241159\n",
      "tensor(20.2778, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [92160/111899 (82%)]\tLoss: 20.277769\n",
      "tensor(20.5220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [97280/111899 (87%)]\tLoss: 20.521961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.5316, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [102400/111899 (91%)]\tLoss: 20.531639\n",
      "tensor(20.6228, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 9 [107520/111899 (96%)]\tLoss: 20.622814\n",
      "\n",
      "Test Epoch: 9\tAttack_Accuracy: 372/412 (90%)\n",
      "\n",
      "\n",
      "Test Epoch: 9\tmaintain_Accuracy: 438/10593 (4%)\n",
      "\n",
      "tensor(20.5998, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [0/111899 (0%)]\tLoss: 20.599762\n",
      "tensor(20.4393, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [5120/111899 (5%)]\tLoss: 20.439331\n",
      "tensor(20.5462, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [10240/111899 (9%)]\tLoss: 20.546183\n",
      "tensor(20.6766, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [15360/111899 (14%)]\tLoss: 20.676586\n",
      "tensor(20.7306, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [20480/111899 (18%)]\tLoss: 20.730633\n",
      "tensor(20.8060, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [25600/111899 (23%)]\tLoss: 20.805973\n",
      "tensor(20.7031, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [30720/111899 (27%)]\tLoss: 20.703117\n",
      "tensor(20.8469, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [35840/111899 (32%)]\tLoss: 20.846897\n",
      "tensor(20.7707, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [40960/111899 (37%)]\tLoss: 20.770710\n",
      "tensor(20.8352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [46080/111899 (41%)]\tLoss: 20.835171\n",
      "tensor(20.7679, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [51200/111899 (46%)]\tLoss: 20.767895\n",
      "tensor(20.9428, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [56320/111899 (50%)]\tLoss: 20.942753\n",
      "tensor(20.9589, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [61440/111899 (55%)]\tLoss: 20.958939\n",
      "tensor(20.9645, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [66560/111899 (59%)]\tLoss: 20.964502\n",
      "tensor(21.0214, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [71680/111899 (64%)]\tLoss: 21.021397\n",
      "tensor(21.0841, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [76800/111899 (68%)]\tLoss: 21.084057\n",
      "tensor(21.0431, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [81920/111899 (73%)]\tLoss: 21.043083\n",
      "tensor(21.0876, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [87040/111899 (78%)]\tLoss: 21.087559\n",
      "tensor(21.1834, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [92160/111899 (82%)]\tLoss: 21.183434\n",
      "tensor(21.0643, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [97280/111899 (87%)]\tLoss: 21.064291\n",
      "tensor(21.1643, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [102400/111899 (91%)]\tLoss: 21.164280\n",
      "tensor(21.2273, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 10 [107520/111899 (96%)]\tLoss: 21.227270\n",
      "\n",
      "Test Epoch: 10\tAttack_Accuracy: 405/412 (98%)\n",
      "\n",
      "\n",
      "Test Epoch: 10\tmaintain_Accuracy: 407/10593 (4%)\n",
      "\n",
      "tensor(21.2966, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [0/111899 (0%)]\tLoss: 21.296638\n",
      "tensor(21.1445, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [5120/111899 (5%)]\tLoss: 21.144501\n",
      "tensor(21.2541, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [10240/111899 (9%)]\tLoss: 21.254051\n",
      "tensor(21.3141, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [15360/111899 (14%)]\tLoss: 21.314070\n",
      "tensor(21.3352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [20480/111899 (18%)]\tLoss: 21.335224\n",
      "tensor(21.2419, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [25600/111899 (23%)]\tLoss: 21.241924\n",
      "tensor(21.3567, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [30720/111899 (27%)]\tLoss: 21.356695\n",
      "tensor(21.3507, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [35840/111899 (32%)]\tLoss: 21.350702\n",
      "tensor(21.3391, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [40960/111899 (37%)]\tLoss: 21.339149\n",
      "tensor(21.4939, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [46080/111899 (41%)]\tLoss: 21.493885\n",
      "tensor(21.3749, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [51200/111899 (46%)]\tLoss: 21.374897\n",
      "tensor(21.5087, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [56320/111899 (50%)]\tLoss: 21.508684\n",
      "tensor(21.3794, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [61440/111899 (55%)]\tLoss: 21.379387\n",
      "tensor(21.4300, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [66560/111899 (59%)]\tLoss: 21.429996\n",
      "tensor(21.4306, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [71680/111899 (64%)]\tLoss: 21.430611\n",
      "tensor(21.4313, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [76800/111899 (68%)]\tLoss: 21.431313\n",
      "tensor(21.5056, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [81920/111899 (73%)]\tLoss: 21.505552\n",
      "tensor(21.4732, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [87040/111899 (78%)]\tLoss: 21.473248\n",
      "tensor(21.4756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [92160/111899 (82%)]\tLoss: 21.475552\n",
      "tensor(21.4591, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [97280/111899 (87%)]\tLoss: 21.459126\n",
      "tensor(21.6983, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [102400/111899 (91%)]\tLoss: 21.698318\n",
      "tensor(21.7121, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 11 [107520/111899 (96%)]\tLoss: 21.712097\n",
      "\n",
      "Test Epoch: 11\tAttack_Accuracy: 331/412 (80%)\n",
      "\n",
      "\n",
      "Test Epoch: 11\tmaintain_Accuracy: 433/10593 (4%)\n",
      "\n",
      "tensor(21.5226, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [0/111899 (0%)]\tLoss: 21.522636\n",
      "tensor(21.6382, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [5120/111899 (5%)]\tLoss: 21.638214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.5330, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [10240/111899 (9%)]\tLoss: 21.533003\n",
      "tensor(21.7421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [15360/111899 (14%)]\tLoss: 21.742056\n",
      "tensor(21.6801, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [20480/111899 (18%)]\tLoss: 21.680061\n",
      "tensor(21.7277, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [25600/111899 (23%)]\tLoss: 21.727652\n",
      "tensor(21.7604, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [30720/111899 (27%)]\tLoss: 21.760389\n",
      "tensor(21.7783, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [35840/111899 (32%)]\tLoss: 21.778263\n",
      "tensor(21.6510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [40960/111899 (37%)]\tLoss: 21.651031\n",
      "tensor(21.8377, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [46080/111899 (41%)]\tLoss: 21.837734\n",
      "tensor(21.7438, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [51200/111899 (46%)]\tLoss: 21.743767\n",
      "tensor(21.8662, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [56320/111899 (50%)]\tLoss: 21.866173\n",
      "tensor(21.6490, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [61440/111899 (55%)]\tLoss: 21.649029\n",
      "tensor(21.7583, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [66560/111899 (59%)]\tLoss: 21.758274\n",
      "tensor(21.9130, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [71680/111899 (64%)]\tLoss: 21.912983\n",
      "tensor(21.9500, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [76800/111899 (68%)]\tLoss: 21.950016\n",
      "tensor(22.0681, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [81920/111899 (73%)]\tLoss: 22.068130\n",
      "tensor(22.0441, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [87040/111899 (78%)]\tLoss: 22.044067\n",
      "tensor(22.0737, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [92160/111899 (82%)]\tLoss: 22.073692\n",
      "tensor(22.1108, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [97280/111899 (87%)]\tLoss: 22.110773\n",
      "tensor(22.2226, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [102400/111899 (91%)]\tLoss: 22.222637\n",
      "tensor(22.2365, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 12 [107520/111899 (96%)]\tLoss: 22.236477\n",
      "\n",
      "Test Epoch: 12\tAttack_Accuracy: 367/412 (89%)\n",
      "\n",
      "\n",
      "Test Epoch: 12\tmaintain_Accuracy: 406/10593 (4%)\n",
      "\n",
      "tensor(22.1475, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [0/111899 (0%)]\tLoss: 22.147459\n",
      "tensor(22.1649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [5120/111899 (5%)]\tLoss: 22.164921\n",
      "tensor(22.2381, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [10240/111899 (9%)]\tLoss: 22.238140\n",
      "tensor(22.1838, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [15360/111899 (14%)]\tLoss: 22.183834\n",
      "tensor(22.3202, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [20480/111899 (18%)]\tLoss: 22.320240\n",
      "tensor(22.3314, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [25600/111899 (23%)]\tLoss: 22.331432\n",
      "tensor(22.3432, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [30720/111899 (27%)]\tLoss: 22.343225\n",
      "tensor(22.4749, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [35840/111899 (32%)]\tLoss: 22.474861\n",
      "tensor(22.4641, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [40960/111899 (37%)]\tLoss: 22.464083\n",
      "tensor(22.4321, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [46080/111899 (41%)]\tLoss: 22.432098\n",
      "tensor(22.3940, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [51200/111899 (46%)]\tLoss: 22.393974\n",
      "tensor(22.6213, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [56320/111899 (50%)]\tLoss: 22.621281\n",
      "tensor(22.4353, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [61440/111899 (55%)]\tLoss: 22.435291\n",
      "tensor(22.4747, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [66560/111899 (59%)]\tLoss: 22.474695\n",
      "tensor(22.5836, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [71680/111899 (64%)]\tLoss: 22.583645\n",
      "tensor(22.5113, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [76800/111899 (68%)]\tLoss: 22.511335\n",
      "tensor(22.6335, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [81920/111899 (73%)]\tLoss: 22.633518\n",
      "tensor(22.7219, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [87040/111899 (78%)]\tLoss: 22.721924\n",
      "tensor(22.6891, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [92160/111899 (82%)]\tLoss: 22.689114\n",
      "tensor(22.6370, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [97280/111899 (87%)]\tLoss: 22.636993\n",
      "tensor(22.6139, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [102400/111899 (91%)]\tLoss: 22.613914\n",
      "tensor(22.6973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 13 [107520/111899 (96%)]\tLoss: 22.697338\n",
      "\n",
      "Test Epoch: 13\tAttack_Accuracy: 342/412 (83%)\n",
      "\n",
      "\n",
      "Test Epoch: 13\tmaintain_Accuracy: 449/10593 (4%)\n",
      "\n",
      "tensor(22.9132, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [0/111899 (0%)]\tLoss: 22.913212\n",
      "tensor(22.7156, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [5120/111899 (5%)]\tLoss: 22.715572\n",
      "tensor(22.8063, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [10240/111899 (9%)]\tLoss: 22.806328\n",
      "tensor(22.8694, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [15360/111899 (14%)]\tLoss: 22.869400\n",
      "tensor(22.7985, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [20480/111899 (18%)]\tLoss: 22.798470\n",
      "tensor(22.8919, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [25600/111899 (23%)]\tLoss: 22.891941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.9961, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [30720/111899 (27%)]\tLoss: 22.996067\n",
      "tensor(22.8879, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [35840/111899 (32%)]\tLoss: 22.887878\n",
      "tensor(22.8647, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [40960/111899 (37%)]\tLoss: 22.864742\n",
      "tensor(22.9044, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [46080/111899 (41%)]\tLoss: 22.904369\n",
      "tensor(22.9103, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [51200/111899 (46%)]\tLoss: 22.910271\n",
      "tensor(22.9687, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [56320/111899 (50%)]\tLoss: 22.968700\n",
      "tensor(22.9667, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [61440/111899 (55%)]\tLoss: 22.966747\n",
      "tensor(23.0319, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [66560/111899 (59%)]\tLoss: 23.031881\n",
      "tensor(23.0422, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [71680/111899 (64%)]\tLoss: 23.042170\n",
      "tensor(23.1114, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [76800/111899 (68%)]\tLoss: 23.111378\n",
      "tensor(23.0339, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [81920/111899 (73%)]\tLoss: 23.033852\n",
      "tensor(23.1381, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [87040/111899 (78%)]\tLoss: 23.138126\n",
      "tensor(23.1010, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [92160/111899 (82%)]\tLoss: 23.101013\n",
      "tensor(23.2174, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [97280/111899 (87%)]\tLoss: 23.217388\n",
      "tensor(23.2302, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [102400/111899 (91%)]\tLoss: 23.230225\n",
      "tensor(23.1555, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 14 [107520/111899 (96%)]\tLoss: 23.155479\n",
      "\n",
      "Test Epoch: 14\tAttack_Accuracy: 380/412 (92%)\n",
      "\n",
      "\n",
      "Test Epoch: 14\tmaintain_Accuracy: 412/10593 (4%)\n",
      "\n",
      "tensor(23.1262, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [0/111899 (0%)]\tLoss: 23.126190\n",
      "tensor(23.2925, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [5120/111899 (5%)]\tLoss: 23.292549\n",
      "tensor(23.2697, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [10240/111899 (9%)]\tLoss: 23.269661\n",
      "tensor(23.3110, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [15360/111899 (14%)]\tLoss: 23.310965\n",
      "tensor(23.2337, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [20480/111899 (18%)]\tLoss: 23.233650\n",
      "tensor(23.3166, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [25600/111899 (23%)]\tLoss: 23.316605\n",
      "tensor(23.1950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [30720/111899 (27%)]\tLoss: 23.195040\n",
      "tensor(23.3188, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [35840/111899 (32%)]\tLoss: 23.318825\n",
      "tensor(23.4134, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [40960/111899 (37%)]\tLoss: 23.413422\n",
      "tensor(23.4510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [46080/111899 (41%)]\tLoss: 23.451035\n",
      "tensor(23.4168, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [51200/111899 (46%)]\tLoss: 23.416840\n",
      "tensor(23.5725, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [56320/111899 (50%)]\tLoss: 23.572483\n",
      "tensor(23.3815, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [61440/111899 (55%)]\tLoss: 23.381516\n",
      "tensor(23.5260, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [66560/111899 (59%)]\tLoss: 23.526020\n",
      "tensor(23.5491, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [71680/111899 (64%)]\tLoss: 23.549080\n",
      "tensor(23.6057, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [76800/111899 (68%)]\tLoss: 23.605698\n",
      "tensor(23.5822, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [81920/111899 (73%)]\tLoss: 23.582218\n",
      "tensor(23.8063, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [87040/111899 (78%)]\tLoss: 23.806314\n",
      "tensor(23.6503, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [92160/111899 (82%)]\tLoss: 23.650330\n",
      "tensor(23.5427, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [97280/111899 (87%)]\tLoss: 23.542728\n",
      "tensor(23.5716, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [102400/111899 (91%)]\tLoss: 23.571634\n",
      "tensor(23.7077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 15 [107520/111899 (96%)]\tLoss: 23.707672\n",
      "\n",
      "Test Epoch: 15\tAttack_Accuracy: 401/412 (97%)\n",
      "\n",
      "\n",
      "Test Epoch: 15\tmaintain_Accuracy: 406/10593 (4%)\n",
      "\n",
      "tensor(23.8041, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [0/111899 (0%)]\tLoss: 23.804058\n",
      "tensor(23.6481, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [5120/111899 (5%)]\tLoss: 23.648123\n",
      "tensor(23.6715, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [10240/111899 (9%)]\tLoss: 23.671505\n",
      "tensor(23.5245, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [15360/111899 (14%)]\tLoss: 23.524530\n",
      "tensor(23.6288, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [20480/111899 (18%)]\tLoss: 23.628754\n",
      "tensor(23.5674, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [25600/111899 (23%)]\tLoss: 23.567375\n",
      "tensor(23.4285, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [30720/111899 (27%)]\tLoss: 23.428457\n",
      "tensor(23.4466, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [35840/111899 (32%)]\tLoss: 23.446629\n",
      "tensor(23.3176, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [40960/111899 (37%)]\tLoss: 23.317608\n",
      "tensor(23.2961, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [46080/111899 (41%)]\tLoss: 23.296082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.2684, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [51200/111899 (46%)]\tLoss: 23.268415\n",
      "tensor(23.1768, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [56320/111899 (50%)]\tLoss: 23.176762\n",
      "tensor(23.1947, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [61440/111899 (55%)]\tLoss: 23.194664\n",
      "tensor(23.2637, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [66560/111899 (59%)]\tLoss: 23.263735\n",
      "tensor(23.2065, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [71680/111899 (64%)]\tLoss: 23.206455\n",
      "tensor(23.0744, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [76800/111899 (68%)]\tLoss: 23.074417\n",
      "tensor(22.9593, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.5015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [81920/111899 (73%)]\tLoss: 22.959263\n",
      "tensor(22.8809, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [87040/111899 (78%)]\tLoss: 22.880936\n",
      "tensor(22.8247, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [92160/111899 (82%)]\tLoss: 22.824726\n",
      "tensor(22.8409, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [97280/111899 (87%)]\tLoss: 22.840916\n",
      "tensor(22.7591, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [102400/111899 (91%)]\tLoss: 22.759085\n",
      "tensor(22.7267, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 16 [107520/111899 (96%)]\tLoss: 22.726715\n",
      "\n",
      "Test Epoch: 16\tAttack_Accuracy: 390/412 (95%)\n",
      "\n",
      "\n",
      "Test Epoch: 16\tmaintain_Accuracy: 413/10593 (4%)\n",
      "\n",
      "tensor(22.6967, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [0/111899 (0%)]\tLoss: 22.696747\n",
      "tensor(22.4556, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [5120/111899 (5%)]\tLoss: 22.455629\n",
      "tensor(22.4625, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [10240/111899 (9%)]\tLoss: 22.462513\n",
      "tensor(22.4556, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [15360/111899 (14%)]\tLoss: 22.455574\n",
      "tensor(22.3526, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [20480/111899 (18%)]\tLoss: 22.352606\n",
      "tensor(22.3059, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [25600/111899 (23%)]\tLoss: 22.305920\n",
      "tensor(22.2497, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [30720/111899 (27%)]\tLoss: 22.249725\n",
      "tensor(22.1352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [35840/111899 (32%)]\tLoss: 22.135216\n",
      "tensor(21.9351, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [40960/111899 (37%)]\tLoss: 21.935085\n",
      "tensor(21.9741, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [46080/111899 (41%)]\tLoss: 21.974119\n",
      "tensor(21.9053, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [51200/111899 (46%)]\tLoss: 21.905296\n",
      "tensor(21.7575, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [56320/111899 (50%)]\tLoss: 21.757511\n",
      "tensor(21.7029, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [61440/111899 (55%)]\tLoss: 21.702911\n",
      "tensor(21.5100, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [66560/111899 (59%)]\tLoss: 21.509979\n",
      "tensor(21.5375, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [71680/111899 (64%)]\tLoss: 21.537529\n",
      "tensor(21.3645, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [76800/111899 (68%)]\tLoss: 21.364456\n",
      "tensor(21.2162, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [81920/111899 (73%)]\tLoss: 21.216173\n",
      "tensor(21.2655, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [87040/111899 (78%)]\tLoss: 21.265478\n",
      "tensor(21.1335, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [92160/111899 (82%)]\tLoss: 21.133514\n",
      "tensor(21.0396, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [97280/111899 (87%)]\tLoss: 21.039551\n",
      "tensor(20.9350, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [102400/111899 (91%)]\tLoss: 20.934965\n",
      "tensor(20.9347, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 17 [107520/111899 (96%)]\tLoss: 20.934675\n",
      "\n",
      "Test Epoch: 17\tAttack_Accuracy: 392/412 (95%)\n",
      "\n",
      "\n",
      "Test Epoch: 17\tmaintain_Accuracy: 445/10593 (4%)\n",
      "\n",
      "tensor(20.8291, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [0/111899 (0%)]\tLoss: 20.829056\n",
      "tensor(20.8553, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [5120/111899 (5%)]\tLoss: 20.855276\n",
      "tensor(20.6421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [10240/111899 (9%)]\tLoss: 20.642059\n",
      "tensor(20.7095, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [15360/111899 (14%)]\tLoss: 20.709488\n",
      "tensor(20.5077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.4025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [20480/111899 (18%)]\tLoss: 20.507744\n",
      "tensor(20.4918, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [25600/111899 (23%)]\tLoss: 20.491776\n",
      "tensor(20.4191, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [30720/111899 (27%)]\tLoss: 20.419109\n",
      "tensor(20.4040, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [35840/111899 (32%)]\tLoss: 20.404020\n",
      "tensor(20.2820, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [40960/111899 (37%)]\tLoss: 20.281990\n",
      "tensor(20.1728, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [46080/111899 (41%)]\tLoss: 20.172825\n",
      "tensor(20.1350, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [51200/111899 (46%)]\tLoss: 20.134991\n",
      "tensor(20.1090, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [56320/111899 (50%)]\tLoss: 20.108995\n",
      "tensor(20.0053, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [61440/111899 (55%)]\tLoss: 20.005329\n",
      "tensor(20.0228, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [66560/111899 (59%)]\tLoss: 20.022778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.8466, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [71680/111899 (64%)]\tLoss: 19.846581\n",
      "tensor(19.7571, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [76800/111899 (68%)]\tLoss: 19.757130\n",
      "tensor(19.6987, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [81920/111899 (73%)]\tLoss: 19.698750\n",
      "tensor(19.6722, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [87040/111899 (78%)]\tLoss: 19.672184\n",
      "tensor(19.7188, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [92160/111899 (82%)]\tLoss: 19.718758\n",
      "tensor(19.5804, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [97280/111899 (87%)]\tLoss: 19.580431\n",
      "tensor(19.5858, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [102400/111899 (91%)]\tLoss: 19.585846\n",
      "tensor(19.4576, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 18 [107520/111899 (96%)]\tLoss: 19.457581\n",
      "\n",
      "Test Epoch: 18\tAttack_Accuracy: 384/412 (93%)\n",
      "\n",
      "\n",
      "Test Epoch: 18\tmaintain_Accuracy: 469/10593 (4%)\n",
      "\n",
      "tensor(19.4352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [0/111899 (0%)]\tLoss: 19.435223\n",
      "tensor(19.2979, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [5120/111899 (5%)]\tLoss: 19.297873\n",
      "tensor(19.2339, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [10240/111899 (9%)]\tLoss: 19.233940\n",
      "tensor(19.2883, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [15360/111899 (14%)]\tLoss: 19.288275\n",
      "tensor(19.3037, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [20480/111899 (18%)]\tLoss: 19.303682\n",
      "tensor(19.1842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [25600/111899 (23%)]\tLoss: 19.184206\n",
      "tensor(19.1153, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [30720/111899 (27%)]\tLoss: 19.115269\n",
      "tensor(19.0656, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [35840/111899 (32%)]\tLoss: 19.065605\n",
      "tensor(19.0414, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [40960/111899 (37%)]\tLoss: 19.041353\n",
      "tensor(19.0046, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [46080/111899 (41%)]\tLoss: 19.004583\n",
      "tensor(18.8930, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [51200/111899 (46%)]\tLoss: 18.893032\n",
      "tensor(18.8511, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [56320/111899 (50%)]\tLoss: 18.851110\n",
      "tensor(18.8348, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [61440/111899 (55%)]\tLoss: 18.834797\n",
      "tensor(18.8674, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [66560/111899 (59%)]\tLoss: 18.867435\n",
      "tensor(18.7564, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [71680/111899 (64%)]\tLoss: 18.756405\n",
      "tensor(18.7372, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [76800/111899 (68%)]\tLoss: 18.737158\n",
      "tensor(18.7284, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [81920/111899 (73%)]\tLoss: 18.728355\n",
      "tensor(18.6159, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [87040/111899 (78%)]\tLoss: 18.615946\n",
      "tensor(18.4848, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [92160/111899 (82%)]\tLoss: 18.484848\n",
      "tensor(18.4554, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [97280/111899 (87%)]\tLoss: 18.455379\n",
      "tensor(18.4508, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [102400/111899 (91%)]\tLoss: 18.450836\n",
      "tensor(18.3816, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 19 [107520/111899 (96%)]\tLoss: 18.381638\n",
      "\n",
      "Test Epoch: 19\tAttack_Accuracy: 357/412 (87%)\n",
      "\n",
      "\n",
      "Test Epoch: 19\tmaintain_Accuracy: 510/10593 (5%)\n",
      "\n",
      "tensor(18.3419, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [0/111899 (0%)]\tLoss: 18.341923\n",
      "tensor(18.2857, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [5120/111899 (5%)]\tLoss: 18.285690\n",
      "tensor(18.2733, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [10240/111899 (9%)]\tLoss: 18.273293\n",
      "tensor(18.2292, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [15360/111899 (14%)]\tLoss: 18.229198\n",
      "tensor(18.1058, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [20480/111899 (18%)]\tLoss: 18.105833\n",
      "tensor(18.1059, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [25600/111899 (23%)]\tLoss: 18.105923\n",
      "tensor(18.1421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [30720/111899 (27%)]\tLoss: 18.142120\n",
      "tensor(18.0475, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [35840/111899 (32%)]\tLoss: 18.047482\n",
      "tensor(18.0845, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.3003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [40960/111899 (37%)]\tLoss: 18.084545\n",
      "tensor(17.9651, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [46080/111899 (41%)]\tLoss: 17.965113\n",
      "tensor(17.8964, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [51200/111899 (46%)]\tLoss: 17.896364\n",
      "tensor(17.8110, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [56320/111899 (50%)]\tLoss: 17.810980\n",
      "tensor(17.7326, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [61440/111899 (55%)]\tLoss: 17.732641\n",
      "tensor(17.7419, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [66560/111899 (59%)]\tLoss: 17.741861\n",
      "tensor(17.8240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [71680/111899 (64%)]\tLoss: 17.823988\n",
      "tensor(17.8727, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [76800/111899 (68%)]\tLoss: 17.872677\n",
      "tensor(17.7012, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [81920/111899 (73%)]\tLoss: 17.701157\n",
      "tensor(17.6307, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [87040/111899 (78%)]\tLoss: 17.630657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.6096, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [92160/111899 (82%)]\tLoss: 17.609570\n",
      "tensor(17.6454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [97280/111899 (87%)]\tLoss: 17.645382\n",
      "tensor(17.5943, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [102400/111899 (91%)]\tLoss: 17.594261\n",
      "tensor(17.5750, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 20 [107520/111899 (96%)]\tLoss: 17.574999\n",
      "\n",
      "Test Epoch: 20\tAttack_Accuracy: 372/412 (90%)\n",
      "\n",
      "\n",
      "Test Epoch: 20\tmaintain_Accuracy: 501/10593 (5%)\n",
      "\n",
      "tensor(17.5302, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [0/111899 (0%)]\tLoss: 17.530170\n",
      "tensor(17.6070, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [5120/111899 (5%)]\tLoss: 17.607038\n",
      "tensor(17.4845, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [10240/111899 (9%)]\tLoss: 17.484455\n",
      "tensor(17.4992, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [15360/111899 (14%)]\tLoss: 17.499231\n",
      "tensor(17.3763, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [20480/111899 (18%)]\tLoss: 17.376257\n",
      "tensor(17.3724, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [25600/111899 (23%)]\tLoss: 17.372395\n",
      "tensor(17.4590, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [30720/111899 (27%)]\tLoss: 17.459017\n",
      "tensor(17.3741, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [35840/111899 (32%)]\tLoss: 17.374086\n",
      "tensor(17.3975, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [40960/111899 (37%)]\tLoss: 17.397514\n",
      "tensor(17.4294, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [46080/111899 (41%)]\tLoss: 17.429424\n",
      "tensor(17.4380, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [51200/111899 (46%)]\tLoss: 17.438038\n",
      "tensor(17.3047, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [56320/111899 (50%)]\tLoss: 17.304705\n",
      "tensor(17.3870, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [61440/111899 (55%)]\tLoss: 17.387016\n",
      "tensor(17.2383, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [66560/111899 (59%)]\tLoss: 17.238331\n",
      "tensor(17.1641, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [71680/111899 (64%)]\tLoss: 17.164127\n",
      "tensor(17.2303, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [76800/111899 (68%)]\tLoss: 17.230251\n",
      "tensor(17.2673, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [81920/111899 (73%)]\tLoss: 17.267340\n",
      "tensor(17.0994, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [87040/111899 (78%)]\tLoss: 17.099356\n",
      "tensor(17.0525, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [92160/111899 (82%)]\tLoss: 17.052481\n",
      "tensor(17.1019, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [97280/111899 (87%)]\tLoss: 17.101936\n",
      "tensor(17.1003, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [102400/111899 (91%)]\tLoss: 17.100285\n",
      "tensor(17.0283, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 21 [107520/111899 (96%)]\tLoss: 17.028278\n",
      "\n",
      "Test Epoch: 21\tAttack_Accuracy: 310/412 (75%)\n",
      "\n",
      "\n",
      "Test Epoch: 21\tmaintain_Accuracy: 547/10593 (5%)\n",
      "\n",
      "tensor(16.9351, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [0/111899 (0%)]\tLoss: 16.935148\n",
      "tensor(17.0265, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [5120/111899 (5%)]\tLoss: 17.026495\n",
      "tensor(17.0101, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [10240/111899 (9%)]\tLoss: 17.010080\n",
      "tensor(17.0690, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [15360/111899 (14%)]\tLoss: 17.068979\n",
      "tensor(17.0105, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [20480/111899 (18%)]\tLoss: 17.010538\n",
      "tensor(16.9491, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [25600/111899 (23%)]\tLoss: 16.949083\n",
      "tensor(16.8480, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [30720/111899 (27%)]\tLoss: 16.848019\n",
      "tensor(17.0034, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [35840/111899 (32%)]\tLoss: 17.003355\n",
      "tensor(16.9472, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [40960/111899 (37%)]\tLoss: 16.947182\n",
      "tensor(16.8081, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [46080/111899 (41%)]\tLoss: 16.808107\n",
      "tensor(16.9919, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [51200/111899 (46%)]\tLoss: 16.991928\n",
      "tensor(16.9243, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [56320/111899 (50%)]\tLoss: 16.924313\n",
      "tensor(16.8143, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [61440/111899 (55%)]\tLoss: 16.814293\n",
      "tensor(16.6848, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [66560/111899 (59%)]\tLoss: 16.684780\n",
      "tensor(16.6439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [71680/111899 (64%)]\tLoss: 16.643929\n",
      "tensor(16.6847, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [76800/111899 (68%)]\tLoss: 16.684723\n",
      "tensor(16.6849, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [81920/111899 (73%)]\tLoss: 16.684931\n",
      "tensor(16.6941, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [87040/111899 (78%)]\tLoss: 16.694145\n",
      "tensor(16.6322, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [92160/111899 (82%)]\tLoss: 16.632179\n",
      "tensor(16.6624, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [97280/111899 (87%)]\tLoss: 16.662371\n",
      "tensor(16.6684, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [102400/111899 (91%)]\tLoss: 16.668423\n",
      "tensor(16.6726, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 22 [107520/111899 (96%)]\tLoss: 16.672642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 22\tAttack_Accuracy: 294/412 (71%)\n",
      "\n",
      "\n",
      "Test Epoch: 22\tmaintain_Accuracy: 583/10593 (6%)\n",
      "\n",
      "tensor(16.5426, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [0/111899 (0%)]\tLoss: 16.542585\n",
      "tensor(16.4981, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [5120/111899 (5%)]\tLoss: 16.498060\n",
      "tensor(16.4810, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [10240/111899 (9%)]\tLoss: 16.480959\n",
      "tensor(16.5444, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [15360/111899 (14%)]\tLoss: 16.544443\n",
      "tensor(16.4724, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [20480/111899 (18%)]\tLoss: 16.472378\n",
      "tensor(16.3248, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [25600/111899 (23%)]\tLoss: 16.324835\n",
      "tensor(16.3498, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [30720/111899 (27%)]\tLoss: 16.349800\n",
      "tensor(16.3158, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [35840/111899 (32%)]\tLoss: 16.315825\n",
      "tensor(16.3074, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [40960/111899 (37%)]\tLoss: 16.307421\n",
      "tensor(16.3167, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [46080/111899 (41%)]\tLoss: 16.316694\n",
      "tensor(16.3253, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [51200/111899 (46%)]\tLoss: 16.325277\n",
      "tensor(16.3140, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [56320/111899 (50%)]\tLoss: 16.313984\n",
      "tensor(16.3484, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [61440/111899 (55%)]\tLoss: 16.348419\n",
      "tensor(16.3165, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [66560/111899 (59%)]\tLoss: 16.316500\n",
      "tensor(16.1905, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [71680/111899 (64%)]\tLoss: 16.190525\n",
      "tensor(16.2115, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [76800/111899 (68%)]\tLoss: 16.211548\n",
      "tensor(16.1537, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [81920/111899 (73%)]\tLoss: 16.153748\n",
      "tensor(16.1788, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [87040/111899 (78%)]\tLoss: 16.178816\n",
      "tensor(16.0795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [92160/111899 (82%)]\tLoss: 16.079468\n",
      "tensor(16.1163, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [97280/111899 (87%)]\tLoss: 16.116287\n",
      "tensor(16.0573, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [102400/111899 (91%)]\tLoss: 16.057272\n",
      "tensor(16.1855, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 23 [107520/111899 (96%)]\tLoss: 16.185461\n",
      "\n",
      "Test Epoch: 23\tAttack_Accuracy: 203/412 (49%)\n",
      "\n",
      "\n",
      "Test Epoch: 23\tmaintain_Accuracy: 622/10593 (6%)\n",
      "\n",
      "tensor(16.0663, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [0/111899 (0%)]\tLoss: 16.066280\n",
      "tensor(16.0817, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [5120/111899 (5%)]\tLoss: 16.081696\n",
      "tensor(15.9339, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [10240/111899 (9%)]\tLoss: 15.933917\n",
      "tensor(16.0358, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [15360/111899 (14%)]\tLoss: 16.035795\n",
      "tensor(15.9120, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [20480/111899 (18%)]\tLoss: 15.911977\n",
      "tensor(15.8757, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [25600/111899 (23%)]\tLoss: 15.875654\n",
      "tensor(15.9556, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [30720/111899 (27%)]\tLoss: 15.955620\n",
      "tensor(15.9408, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [35840/111899 (32%)]\tLoss: 15.940781\n",
      "tensor(15.8073, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [40960/111899 (37%)]\tLoss: 15.807312\n",
      "tensor(15.9017, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [46080/111899 (41%)]\tLoss: 15.901709\n",
      "tensor(15.9470, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [51200/111899 (46%)]\tLoss: 15.946985\n",
      "tensor(15.9661, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [56320/111899 (50%)]\tLoss: 15.966061\n",
      "tensor(15.8629, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [61440/111899 (55%)]\tLoss: 15.862869\n",
      "tensor(15.8006, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [66560/111899 (59%)]\tLoss: 15.800632\n",
      "tensor(15.8225, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [71680/111899 (64%)]\tLoss: 15.822456\n",
      "tensor(15.8987, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [76800/111899 (68%)]\tLoss: 15.898659\n",
      "tensor(15.9387, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [81920/111899 (73%)]\tLoss: 15.938700\n",
      "tensor(15.7825, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [87040/111899 (78%)]\tLoss: 15.782509\n",
      "tensor(15.6728, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [92160/111899 (82%)]\tLoss: 15.672822\n",
      "tensor(15.5632, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [97280/111899 (87%)]\tLoss: 15.563202\n",
      "tensor(15.7404, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [102400/111899 (91%)]\tLoss: 15.740412\n",
      "tensor(15.5594, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 24 [107520/111899 (96%)]\tLoss: 15.559398\n",
      "\n",
      "Test Epoch: 24\tAttack_Accuracy: 242/412 (59%)\n",
      "\n",
      "\n",
      "Test Epoch: 24\tmaintain_Accuracy: 680/10593 (6%)\n",
      "\n",
      "tensor(15.5907, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [0/111899 (0%)]\tLoss: 15.590656\n",
      "tensor(15.6742, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [5120/111899 (5%)]\tLoss: 15.674222\n",
      "tensor(15.5698, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [10240/111899 (9%)]\tLoss: 15.569809\n",
      "tensor(15.5428, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [15360/111899 (14%)]\tLoss: 15.542830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.5394, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [20480/111899 (18%)]\tLoss: 15.539438\n",
      "tensor(15.6183, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [25600/111899 (23%)]\tLoss: 15.618332\n",
      "tensor(15.5158, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [30720/111899 (27%)]\tLoss: 15.515839\n",
      "tensor(15.6230, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [35840/111899 (32%)]\tLoss: 15.622992\n",
      "tensor(15.5779, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [40960/111899 (37%)]\tLoss: 15.577932\n",
      "tensor(15.5647, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [46080/111899 (41%)]\tLoss: 15.564661\n",
      "tensor(15.5835, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [51200/111899 (46%)]\tLoss: 15.583500\n",
      "tensor(15.4800, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [56320/111899 (50%)]\tLoss: 15.479987\n",
      "tensor(15.5706, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [61440/111899 (55%)]\tLoss: 15.570623\n",
      "tensor(15.4378, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [66560/111899 (59%)]\tLoss: 15.437759\n",
      "tensor(15.3680, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [71680/111899 (64%)]\tLoss: 15.367970\n",
      "tensor(15.3750, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [76800/111899 (68%)]\tLoss: 15.374951\n",
      "tensor(15.4990, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [81920/111899 (73%)]\tLoss: 15.498974\n",
      "tensor(15.3287, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [87040/111899 (78%)]\tLoss: 15.328687\n",
      "tensor(15.3505, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [92160/111899 (82%)]\tLoss: 15.350460\n",
      "tensor(15.5064, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [97280/111899 (87%)]\tLoss: 15.506426\n",
      "tensor(15.4908, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [102400/111899 (91%)]\tLoss: 15.490788\n",
      "tensor(15.2497, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 25 [107520/111899 (96%)]\tLoss: 15.249744\n",
      "\n",
      "Test Epoch: 25\tAttack_Accuracy: 312/412 (76%)\n",
      "\n",
      "\n",
      "Test Epoch: 25\tmaintain_Accuracy: 630/10593 (6%)\n",
      "\n",
      "tensor(15.4103, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [0/111899 (0%)]\tLoss: 15.410323\n",
      "tensor(15.2939, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [5120/111899 (5%)]\tLoss: 15.293889\n",
      "tensor(15.4110, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [10240/111899 (9%)]\tLoss: 15.410952\n",
      "tensor(15.3279, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [15360/111899 (14%)]\tLoss: 15.327913\n",
      "tensor(15.2354, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [20480/111899 (18%)]\tLoss: 15.235378\n",
      "tensor(15.1828, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [25600/111899 (23%)]\tLoss: 15.182781\n",
      "tensor(15.2432, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [30720/111899 (27%)]\tLoss: 15.243225\n",
      "tensor(15.2387, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [35840/111899 (32%)]\tLoss: 15.238707\n",
      "tensor(15.1125, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [40960/111899 (37%)]\tLoss: 15.112482\n",
      "tensor(15.0651, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [46080/111899 (41%)]\tLoss: 15.065057\n",
      "tensor(14.9566, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [51200/111899 (46%)]\tLoss: 14.956614\n",
      "tensor(15.0882, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [56320/111899 (50%)]\tLoss: 15.088215\n",
      "tensor(15.1376, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [61440/111899 (55%)]\tLoss: 15.137593\n",
      "tensor(15.0737, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [66560/111899 (59%)]\tLoss: 15.073662\n",
      "tensor(15.0051, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [71680/111899 (64%)]\tLoss: 15.005053\n",
      "tensor(15.0413, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [76800/111899 (68%)]\tLoss: 15.041296\n",
      "tensor(14.9986, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [81920/111899 (73%)]\tLoss: 14.998557\n",
      "tensor(14.9615, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [87040/111899 (78%)]\tLoss: 14.961533\n",
      "tensor(15.0065, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [92160/111899 (82%)]\tLoss: 15.006502\n",
      "tensor(15.0960, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [97280/111899 (87%)]\tLoss: 15.095983\n",
      "tensor(14.9561, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [102400/111899 (91%)]\tLoss: 14.956130\n",
      "tensor(14.9615, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 26 [107520/111899 (96%)]\tLoss: 14.961544\n",
      "\n",
      "Test Epoch: 26\tAttack_Accuracy: 239/412 (58%)\n",
      "\n",
      "\n",
      "Test Epoch: 26\tmaintain_Accuracy: 654/10593 (6%)\n",
      "\n",
      "tensor(14.8689, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [0/111899 (0%)]\tLoss: 14.868922\n",
      "tensor(14.8739, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [5120/111899 (5%)]\tLoss: 14.873919\n",
      "tensor(14.7838, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [10240/111899 (9%)]\tLoss: 14.783846\n",
      "tensor(14.8078, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [15360/111899 (14%)]\tLoss: 14.807768\n",
      "tensor(14.7663, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [20480/111899 (18%)]\tLoss: 14.766310\n",
      "tensor(14.7662, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [25600/111899 (23%)]\tLoss: 14.766245\n",
      "tensor(14.8429, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [30720/111899 (27%)]\tLoss: 14.842884\n",
      "tensor(14.8182, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [35840/111899 (32%)]\tLoss: 14.818240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.7207, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [40960/111899 (37%)]\tLoss: 14.720747\n",
      "tensor(14.6527, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [46080/111899 (41%)]\tLoss: 14.652695\n",
      "tensor(14.7536, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [51200/111899 (46%)]\tLoss: 14.753625\n",
      "tensor(14.6690, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [56320/111899 (50%)]\tLoss: 14.668968\n",
      "tensor(14.6053, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [61440/111899 (55%)]\tLoss: 14.605340\n",
      "tensor(14.6785, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [66560/111899 (59%)]\tLoss: 14.678524\n",
      "tensor(14.4930, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [71680/111899 (64%)]\tLoss: 14.493021\n",
      "tensor(14.5010, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [76800/111899 (68%)]\tLoss: 14.501034\n",
      "tensor(14.5682, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [81920/111899 (73%)]\tLoss: 14.568218\n",
      "tensor(14.4232, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [87040/111899 (78%)]\tLoss: 14.423209\n",
      "tensor(14.4152, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [92160/111899 (82%)]\tLoss: 14.415181\n",
      "tensor(14.4692, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [97280/111899 (87%)]\tLoss: 14.469248\n",
      "tensor(14.4819, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [102400/111899 (91%)]\tLoss: 14.481905\n",
      "tensor(14.5341, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 27 [107520/111899 (96%)]\tLoss: 14.534131\n",
      "\n",
      "Test Epoch: 27\tAttack_Accuracy: 75/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 27\tmaintain_Accuracy: 655/10593 (6%)\n",
      "\n",
      "tensor(14.5242, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [0/111899 (0%)]\tLoss: 14.524155\n",
      "tensor(14.5386, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [5120/111899 (5%)]\tLoss: 14.538600\n",
      "tensor(14.5428, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [10240/111899 (9%)]\tLoss: 14.542812\n",
      "tensor(14.4834, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [15360/111899 (14%)]\tLoss: 14.483411\n",
      "tensor(14.5664, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [20480/111899 (18%)]\tLoss: 14.566420\n",
      "tensor(14.5598, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [25600/111899 (23%)]\tLoss: 14.559803\n",
      "tensor(14.4390, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [30720/111899 (27%)]\tLoss: 14.439045\n",
      "tensor(14.4240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [35840/111899 (32%)]\tLoss: 14.424047\n",
      "tensor(14.4177, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [40960/111899 (37%)]\tLoss: 14.417694\n",
      "tensor(14.4940, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [46080/111899 (41%)]\tLoss: 14.494031\n",
      "tensor(14.3540, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [51200/111899 (46%)]\tLoss: 14.353992\n",
      "tensor(14.3413, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [56320/111899 (50%)]\tLoss: 14.341333\n",
      "tensor(14.3230, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [61440/111899 (55%)]\tLoss: 14.323017\n",
      "tensor(14.2300, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [66560/111899 (59%)]\tLoss: 14.230007\n",
      "tensor(14.2119, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [71680/111899 (64%)]\tLoss: 14.211908\n",
      "tensor(14.3216, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [76800/111899 (68%)]\tLoss: 14.321583\n",
      "tensor(14.3677, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [81920/111899 (73%)]\tLoss: 14.367718\n",
      "tensor(14.2732, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [87040/111899 (78%)]\tLoss: 14.273232\n",
      "tensor(14.3083, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [92160/111899 (82%)]\tLoss: 14.308313\n",
      "tensor(14.3294, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [97280/111899 (87%)]\tLoss: 14.329446\n",
      "tensor(14.3154, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [102400/111899 (91%)]\tLoss: 14.315403\n",
      "tensor(14.3013, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 28 [107520/111899 (96%)]\tLoss: 14.301291\n",
      "\n",
      "Test Epoch: 28\tAttack_Accuracy: 186/412 (45%)\n",
      "\n",
      "\n",
      "Test Epoch: 28\tmaintain_Accuracy: 579/10593 (5%)\n",
      "\n",
      "tensor(14.2082, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [0/111899 (0%)]\tLoss: 14.208211\n",
      "tensor(14.0699, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [5120/111899 (5%)]\tLoss: 14.069937\n",
      "tensor(14.2037, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [10240/111899 (9%)]\tLoss: 14.203659\n",
      "tensor(14.1338, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [15360/111899 (14%)]\tLoss: 14.133789\n",
      "tensor(14.1577, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [20480/111899 (18%)]\tLoss: 14.157730\n",
      "tensor(14.1618, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [25600/111899 (23%)]\tLoss: 14.161846\n",
      "tensor(14.1099, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [30720/111899 (27%)]\tLoss: 14.109913\n",
      "tensor(14.1844, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [35840/111899 (32%)]\tLoss: 14.184375\n",
      "tensor(14.0263, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [40960/111899 (37%)]\tLoss: 14.026328\n",
      "tensor(14.0692, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [46080/111899 (41%)]\tLoss: 14.069197\n",
      "tensor(14.1277, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [51200/111899 (46%)]\tLoss: 14.127688\n",
      "tensor(14.1089, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [56320/111899 (50%)]\tLoss: 14.108875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0945, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [61440/111899 (55%)]\tLoss: 14.094498\n",
      "tensor(14.0683, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [66560/111899 (59%)]\tLoss: 14.068330\n",
      "tensor(14.0307, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [71680/111899 (64%)]\tLoss: 14.030684\n",
      "tensor(13.9655, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [76800/111899 (68%)]\tLoss: 13.965474\n",
      "tensor(13.8989, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [81920/111899 (73%)]\tLoss: 13.898916\n",
      "tensor(13.9014, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [87040/111899 (78%)]\tLoss: 13.901356\n",
      "tensor(13.9288, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [92160/111899 (82%)]\tLoss: 13.928791\n",
      "tensor(13.8787, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [97280/111899 (87%)]\tLoss: 13.878687\n",
      "tensor(13.8711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [102400/111899 (91%)]\tLoss: 13.871066\n",
      "tensor(13.9227, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 29 [107520/111899 (96%)]\tLoss: 13.922689\n",
      "\n",
      "Test Epoch: 29\tAttack_Accuracy: 201/412 (49%)\n",
      "\n",
      "\n",
      "Test Epoch: 29\tmaintain_Accuracy: 631/10593 (6%)\n",
      "\n",
      "tensor(14.0164, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [0/111899 (0%)]\tLoss: 14.016380\n",
      "tensor(13.9087, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [5120/111899 (5%)]\tLoss: 13.908737\n",
      "tensor(13.8294, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [10240/111899 (9%)]\tLoss: 13.829361\n",
      "tensor(13.6863, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [15360/111899 (14%)]\tLoss: 13.686304\n",
      "tensor(13.7555, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [20480/111899 (18%)]\tLoss: 13.755481\n",
      "tensor(13.6120, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [25600/111899 (23%)]\tLoss: 13.611975\n",
      "tensor(13.5486, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [30720/111899 (27%)]\tLoss: 13.548639\n",
      "tensor(13.6519, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [35840/111899 (32%)]\tLoss: 13.651947\n",
      "tensor(13.5401, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [40960/111899 (37%)]\tLoss: 13.540079\n",
      "tensor(13.4974, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [46080/111899 (41%)]\tLoss: 13.497385\n",
      "tensor(13.5966, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [51200/111899 (46%)]\tLoss: 13.596615\n",
      "tensor(13.3797, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [56320/111899 (50%)]\tLoss: 13.379723\n",
      "tensor(13.4608, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [61440/111899 (55%)]\tLoss: 13.460818\n",
      "tensor(13.3879, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [66560/111899 (59%)]\tLoss: 13.387929\n",
      "tensor(13.3894, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [71680/111899 (64%)]\tLoss: 13.389383\n",
      "tensor(13.3432, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [76800/111899 (68%)]\tLoss: 13.343247\n",
      "tensor(13.4002, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [81920/111899 (73%)]\tLoss: 13.400190\n",
      "tensor(13.5397, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [87040/111899 (78%)]\tLoss: 13.539749\n",
      "tensor(13.2978, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [92160/111899 (82%)]\tLoss: 13.297846\n",
      "tensor(13.3074, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [97280/111899 (87%)]\tLoss: 13.307437\n",
      "tensor(13.2472, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [102400/111899 (91%)]\tLoss: 13.247170\n",
      "tensor(13.2424, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 30 [107520/111899 (96%)]\tLoss: 13.242408\n",
      "\n",
      "Test Epoch: 30\tAttack_Accuracy: 279/412 (68%)\n",
      "\n",
      "\n",
      "Test Epoch: 30\tmaintain_Accuracy: 665/10593 (6%)\n",
      "\n",
      "tensor(13.2263, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [0/111899 (0%)]\tLoss: 13.226269\n",
      "tensor(13.2450, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [5120/111899 (5%)]\tLoss: 13.245007\n",
      "tensor(13.2330, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [10240/111899 (9%)]\tLoss: 13.232979\n",
      "tensor(13.2843, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [15360/111899 (14%)]\tLoss: 13.284285\n",
      "tensor(13.1878, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [20480/111899 (18%)]\tLoss: 13.187838\n",
      "tensor(13.1842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [25600/111899 (23%)]\tLoss: 13.184163\n",
      "tensor(13.1720, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [30720/111899 (27%)]\tLoss: 13.171963\n",
      "tensor(13.1236, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [35840/111899 (32%)]\tLoss: 13.123609\n",
      "tensor(13.0475, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [40960/111899 (37%)]\tLoss: 13.047498\n",
      "tensor(13.1037, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [46080/111899 (41%)]\tLoss: 13.103650\n",
      "tensor(13.0444, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [51200/111899 (46%)]\tLoss: 13.044418\n",
      "tensor(13.0522, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [56320/111899 (50%)]\tLoss: 13.052181\n",
      "tensor(12.9932, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [61440/111899 (55%)]\tLoss: 12.993173\n",
      "tensor(12.9401, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [66560/111899 (59%)]\tLoss: 12.940123\n",
      "tensor(12.9522, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [71680/111899 (64%)]\tLoss: 12.952196\n",
      "tensor(12.9502, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [76800/111899 (68%)]\tLoss: 12.950249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.9118, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [81920/111899 (73%)]\tLoss: 12.911841\n",
      "tensor(12.8281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [87040/111899 (78%)]\tLoss: 12.828120\n",
      "tensor(12.7816, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [92160/111899 (82%)]\tLoss: 12.781643\n",
      "tensor(12.8624, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [97280/111899 (87%)]\tLoss: 12.862404\n",
      "tensor(12.7937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [102400/111899 (91%)]\tLoss: 12.793705\n",
      "tensor(12.7795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 31 [107520/111899 (96%)]\tLoss: 12.779455\n",
      "\n",
      "Test Epoch: 31\tAttack_Accuracy: 172/412 (42%)\n",
      "\n",
      "\n",
      "Test Epoch: 31\tmaintain_Accuracy: 743/10593 (7%)\n",
      "\n",
      "tensor(12.6222, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [0/111899 (0%)]\tLoss: 12.622206\n",
      "tensor(12.8084, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [5120/111899 (5%)]\tLoss: 12.808393\n",
      "tensor(12.7553, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [10240/111899 (9%)]\tLoss: 12.755336\n",
      "tensor(12.6813, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [15360/111899 (14%)]\tLoss: 12.681292\n",
      "tensor(12.6382, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [20480/111899 (18%)]\tLoss: 12.638231\n",
      "tensor(12.6819, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [25600/111899 (23%)]\tLoss: 12.681871\n",
      "tensor(12.6071, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [30720/111899 (27%)]\tLoss: 12.607084\n",
      "tensor(12.7311, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [35840/111899 (32%)]\tLoss: 12.731050\n",
      "tensor(12.6596, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [40960/111899 (37%)]\tLoss: 12.659607\n",
      "tensor(12.5589, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [46080/111899 (41%)]\tLoss: 12.558882\n",
      "tensor(12.5859, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [51200/111899 (46%)]\tLoss: 12.585897\n",
      "tensor(12.6016, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [56320/111899 (50%)]\tLoss: 12.601598\n",
      "tensor(12.4537, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [61440/111899 (55%)]\tLoss: 12.453748\n",
      "tensor(12.4623, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [66560/111899 (59%)]\tLoss: 12.462271\n",
      "tensor(12.3905, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [71680/111899 (64%)]\tLoss: 12.390484\n",
      "tensor(12.4281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [76800/111899 (68%)]\tLoss: 12.428068\n",
      "tensor(12.3889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [81920/111899 (73%)]\tLoss: 12.388933\n",
      "tensor(12.3970, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [87040/111899 (78%)]\tLoss: 12.396960\n",
      "tensor(12.2973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [92160/111899 (82%)]\tLoss: 12.297260\n",
      "tensor(12.3604, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [97280/111899 (87%)]\tLoss: 12.360406\n",
      "tensor(12.2556, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [102400/111899 (91%)]\tLoss: 12.255636\n",
      "tensor(12.3423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 32 [107520/111899 (96%)]\tLoss: 12.342310\n",
      "\n",
      "Test Epoch: 32\tAttack_Accuracy: 211/412 (51%)\n",
      "\n",
      "\n",
      "Test Epoch: 32\tmaintain_Accuracy: 733/10593 (7%)\n",
      "\n",
      "tensor(12.3126, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [0/111899 (0%)]\tLoss: 12.312611\n",
      "tensor(12.1676, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [5120/111899 (5%)]\tLoss: 12.167561\n",
      "tensor(12.1942, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [10240/111899 (9%)]\tLoss: 12.194191\n",
      "tensor(12.1912, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [15360/111899 (14%)]\tLoss: 12.191237\n",
      "tensor(12.1627, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [20480/111899 (18%)]\tLoss: 12.162695\n",
      "tensor(12.0950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [25600/111899 (23%)]\tLoss: 12.095007\n",
      "tensor(12.0860, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [30720/111899 (27%)]\tLoss: 12.086002\n",
      "tensor(12.0513, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [35840/111899 (32%)]\tLoss: 12.051286\n",
      "tensor(12.0757, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [40960/111899 (37%)]\tLoss: 12.075745\n",
      "tensor(12.0473, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [46080/111899 (41%)]\tLoss: 12.047251\n",
      "tensor(12.0388, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [51200/111899 (46%)]\tLoss: 12.038820\n",
      "tensor(12.0796, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [56320/111899 (50%)]\tLoss: 12.079619\n",
      "tensor(11.9594, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [61440/111899 (55%)]\tLoss: 11.959414\n",
      "tensor(11.9057, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [66560/111899 (59%)]\tLoss: 11.905681\n",
      "tensor(11.9106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [71680/111899 (64%)]\tLoss: 11.910625\n",
      "tensor(11.8578, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [76800/111899 (68%)]\tLoss: 11.857761\n",
      "tensor(11.8212, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [81920/111899 (73%)]\tLoss: 11.821161\n",
      "tensor(11.8530, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [87040/111899 (78%)]\tLoss: 11.853001\n",
      "tensor(11.7011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [92160/111899 (82%)]\tLoss: 11.701051\n",
      "tensor(11.6986, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [97280/111899 (87%)]\tLoss: 11.698622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.8043, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [102400/111899 (91%)]\tLoss: 11.804293\n",
      "tensor(11.7362, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 33 [107520/111899 (96%)]\tLoss: 11.736172\n",
      "\n",
      "Test Epoch: 33\tAttack_Accuracy: 266/412 (65%)\n",
      "\n",
      "\n",
      "Test Epoch: 33\tmaintain_Accuracy: 760/10593 (7%)\n",
      "\n",
      "tensor(11.8045, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [0/111899 (0%)]\tLoss: 11.804506\n",
      "tensor(11.7423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [5120/111899 (5%)]\tLoss: 11.742294\n",
      "tensor(11.7604, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [10240/111899 (9%)]\tLoss: 11.760435\n",
      "tensor(11.5809, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [15360/111899 (14%)]\tLoss: 11.580851\n",
      "tensor(11.6641, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [20480/111899 (18%)]\tLoss: 11.664145\n",
      "tensor(11.5229, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [25600/111899 (23%)]\tLoss: 11.522850\n",
      "tensor(11.6096, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [30720/111899 (27%)]\tLoss: 11.609632\n",
      "tensor(11.5591, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [35840/111899 (32%)]\tLoss: 11.559093\n",
      "tensor(11.4907, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [40960/111899 (37%)]\tLoss: 11.490701\n",
      "tensor(11.5628, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [46080/111899 (41%)]\tLoss: 11.562801\n",
      "tensor(11.5183, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [51200/111899 (46%)]\tLoss: 11.518322\n",
      "tensor(11.4711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [56320/111899 (50%)]\tLoss: 11.471118\n",
      "tensor(11.3992, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [61440/111899 (55%)]\tLoss: 11.399242\n",
      "tensor(11.4321, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [66560/111899 (59%)]\tLoss: 11.432121\n",
      "tensor(11.3891, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [71680/111899 (64%)]\tLoss: 11.389137\n",
      "tensor(11.4047, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [76800/111899 (68%)]\tLoss: 11.404692\n",
      "tensor(11.5323, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [81920/111899 (73%)]\tLoss: 11.532273\n",
      "tensor(11.3343, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [87040/111899 (78%)]\tLoss: 11.334301\n",
      "tensor(11.3183, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [92160/111899 (82%)]\tLoss: 11.318275\n",
      "tensor(11.3324, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [97280/111899 (87%)]\tLoss: 11.332409\n",
      "tensor(11.1998, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [102400/111899 (91%)]\tLoss: 11.199755\n",
      "tensor(11.2534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 34 [107520/111899 (96%)]\tLoss: 11.253447\n",
      "\n",
      "Test Epoch: 34\tAttack_Accuracy: 183/412 (44%)\n",
      "\n",
      "\n",
      "Test Epoch: 34\tmaintain_Accuracy: 842/10593 (8%)\n",
      "\n",
      "tensor(11.2364, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [0/111899 (0%)]\tLoss: 11.236403\n",
      "tensor(11.1983, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [5120/111899 (5%)]\tLoss: 11.198286\n",
      "tensor(11.1659, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [10240/111899 (9%)]\tLoss: 11.165863\n",
      "tensor(11.1701, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [15360/111899 (14%)]\tLoss: 11.170090\n",
      "tensor(11.0711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [20480/111899 (18%)]\tLoss: 11.071057\n",
      "tensor(10.9731, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [25600/111899 (23%)]\tLoss: 10.973093\n",
      "tensor(11.1240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [30720/111899 (27%)]\tLoss: 11.124002\n",
      "tensor(11.1023, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [35840/111899 (32%)]\tLoss: 11.102328\n",
      "tensor(10.9506, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [40960/111899 (37%)]\tLoss: 10.950556\n",
      "tensor(10.9473, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [46080/111899 (41%)]\tLoss: 10.947288\n",
      "tensor(10.9602, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [51200/111899 (46%)]\tLoss: 10.960176\n",
      "tensor(10.9733, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [56320/111899 (50%)]\tLoss: 10.973310\n",
      "tensor(10.8929, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [61440/111899 (55%)]\tLoss: 10.892948\n",
      "tensor(10.8790, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [66560/111899 (59%)]\tLoss: 10.878965\n",
      "tensor(10.8890, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [71680/111899 (64%)]\tLoss: 10.888958\n",
      "tensor(10.8987, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [76800/111899 (68%)]\tLoss: 10.898694\n",
      "tensor(10.9530, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [81920/111899 (73%)]\tLoss: 10.953033\n",
      "tensor(10.8634, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [87040/111899 (78%)]\tLoss: 10.863365\n",
      "tensor(10.8793, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [92160/111899 (82%)]\tLoss: 10.879295\n",
      "tensor(10.7608, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [97280/111899 (87%)]\tLoss: 10.760773\n",
      "tensor(10.7467, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [102400/111899 (91%)]\tLoss: 10.746713\n",
      "tensor(10.8127, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 35 [107520/111899 (96%)]\tLoss: 10.812702\n",
      "\n",
      "Test Epoch: 35\tAttack_Accuracy: 161/412 (39%)\n",
      "\n",
      "\n",
      "Test Epoch: 35\tmaintain_Accuracy: 882/10593 (8%)\n",
      "\n",
      "tensor(10.8315, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [0/111899 (0%)]\tLoss: 10.831488\n",
      "tensor(10.8422, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [5120/111899 (5%)]\tLoss: 10.842180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8170, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [10240/111899 (9%)]\tLoss: 10.817032\n",
      "tensor(10.7060, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [15360/111899 (14%)]\tLoss: 10.705971\n",
      "tensor(10.6238, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [20480/111899 (18%)]\tLoss: 10.623798\n",
      "tensor(10.6393, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [25600/111899 (23%)]\tLoss: 10.639283\n",
      "tensor(10.6193, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [30720/111899 (27%)]\tLoss: 10.619346\n",
      "tensor(10.6063, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [35840/111899 (32%)]\tLoss: 10.606295\n",
      "tensor(10.6779, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [40960/111899 (37%)]\tLoss: 10.677938\n",
      "tensor(10.4613, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [46080/111899 (41%)]\tLoss: 10.461346\n",
      "tensor(10.5163, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [51200/111899 (46%)]\tLoss: 10.516264\n",
      "tensor(10.5604, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [56320/111899 (50%)]\tLoss: 10.560385\n",
      "tensor(10.5827, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [61440/111899 (55%)]\tLoss: 10.582664\n",
      "tensor(10.4872, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [66560/111899 (59%)]\tLoss: 10.487246\n",
      "tensor(10.5293, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [71680/111899 (64%)]\tLoss: 10.529253\n",
      "tensor(10.5513, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [76800/111899 (68%)]\tLoss: 10.551297\n",
      "tensor(10.3908, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [81920/111899 (73%)]\tLoss: 10.390759\n",
      "tensor(10.5296, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [87040/111899 (78%)]\tLoss: 10.529616\n",
      "tensor(10.4963, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [92160/111899 (82%)]\tLoss: 10.496277\n",
      "tensor(10.3142, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [97280/111899 (87%)]\tLoss: 10.314178\n",
      "tensor(10.4132, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [102400/111899 (91%)]\tLoss: 10.413213\n",
      "tensor(10.4379, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 36 [107520/111899 (96%)]\tLoss: 10.437943\n",
      "\n",
      "Test Epoch: 36\tAttack_Accuracy: 164/412 (40%)\n",
      "\n",
      "\n",
      "Test Epoch: 36\tmaintain_Accuracy: 897/10593 (8%)\n",
      "\n",
      "tensor(10.3550, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [0/111899 (0%)]\tLoss: 10.355019\n",
      "tensor(10.4033, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [5120/111899 (5%)]\tLoss: 10.403259\n",
      "tensor(10.3837, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [10240/111899 (9%)]\tLoss: 10.383728\n",
      "tensor(10.3394, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [15360/111899 (14%)]\tLoss: 10.339434\n",
      "tensor(10.2482, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [20480/111899 (18%)]\tLoss: 10.248188\n",
      "tensor(10.3407, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [25600/111899 (23%)]\tLoss: 10.340702\n",
      "tensor(10.2478, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [30720/111899 (27%)]\tLoss: 10.247829\n",
      "tensor(10.2385, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [35840/111899 (32%)]\tLoss: 10.238459\n",
      "tensor(10.3145, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [40960/111899 (37%)]\tLoss: 10.314528\n",
      "tensor(10.2999, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [46080/111899 (41%)]\tLoss: 10.299873\n",
      "tensor(10.2613, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [51200/111899 (46%)]\tLoss: 10.261299\n",
      "tensor(10.1488, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [56320/111899 (50%)]\tLoss: 10.148823\n",
      "tensor(10.1706, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [61440/111899 (55%)]\tLoss: 10.170629\n",
      "tensor(10.2806, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [66560/111899 (59%)]\tLoss: 10.280582\n",
      "tensor(10.2464, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [71680/111899 (64%)]\tLoss: 10.246378\n",
      "tensor(10.0762, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [76800/111899 (68%)]\tLoss: 10.076218\n",
      "tensor(10.0900, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [81920/111899 (73%)]\tLoss: 10.090034\n",
      "tensor(10.1728, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [87040/111899 (78%)]\tLoss: 10.172766\n",
      "tensor(10.1080, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [92160/111899 (82%)]\tLoss: 10.107952\n",
      "tensor(10.1027, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [97280/111899 (87%)]\tLoss: 10.102736\n",
      "tensor(10.0213, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [102400/111899 (91%)]\tLoss: 10.021258\n",
      "tensor(10.1297, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 37 [107520/111899 (96%)]\tLoss: 10.129663\n",
      "\n",
      "Test Epoch: 37\tAttack_Accuracy: 175/412 (42%)\n",
      "\n",
      "\n",
      "Test Epoch: 37\tmaintain_Accuracy: 948/10593 (9%)\n",
      "\n",
      "tensor(10.0938, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [0/111899 (0%)]\tLoss: 10.093840\n",
      "tensor(10.0807, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [5120/111899 (5%)]\tLoss: 10.080681\n",
      "tensor(10.0406, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [10240/111899 (9%)]\tLoss: 10.040574\n",
      "tensor(9.9645, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [15360/111899 (14%)]\tLoss: 9.964516\n",
      "tensor(10.1017, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [20480/111899 (18%)]\tLoss: 10.101698\n",
      "tensor(9.9454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [25600/111899 (23%)]\tLoss: 9.945388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0331, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [30720/111899 (27%)]\tLoss: 10.033079\n",
      "tensor(10.0095, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [35840/111899 (32%)]\tLoss: 10.009532\n",
      "tensor(9.9505, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [40960/111899 (37%)]\tLoss: 9.950506\n",
      "tensor(10.0045, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [46080/111899 (41%)]\tLoss: 10.004477\n",
      "tensor(9.9415, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [51200/111899 (46%)]\tLoss: 9.941484\n",
      "tensor(9.9743, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [56320/111899 (50%)]\tLoss: 9.974298\n",
      "tensor(9.9190, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [61440/111899 (55%)]\tLoss: 9.919010\n",
      "tensor(9.8330, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [66560/111899 (59%)]\tLoss: 9.832955\n",
      "tensor(9.8863, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [71680/111899 (64%)]\tLoss: 9.886253\n",
      "tensor(9.8842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [76800/111899 (68%)]\tLoss: 9.884230\n",
      "tensor(9.8826, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [81920/111899 (73%)]\tLoss: 9.882586\n",
      "tensor(9.8305, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [87040/111899 (78%)]\tLoss: 9.830523\n",
      "tensor(9.8438, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [92160/111899 (82%)]\tLoss: 9.843781\n",
      "tensor(9.9576, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [97280/111899 (87%)]\tLoss: 9.957553\n",
      "tensor(9.8788, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [102400/111899 (91%)]\tLoss: 9.878841\n",
      "tensor(9.8385, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 38 [107520/111899 (96%)]\tLoss: 9.838539\n",
      "\n",
      "Test Epoch: 38\tAttack_Accuracy: 142/412 (34%)\n",
      "\n",
      "\n",
      "Test Epoch: 38\tmaintain_Accuracy: 1003/10593 (9%)\n",
      "\n",
      "tensor(9.8609, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [0/111899 (0%)]\tLoss: 9.860926\n",
      "tensor(9.7731, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [5120/111899 (5%)]\tLoss: 9.773126\n",
      "tensor(9.7814, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [10240/111899 (9%)]\tLoss: 9.781438\n",
      "tensor(9.6823, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [15360/111899 (14%)]\tLoss: 9.682324\n",
      "tensor(9.6538, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [20480/111899 (18%)]\tLoss: 9.653841\n",
      "tensor(9.7937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [25600/111899 (23%)]\tLoss: 9.793664\n",
      "tensor(9.7424, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [30720/111899 (27%)]\tLoss: 9.742379\n",
      "tensor(9.6967, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [35840/111899 (32%)]\tLoss: 9.696672\n",
      "tensor(9.7160, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [40960/111899 (37%)]\tLoss: 9.716021\n",
      "tensor(9.6221, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [46080/111899 (41%)]\tLoss: 9.622063\n",
      "tensor(9.6844, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [51200/111899 (46%)]\tLoss: 9.684407\n",
      "tensor(9.7234, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [56320/111899 (50%)]\tLoss: 9.723373\n",
      "tensor(9.7023, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [61440/111899 (55%)]\tLoss: 9.702299\n",
      "tensor(9.5914, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [66560/111899 (59%)]\tLoss: 9.591351\n",
      "tensor(9.5782, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [71680/111899 (64%)]\tLoss: 9.578165\n",
      "tensor(9.5999, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [76800/111899 (68%)]\tLoss: 9.599883\n",
      "tensor(9.5646, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [81920/111899 (73%)]\tLoss: 9.564569\n",
      "tensor(9.5632, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [87040/111899 (78%)]\tLoss: 9.563171\n",
      "tensor(9.6409, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [92160/111899 (82%)]\tLoss: 9.640871\n",
      "tensor(9.4656, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [97280/111899 (87%)]\tLoss: 9.465552\n",
      "tensor(9.5199, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [102400/111899 (91%)]\tLoss: 9.519922\n",
      "tensor(9.5391, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 39 [107520/111899 (96%)]\tLoss: 9.539129\n",
      "\n",
      "Test Epoch: 39\tAttack_Accuracy: 104/412 (25%)\n",
      "\n",
      "\n",
      "Test Epoch: 39\tmaintain_Accuracy: 1079/10593 (10%)\n",
      "\n",
      "tensor(9.4546, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [0/111899 (0%)]\tLoss: 9.454557\n",
      "tensor(9.4963, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [5120/111899 (5%)]\tLoss: 9.496342\n",
      "tensor(9.5863, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [10240/111899 (9%)]\tLoss: 9.586298\n",
      "tensor(9.4920, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [15360/111899 (14%)]\tLoss: 9.491985\n",
      "tensor(9.5238, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [20480/111899 (18%)]\tLoss: 9.523837\n",
      "tensor(9.5311, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [25600/111899 (23%)]\tLoss: 9.531086\n",
      "tensor(9.4271, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [30720/111899 (27%)]\tLoss: 9.427127\n",
      "tensor(9.5505, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [35840/111899 (32%)]\tLoss: 9.550529\n",
      "tensor(9.4909, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [40960/111899 (37%)]\tLoss: 9.490884\n",
      "tensor(9.4986, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [46080/111899 (41%)]\tLoss: 9.498553\n",
      "tensor(9.4115, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [51200/111899 (46%)]\tLoss: 9.411452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.3816, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [56320/111899 (50%)]\tLoss: 9.381609\n",
      "tensor(9.4006, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [61440/111899 (55%)]\tLoss: 9.400573\n",
      "tensor(9.4529, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [66560/111899 (59%)]\tLoss: 9.452935\n",
      "tensor(9.4050, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [71680/111899 (64%)]\tLoss: 9.404997\n",
      "tensor(9.3944, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [76800/111899 (68%)]\tLoss: 9.394413\n",
      "tensor(9.3471, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [81920/111899 (73%)]\tLoss: 9.347088\n",
      "tensor(9.3073, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [87040/111899 (78%)]\tLoss: 9.307261\n",
      "tensor(9.2587, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [92160/111899 (82%)]\tLoss: 9.258709\n",
      "tensor(9.4123, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [97280/111899 (87%)]\tLoss: 9.412322\n",
      "tensor(9.2923, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [102400/111899 (91%)]\tLoss: 9.292270\n",
      "tensor(9.3110, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 40 [107520/111899 (96%)]\tLoss: 9.310997\n",
      "\n",
      "Test Epoch: 40\tAttack_Accuracy: 113/412 (27%)\n",
      "\n",
      "\n",
      "Test Epoch: 40\tmaintain_Accuracy: 1138/10593 (11%)\n",
      "\n",
      "tensor(9.2200, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [0/111899 (0%)]\tLoss: 9.219960\n",
      "tensor(9.3426, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [5120/111899 (5%)]\tLoss: 9.342607\n",
      "tensor(9.3369, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [10240/111899 (9%)]\tLoss: 9.336855\n",
      "tensor(9.3099, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [15360/111899 (14%)]\tLoss: 9.309942\n",
      "tensor(9.2596, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [20480/111899 (18%)]\tLoss: 9.259616\n",
      "tensor(9.2170, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [25600/111899 (23%)]\tLoss: 9.216972\n",
      "tensor(9.3202, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [30720/111899 (27%)]\tLoss: 9.320198\n",
      "tensor(9.2695, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [35840/111899 (32%)]\tLoss: 9.269509\n",
      "tensor(9.2135, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [40960/111899 (37%)]\tLoss: 9.213474\n",
      "tensor(9.2610, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [46080/111899 (41%)]\tLoss: 9.260959\n",
      "tensor(9.1516, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [51200/111899 (46%)]\tLoss: 9.151607\n",
      "tensor(9.2635, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [56320/111899 (50%)]\tLoss: 9.263517\n",
      "tensor(9.1949, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [61440/111899 (55%)]\tLoss: 9.194878\n",
      "tensor(9.1534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [66560/111899 (59%)]\tLoss: 9.153387\n",
      "tensor(9.1250, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [71680/111899 (64%)]\tLoss: 9.125042\n",
      "tensor(9.2134, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [76800/111899 (68%)]\tLoss: 9.213350\n",
      "tensor(9.1438, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [81920/111899 (73%)]\tLoss: 9.143797\n",
      "tensor(9.0987, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [87040/111899 (78%)]\tLoss: 9.098740\n",
      "tensor(9.1394, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [92160/111899 (82%)]\tLoss: 9.139415\n",
      "tensor(9.1844, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [97280/111899 (87%)]\tLoss: 9.184360\n",
      "tensor(9.1315, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [102400/111899 (91%)]\tLoss: 9.131475\n",
      "tensor(9.0839, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 41 [107520/111899 (96%)]\tLoss: 9.083925\n",
      "\n",
      "Test Epoch: 41\tAttack_Accuracy: 114/412 (28%)\n",
      "\n",
      "\n",
      "Test Epoch: 41\tmaintain_Accuracy: 1173/10593 (11%)\n",
      "\n",
      "tensor(9.0795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [0/111899 (0%)]\tLoss: 9.079534\n",
      "tensor(9.0050, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [5120/111899 (5%)]\tLoss: 9.005014\n",
      "tensor(9.0585, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [10240/111899 (9%)]\tLoss: 9.058525\n",
      "tensor(9.0738, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [15360/111899 (14%)]\tLoss: 9.073832\n",
      "tensor(9.1042, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [20480/111899 (18%)]\tLoss: 9.104156\n",
      "tensor(9.0336, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [25600/111899 (23%)]\tLoss: 9.033633\n",
      "tensor(9.1093, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [30720/111899 (27%)]\tLoss: 9.109289\n",
      "tensor(9.0989, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [35840/111899 (32%)]\tLoss: 9.098917\n",
      "tensor(9.0774, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [40960/111899 (37%)]\tLoss: 9.077377\n",
      "tensor(8.9440, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [46080/111899 (41%)]\tLoss: 8.943988\n",
      "tensor(9.0137, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [51200/111899 (46%)]\tLoss: 9.013668\n",
      "tensor(8.9590, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [56320/111899 (50%)]\tLoss: 8.958994\n",
      "tensor(8.9901, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [61440/111899 (55%)]\tLoss: 8.990082\n",
      "tensor(8.9850, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [66560/111899 (59%)]\tLoss: 8.985012\n",
      "tensor(8.9668, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [71680/111899 (64%)]\tLoss: 8.966835\n",
      "tensor(8.9593, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [76800/111899 (68%)]\tLoss: 8.959259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.9753, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [81920/111899 (73%)]\tLoss: 8.975292\n",
      "tensor(8.9436, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [87040/111899 (78%)]\tLoss: 8.943636\n",
      "tensor(8.9336, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [92160/111899 (82%)]\tLoss: 8.933571\n",
      "tensor(8.9969, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [97280/111899 (87%)]\tLoss: 8.996944\n",
      "tensor(8.9519, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [102400/111899 (91%)]\tLoss: 8.951947\n",
      "tensor(8.9435, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 42 [107520/111899 (96%)]\tLoss: 8.943490\n",
      "\n",
      "Test Epoch: 42\tAttack_Accuracy: 120/412 (29%)\n",
      "\n",
      "\n",
      "Test Epoch: 42\tmaintain_Accuracy: 1201/10593 (11%)\n",
      "\n",
      "tensor(8.9134, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [0/111899 (0%)]\tLoss: 8.913363\n",
      "tensor(8.8734, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [5120/111899 (5%)]\tLoss: 8.873440\n",
      "tensor(8.8105, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [10240/111899 (9%)]\tLoss: 8.810503\n",
      "tensor(8.8937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [15360/111899 (14%)]\tLoss: 8.893723\n",
      "tensor(8.9126, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [20480/111899 (18%)]\tLoss: 8.912568\n",
      "tensor(8.9644, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [25600/111899 (23%)]\tLoss: 8.964415\n",
      "tensor(8.8057, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [30720/111899 (27%)]\tLoss: 8.805692\n",
      "tensor(8.7851, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [35840/111899 (32%)]\tLoss: 8.785089\n",
      "tensor(8.8514, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [40960/111899 (37%)]\tLoss: 8.851419\n",
      "tensor(8.8126, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [46080/111899 (41%)]\tLoss: 8.812622\n",
      "tensor(8.7711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [51200/111899 (46%)]\tLoss: 8.771133\n",
      "tensor(8.8534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [56320/111899 (50%)]\tLoss: 8.853397\n",
      "tensor(8.8251, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [61440/111899 (55%)]\tLoss: 8.825064\n",
      "tensor(8.7136, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [66560/111899 (59%)]\tLoss: 8.713561\n",
      "tensor(8.7526, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [71680/111899 (64%)]\tLoss: 8.752645\n",
      "tensor(8.7344, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [76800/111899 (68%)]\tLoss: 8.734387\n",
      "tensor(8.7179, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [81920/111899 (73%)]\tLoss: 8.717927\n",
      "tensor(8.7893, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [87040/111899 (78%)]\tLoss: 8.789346\n",
      "tensor(8.6994, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [92160/111899 (82%)]\tLoss: 8.699430\n",
      "tensor(8.7413, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [97280/111899 (87%)]\tLoss: 8.741265\n",
      "tensor(8.7372, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [102400/111899 (91%)]\tLoss: 8.737232\n",
      "tensor(8.7875, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 43 [107520/111899 (96%)]\tLoss: 8.787516\n",
      "\n",
      "Test Epoch: 43\tAttack_Accuracy: 99/412 (24%)\n",
      "\n",
      "\n",
      "Test Epoch: 43\tmaintain_Accuracy: 1279/10593 (12%)\n",
      "\n",
      "tensor(8.7123, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [0/111899 (0%)]\tLoss: 8.712257\n",
      "tensor(8.8029, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [5120/111899 (5%)]\tLoss: 8.802901\n",
      "tensor(8.7458, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [10240/111899 (9%)]\tLoss: 8.745804\n",
      "tensor(8.6463, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [15360/111899 (14%)]\tLoss: 8.646338\n",
      "tensor(8.7043, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [20480/111899 (18%)]\tLoss: 8.704292\n",
      "tensor(8.6073, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [25600/111899 (23%)]\tLoss: 8.607332\n",
      "tensor(8.6333, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [30720/111899 (27%)]\tLoss: 8.633348\n",
      "tensor(8.6985, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [35840/111899 (32%)]\tLoss: 8.698542\n",
      "tensor(8.7336, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [40960/111899 (37%)]\tLoss: 8.733635\n",
      "tensor(8.6103, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [46080/111899 (41%)]\tLoss: 8.610329\n",
      "tensor(8.5803, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [51200/111899 (46%)]\tLoss: 8.580295\n",
      "tensor(8.6694, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [56320/111899 (50%)]\tLoss: 8.669433\n",
      "tensor(8.6772, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [61440/111899 (55%)]\tLoss: 8.677231\n",
      "tensor(8.5857, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [66560/111899 (59%)]\tLoss: 8.585669\n",
      "tensor(8.6259, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [71680/111899 (64%)]\tLoss: 8.625854\n",
      "tensor(8.6183, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [76800/111899 (68%)]\tLoss: 8.618347\n",
      "tensor(8.6451, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [81920/111899 (73%)]\tLoss: 8.645138\n",
      "tensor(8.6242, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [87040/111899 (78%)]\tLoss: 8.624226\n",
      "tensor(8.5733, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [92160/111899 (82%)]\tLoss: 8.573305\n",
      "tensor(8.5811, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [97280/111899 (87%)]\tLoss: 8.581122\n",
      "tensor(8.5174, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [102400/111899 (91%)]\tLoss: 8.517417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5467, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 44 [107520/111899 (96%)]\tLoss: 8.546741\n",
      "\n",
      "Test Epoch: 44\tAttack_Accuracy: 102/412 (25%)\n",
      "\n",
      "\n",
      "Test Epoch: 44\tmaintain_Accuracy: 1398/10593 (13%)\n",
      "\n",
      "tensor(8.4793, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [0/111899 (0%)]\tLoss: 8.479286\n",
      "tensor(8.4875, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [5120/111899 (5%)]\tLoss: 8.487518\n",
      "tensor(8.3904, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [10240/111899 (9%)]\tLoss: 8.390436\n",
      "tensor(8.5256, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [15360/111899 (14%)]\tLoss: 8.525601\n",
      "tensor(8.4842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [20480/111899 (18%)]\tLoss: 8.484231\n",
      "tensor(8.4567, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [25600/111899 (23%)]\tLoss: 8.456730\n",
      "tensor(8.4423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [30720/111899 (27%)]\tLoss: 8.442320\n",
      "tensor(8.4256, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [35840/111899 (32%)]\tLoss: 8.425563\n",
      "tensor(8.4443, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [40960/111899 (37%)]\tLoss: 8.444319\n",
      "tensor(8.4816, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [46080/111899 (41%)]\tLoss: 8.481556\n",
      "tensor(8.4497, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [51200/111899 (46%)]\tLoss: 8.449725\n",
      "tensor(8.3785, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [56320/111899 (50%)]\tLoss: 8.378471\n",
      "tensor(8.3947, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [61440/111899 (55%)]\tLoss: 8.394671\n",
      "tensor(8.4356, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [66560/111899 (59%)]\tLoss: 8.435596\n",
      "tensor(8.3199, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [71680/111899 (64%)]\tLoss: 8.319892\n",
      "tensor(8.3622, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [76800/111899 (68%)]\tLoss: 8.362155\n",
      "tensor(8.4258, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [81920/111899 (73%)]\tLoss: 8.425806\n",
      "tensor(8.3823, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [87040/111899 (78%)]\tLoss: 8.382290\n",
      "tensor(8.4324, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [92160/111899 (82%)]\tLoss: 8.432376\n",
      "tensor(8.3418, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [97280/111899 (87%)]\tLoss: 8.341839\n",
      "tensor(8.2623, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [102400/111899 (91%)]\tLoss: 8.262298\n",
      "tensor(8.3534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 45 [107520/111899 (96%)]\tLoss: 8.353442\n",
      "\n",
      "Test Epoch: 45\tAttack_Accuracy: 94/412 (23%)\n",
      "\n",
      "\n",
      "Test Epoch: 45\tmaintain_Accuracy: 1468/10593 (14%)\n",
      "\n",
      "tensor(8.3432, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [0/111899 (0%)]\tLoss: 8.343217\n",
      "tensor(8.3078, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [5120/111899 (5%)]\tLoss: 8.307831\n",
      "tensor(8.3040, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [10240/111899 (9%)]\tLoss: 8.303995\n",
      "tensor(8.2002, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [15360/111899 (14%)]\tLoss: 8.200199\n",
      "tensor(8.2803, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [20480/111899 (18%)]\tLoss: 8.280333\n",
      "tensor(8.3438, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [25600/111899 (23%)]\tLoss: 8.343824\n",
      "tensor(8.3429, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [30720/111899 (27%)]\tLoss: 8.342860\n",
      "tensor(8.4240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [35840/111899 (32%)]\tLoss: 8.423987\n",
      "tensor(8.2918, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [40960/111899 (37%)]\tLoss: 8.291846\n",
      "tensor(8.2756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [46080/111899 (41%)]\tLoss: 8.275560\n",
      "tensor(8.2364, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [51200/111899 (46%)]\tLoss: 8.236412\n",
      "tensor(8.2926, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [56320/111899 (50%)]\tLoss: 8.292637\n",
      "tensor(8.2575, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [61440/111899 (55%)]\tLoss: 8.257534\n",
      "tensor(8.3770, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [66560/111899 (59%)]\tLoss: 8.376997\n",
      "tensor(8.3741, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [71680/111899 (64%)]\tLoss: 8.374114\n",
      "tensor(8.2724, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [76800/111899 (68%)]\tLoss: 8.272418\n",
      "tensor(8.3042, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [81920/111899 (73%)]\tLoss: 8.304192\n",
      "tensor(8.3534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [87040/111899 (78%)]\tLoss: 8.353424\n",
      "tensor(8.3356, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [92160/111899 (82%)]\tLoss: 8.335644\n",
      "tensor(8.3481, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [97280/111899 (87%)]\tLoss: 8.348114\n",
      "tensor(8.3338, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [102400/111899 (91%)]\tLoss: 8.333777\n",
      "tensor(8.3071, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 46 [107520/111899 (96%)]\tLoss: 8.307076\n",
      "\n",
      "Test Epoch: 46\tAttack_Accuracy: 105/412 (25%)\n",
      "\n",
      "\n",
      "Test Epoch: 46\tmaintain_Accuracy: 1493/10593 (14%)\n",
      "\n",
      "tensor(8.2928, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [0/111899 (0%)]\tLoss: 8.292823\n",
      "tensor(8.2551, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [5120/111899 (5%)]\tLoss: 8.255090\n",
      "tensor(8.2333, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [10240/111899 (9%)]\tLoss: 8.233283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.3353, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [15360/111899 (14%)]\tLoss: 8.335332\n",
      "tensor(8.3167, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [20480/111899 (18%)]\tLoss: 8.316712\n",
      "tensor(8.2959, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [25600/111899 (23%)]\tLoss: 8.295855\n",
      "tensor(8.2452, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [30720/111899 (27%)]\tLoss: 8.245177\n",
      "tensor(8.2203, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [35840/111899 (32%)]\tLoss: 8.220349\n",
      "tensor(8.3124, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [40960/111899 (37%)]\tLoss: 8.312422\n",
      "tensor(8.2681, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [46080/111899 (41%)]\tLoss: 8.268097\n",
      "tensor(8.2032, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [51200/111899 (46%)]\tLoss: 8.203200\n",
      "tensor(8.2300, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [56320/111899 (50%)]\tLoss: 8.229980\n",
      "tensor(8.2417, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [61440/111899 (55%)]\tLoss: 8.241694\n",
      "tensor(8.1679, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [66560/111899 (59%)]\tLoss: 8.167946\n",
      "tensor(8.2316, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [71680/111899 (64%)]\tLoss: 8.231627\n",
      "tensor(8.1461, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [76800/111899 (68%)]\tLoss: 8.146114\n",
      "tensor(8.3180, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [81920/111899 (73%)]\tLoss: 8.317966\n",
      "tensor(8.0928, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [87040/111899 (78%)]\tLoss: 8.092787\n",
      "tensor(8.2462, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [92160/111899 (82%)]\tLoss: 8.246217\n",
      "tensor(8.2472, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [97280/111899 (87%)]\tLoss: 8.247201\n",
      "tensor(8.1999, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [102400/111899 (91%)]\tLoss: 8.199903\n",
      "tensor(8.2136, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 47 [107520/111899 (96%)]\tLoss: 8.213570\n",
      "\n",
      "Test Epoch: 47\tAttack_Accuracy: 76/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 47\tmaintain_Accuracy: 1507/10593 (14%)\n",
      "\n",
      "tensor(8.1654, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [0/111899 (0%)]\tLoss: 8.165407\n",
      "tensor(8.1528, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [5120/111899 (5%)]\tLoss: 8.152792\n",
      "tensor(8.1575, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [10240/111899 (9%)]\tLoss: 8.157509\n",
      "tensor(8.0939, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [15360/111899 (14%)]\tLoss: 8.093867\n",
      "tensor(8.0962, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [20480/111899 (18%)]\tLoss: 8.096228\n",
      "tensor(8.1964, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [25600/111899 (23%)]\tLoss: 8.196427\n",
      "tensor(8.2150, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [30720/111899 (27%)]\tLoss: 8.214958\n",
      "tensor(8.1523, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [35840/111899 (32%)]\tLoss: 8.152276\n",
      "tensor(8.1142, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [40960/111899 (37%)]\tLoss: 8.114164\n",
      "tensor(8.1900, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [46080/111899 (41%)]\tLoss: 8.190042\n",
      "tensor(8.2240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [51200/111899 (46%)]\tLoss: 8.224044\n",
      "tensor(8.1097, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [56320/111899 (50%)]\tLoss: 8.109674\n",
      "tensor(8.1018, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [61440/111899 (55%)]\tLoss: 8.101810\n",
      "tensor(8.0306, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [66560/111899 (59%)]\tLoss: 8.030613\n",
      "tensor(8.0857, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [71680/111899 (64%)]\tLoss: 8.085652\n",
      "tensor(8.0885, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [76800/111899 (68%)]\tLoss: 8.088531\n",
      "tensor(8.1527, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [81920/111899 (73%)]\tLoss: 8.152700\n",
      "tensor(8.1495, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [87040/111899 (78%)]\tLoss: 8.149534\n",
      "tensor(8.1566, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [92160/111899 (82%)]\tLoss: 8.156645\n",
      "tensor(8.1340, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [97280/111899 (87%)]\tLoss: 8.133950\n",
      "tensor(8.0851, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [102400/111899 (91%)]\tLoss: 8.085121\n",
      "tensor(8.0273, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 48 [107520/111899 (96%)]\tLoss: 8.027342\n",
      "\n",
      "Test Epoch: 48\tAttack_Accuracy: 83/412 (20%)\n",
      "\n",
      "\n",
      "Test Epoch: 48\tmaintain_Accuracy: 1507/10593 (14%)\n",
      "\n",
      "tensor(8.0991, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [0/111899 (0%)]\tLoss: 8.099064\n",
      "tensor(8.1589, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [5120/111899 (5%)]\tLoss: 8.158930\n",
      "tensor(8.0157, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [10240/111899 (9%)]\tLoss: 8.015722\n",
      "tensor(8.1045, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [15360/111899 (14%)]\tLoss: 8.104533\n",
      "tensor(8.0322, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [20480/111899 (18%)]\tLoss: 8.032191\n",
      "tensor(7.9545, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [25600/111899 (23%)]\tLoss: 7.954460\n",
      "tensor(8.0375, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [30720/111899 (27%)]\tLoss: 8.037531\n",
      "tensor(7.8784, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [35840/111899 (32%)]\tLoss: 7.878436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.9786, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [40960/111899 (37%)]\tLoss: 7.978575\n",
      "tensor(8.0353, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [46080/111899 (41%)]\tLoss: 8.035276\n",
      "tensor(7.9158, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [51200/111899 (46%)]\tLoss: 7.915772\n",
      "tensor(7.9831, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [56320/111899 (50%)]\tLoss: 7.983068\n",
      "tensor(7.9795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [61440/111899 (55%)]\tLoss: 7.979486\n",
      "tensor(8.0597, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [66560/111899 (59%)]\tLoss: 8.059726\n",
      "tensor(7.9747, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [71680/111899 (64%)]\tLoss: 7.974696\n",
      "tensor(7.9937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [76800/111899 (68%)]\tLoss: 7.993703\n",
      "tensor(7.9292, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [81920/111899 (73%)]\tLoss: 7.929206\n",
      "tensor(7.9932, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [87040/111899 (78%)]\tLoss: 7.993189\n",
      "tensor(7.9284, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [92160/111899 (82%)]\tLoss: 7.928364\n",
      "tensor(8.0159, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [97280/111899 (87%)]\tLoss: 8.015886\n",
      "tensor(8.0836, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [102400/111899 (91%)]\tLoss: 8.083591\n",
      "tensor(7.9161, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 49 [107520/111899 (96%)]\tLoss: 7.916125\n",
      "\n",
      "Test Epoch: 49\tAttack_Accuracy: 73/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 49\tmaintain_Accuracy: 1569/10593 (15%)\n",
      "\n",
      "tensor(7.9077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [0/111899 (0%)]\tLoss: 7.907668\n",
      "tensor(7.9615, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [5120/111899 (5%)]\tLoss: 7.961515\n",
      "tensor(7.9603, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [10240/111899 (9%)]\tLoss: 7.960319\n",
      "tensor(7.9187, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [15360/111899 (14%)]\tLoss: 7.918660\n",
      "tensor(7.9218, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [20480/111899 (18%)]\tLoss: 7.921816\n",
      "tensor(7.9217, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [25600/111899 (23%)]\tLoss: 7.921709\n",
      "tensor(7.9807, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [30720/111899 (27%)]\tLoss: 7.980692\n",
      "tensor(7.8428, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [35840/111899 (32%)]\tLoss: 7.842790\n",
      "tensor(7.9600, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [40960/111899 (37%)]\tLoss: 7.959963\n",
      "tensor(7.8505, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [46080/111899 (41%)]\tLoss: 7.850451\n",
      "tensor(7.9956, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [51200/111899 (46%)]\tLoss: 7.995555\n",
      "tensor(7.8913, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [56320/111899 (50%)]\tLoss: 7.891294\n",
      "tensor(7.9106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [61440/111899 (55%)]\tLoss: 7.910608\n",
      "tensor(7.8503, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [66560/111899 (59%)]\tLoss: 7.850325\n",
      "tensor(7.8582, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [71680/111899 (64%)]\tLoss: 7.858182\n",
      "tensor(7.8065, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [76800/111899 (68%)]\tLoss: 7.806537\n",
      "tensor(7.8718, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [81920/111899 (73%)]\tLoss: 7.871810\n",
      "tensor(7.7910, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [87040/111899 (78%)]\tLoss: 7.790954\n",
      "tensor(7.9133, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [92160/111899 (82%)]\tLoss: 7.913269\n",
      "tensor(7.8871, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [97280/111899 (87%)]\tLoss: 7.887058\n",
      "tensor(7.8100, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [102400/111899 (91%)]\tLoss: 7.809951\n",
      "tensor(7.7660, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 50 [107520/111899 (96%)]\tLoss: 7.765983\n",
      "\n",
      "Test Epoch: 50\tAttack_Accuracy: 77/412 (19%)\n",
      "\n",
      "\n",
      "Test Epoch: 50\tmaintain_Accuracy: 1571/10593 (15%)\n",
      "\n",
      "tensor(7.8627, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [0/111899 (0%)]\tLoss: 7.862721\n",
      "tensor(7.8166, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [5120/111899 (5%)]\tLoss: 7.816562\n",
      "tensor(7.7973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [10240/111899 (9%)]\tLoss: 7.797287\n",
      "tensor(7.8060, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [15360/111899 (14%)]\tLoss: 7.806018\n",
      "tensor(7.8063, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [20480/111899 (18%)]\tLoss: 7.806260\n",
      "tensor(7.7712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [25600/111899 (23%)]\tLoss: 7.771177\n",
      "tensor(7.9350, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [30720/111899 (27%)]\tLoss: 7.934968\n",
      "tensor(7.9390, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [35840/111899 (32%)]\tLoss: 7.938964\n",
      "tensor(7.7856, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [40960/111899 (37%)]\tLoss: 7.785613\n",
      "tensor(7.8577, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [46080/111899 (41%)]\tLoss: 7.857729\n",
      "tensor(7.9169, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [51200/111899 (46%)]\tLoss: 7.916943\n",
      "tensor(7.8581, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [56320/111899 (50%)]\tLoss: 7.858083\n",
      "tensor(7.7861, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [61440/111899 (55%)]\tLoss: 7.786122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.9027, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [66560/111899 (59%)]\tLoss: 7.902749\n",
      "tensor(7.9423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [71680/111899 (64%)]\tLoss: 7.942299\n",
      "tensor(7.7426, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [76800/111899 (68%)]\tLoss: 7.742649\n",
      "tensor(7.8057, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [81920/111899 (73%)]\tLoss: 7.805726\n",
      "tensor(7.7341, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [87040/111899 (78%)]\tLoss: 7.734078\n",
      "tensor(7.7196, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [92160/111899 (82%)]\tLoss: 7.719633\n",
      "tensor(7.8350, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [97280/111899 (87%)]\tLoss: 7.835033\n",
      "tensor(7.7894, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [102400/111899 (91%)]\tLoss: 7.789390\n",
      "tensor(7.8842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 51 [107520/111899 (96%)]\tLoss: 7.884239\n",
      "\n",
      "Test Epoch: 51\tAttack_Accuracy: 67/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 51\tmaintain_Accuracy: 1621/10593 (15%)\n",
      "\n",
      "tensor(7.7890, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [0/111899 (0%)]\tLoss: 7.788958\n",
      "tensor(7.8146, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [5120/111899 (5%)]\tLoss: 7.814569\n",
      "tensor(7.7481, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [10240/111899 (9%)]\tLoss: 7.748120\n",
      "tensor(7.7906, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [15360/111899 (14%)]\tLoss: 7.790584\n",
      "tensor(7.6801, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [20480/111899 (18%)]\tLoss: 7.680095\n",
      "tensor(7.7615, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [25600/111899 (23%)]\tLoss: 7.761469\n",
      "tensor(7.7576, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [30720/111899 (27%)]\tLoss: 7.757587\n",
      "tensor(7.7073, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [35840/111899 (32%)]\tLoss: 7.707337\n",
      "tensor(7.7502, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [40960/111899 (37%)]\tLoss: 7.750180\n",
      "tensor(7.7262, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [46080/111899 (41%)]\tLoss: 7.726177\n",
      "tensor(7.7478, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [51200/111899 (46%)]\tLoss: 7.747810\n",
      "tensor(7.6169, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [56320/111899 (50%)]\tLoss: 7.616933\n",
      "tensor(7.6946, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [61440/111899 (55%)]\tLoss: 7.694637\n",
      "tensor(7.6585, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [66560/111899 (59%)]\tLoss: 7.658488\n",
      "tensor(7.7969, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [71680/111899 (64%)]\tLoss: 7.796928\n",
      "tensor(7.6581, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [76800/111899 (68%)]\tLoss: 7.658063\n",
      "tensor(7.6275, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [81920/111899 (73%)]\tLoss: 7.627545\n",
      "tensor(7.7705, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [87040/111899 (78%)]\tLoss: 7.770475\n",
      "tensor(7.6363, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [92160/111899 (82%)]\tLoss: 7.636272\n",
      "tensor(7.5642, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [97280/111899 (87%)]\tLoss: 7.564152\n",
      "tensor(7.6793, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [102400/111899 (91%)]\tLoss: 7.679258\n",
      "tensor(7.6837, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 52 [107520/111899 (96%)]\tLoss: 7.683693\n",
      "\n",
      "Test Epoch: 52\tAttack_Accuracy: 83/412 (20%)\n",
      "\n",
      "\n",
      "Test Epoch: 52\tmaintain_Accuracy: 1647/10593 (16%)\n",
      "\n",
      "tensor(7.7250, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [0/111899 (0%)]\tLoss: 7.724972\n",
      "tensor(7.7254, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [5120/111899 (5%)]\tLoss: 7.725351\n",
      "tensor(7.6758, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [10240/111899 (9%)]\tLoss: 7.675800\n",
      "tensor(7.7512, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [15360/111899 (14%)]\tLoss: 7.751182\n",
      "tensor(7.6832, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [20480/111899 (18%)]\tLoss: 7.683246\n",
      "tensor(7.6352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [25600/111899 (23%)]\tLoss: 7.635210\n",
      "tensor(7.6836, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [30720/111899 (27%)]\tLoss: 7.683643\n",
      "tensor(7.5960, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [35840/111899 (32%)]\tLoss: 7.596025\n",
      "tensor(7.6620, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [40960/111899 (37%)]\tLoss: 7.662034\n",
      "tensor(7.5951, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [46080/111899 (41%)]\tLoss: 7.595143\n",
      "tensor(7.6192, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [51200/111899 (46%)]\tLoss: 7.619212\n",
      "tensor(7.6808, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [56320/111899 (50%)]\tLoss: 7.680794\n",
      "tensor(7.5364, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [61440/111899 (55%)]\tLoss: 7.536386\n",
      "tensor(7.6572, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [66560/111899 (59%)]\tLoss: 7.657170\n",
      "tensor(7.6135, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [71680/111899 (64%)]\tLoss: 7.613458\n",
      "tensor(7.5603, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [76800/111899 (68%)]\tLoss: 7.560273\n",
      "tensor(7.6385, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [81920/111899 (73%)]\tLoss: 7.638504\n",
      "tensor(7.5106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [87040/111899 (78%)]\tLoss: 7.510633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [92160/111899 (82%)]\tLoss: 7.593723\n",
      "tensor(7.6443, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [97280/111899 (87%)]\tLoss: 7.644288\n",
      "tensor(7.6310, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [102400/111899 (91%)]\tLoss: 7.631019\n",
      "tensor(7.5235, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 53 [107520/111899 (96%)]\tLoss: 7.523501\n",
      "\n",
      "Test Epoch: 53\tAttack_Accuracy: 86/412 (21%)\n",
      "\n",
      "\n",
      "Test Epoch: 53\tmaintain_Accuracy: 1651/10593 (16%)\n",
      "\n",
      "tensor(7.7208, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [0/111899 (0%)]\tLoss: 7.720805\n",
      "tensor(7.5852, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [5120/111899 (5%)]\tLoss: 7.585222\n",
      "tensor(7.5671, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [10240/111899 (9%)]\tLoss: 7.567105\n",
      "tensor(7.5960, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [15360/111899 (14%)]\tLoss: 7.595995\n",
      "tensor(7.4978, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [20480/111899 (18%)]\tLoss: 7.497814\n",
      "tensor(7.5752, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [25600/111899 (23%)]\tLoss: 7.575197\n",
      "tensor(7.5866, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [30720/111899 (27%)]\tLoss: 7.586603\n",
      "tensor(7.5408, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [35840/111899 (32%)]\tLoss: 7.540771\n",
      "tensor(7.5665, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [40960/111899 (37%)]\tLoss: 7.566504\n",
      "tensor(7.5672, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [46080/111899 (41%)]\tLoss: 7.567243\n",
      "tensor(7.5420, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [51200/111899 (46%)]\tLoss: 7.542037\n",
      "tensor(7.4954, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [56320/111899 (50%)]\tLoss: 7.495361\n",
      "tensor(7.4731, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [61440/111899 (55%)]\tLoss: 7.473104\n",
      "tensor(7.5504, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [66560/111899 (59%)]\tLoss: 7.550380\n",
      "tensor(7.6071, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [71680/111899 (64%)]\tLoss: 7.607126\n",
      "tensor(7.5712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [76800/111899 (68%)]\tLoss: 7.571196\n",
      "tensor(7.6408, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [81920/111899 (73%)]\tLoss: 7.640841\n",
      "tensor(7.5041, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [87040/111899 (78%)]\tLoss: 7.504138\n",
      "tensor(7.4429, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [92160/111899 (82%)]\tLoss: 7.442856\n",
      "tensor(7.5266, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [97280/111899 (87%)]\tLoss: 7.526629\n",
      "tensor(7.4581, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [102400/111899 (91%)]\tLoss: 7.458054\n",
      "tensor(7.5889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 54 [107520/111899 (96%)]\tLoss: 7.588888\n",
      "\n",
      "Test Epoch: 54\tAttack_Accuracy: 59/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 54\tmaintain_Accuracy: 1696/10593 (16%)\n",
      "\n",
      "tensor(7.5143, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [0/111899 (0%)]\tLoss: 7.514287\n",
      "tensor(7.4683, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [5120/111899 (5%)]\tLoss: 7.468308\n",
      "tensor(7.4331, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [10240/111899 (9%)]\tLoss: 7.433096\n",
      "tensor(7.4489, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [15360/111899 (14%)]\tLoss: 7.448917\n",
      "tensor(7.4950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [20480/111899 (18%)]\tLoss: 7.494950\n",
      "tensor(7.5048, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [25600/111899 (23%)]\tLoss: 7.504848\n",
      "tensor(7.4618, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [30720/111899 (27%)]\tLoss: 7.461834\n",
      "tensor(7.4362, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [35840/111899 (32%)]\tLoss: 7.436197\n",
      "tensor(7.4712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [40960/111899 (37%)]\tLoss: 7.471171\n",
      "tensor(7.4263, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [46080/111899 (41%)]\tLoss: 7.426341\n",
      "tensor(7.4282, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [51200/111899 (46%)]\tLoss: 7.428190\n",
      "tensor(7.5276, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [56320/111899 (50%)]\tLoss: 7.527609\n",
      "tensor(7.5439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [61440/111899 (55%)]\tLoss: 7.543906\n",
      "tensor(7.4281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [66560/111899 (59%)]\tLoss: 7.428083\n",
      "tensor(7.4061, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [71680/111899 (64%)]\tLoss: 7.406065\n",
      "tensor(7.4284, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [76800/111899 (68%)]\tLoss: 7.428367\n",
      "tensor(7.4407, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [81920/111899 (73%)]\tLoss: 7.440679\n",
      "tensor(7.5031, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [87040/111899 (78%)]\tLoss: 7.503059\n",
      "tensor(7.3768, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [92160/111899 (82%)]\tLoss: 7.376781\n",
      "tensor(7.4435, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [97280/111899 (87%)]\tLoss: 7.443491\n",
      "tensor(7.4290, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [102400/111899 (91%)]\tLoss: 7.429023\n",
      "tensor(7.3685, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 55 [107520/111899 (96%)]\tLoss: 7.368474\n",
      "\n",
      "Test Epoch: 55\tAttack_Accuracy: 81/412 (20%)\n",
      "\n",
      "\n",
      "Test Epoch: 55\tmaintain_Accuracy: 1693/10593 (16%)\n",
      "\n",
      "tensor(7.4093, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [0/111899 (0%)]\tLoss: 7.409292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4908, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [5120/111899 (5%)]\tLoss: 7.490770\n",
      "tensor(7.3903, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [10240/111899 (9%)]\tLoss: 7.390251\n",
      "tensor(7.4390, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [15360/111899 (14%)]\tLoss: 7.438967\n",
      "tensor(7.4221, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [20480/111899 (18%)]\tLoss: 7.422079\n",
      "tensor(7.4213, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [25600/111899 (23%)]\tLoss: 7.421269\n",
      "tensor(7.3553, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [30720/111899 (27%)]\tLoss: 7.355274\n",
      "tensor(7.3540, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [35840/111899 (32%)]\tLoss: 7.353972\n",
      "tensor(7.3577, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [40960/111899 (37%)]\tLoss: 7.357723\n",
      "tensor(7.3448, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [46080/111899 (41%)]\tLoss: 7.344792\n",
      "tensor(7.3797, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [51200/111899 (46%)]\tLoss: 7.379678\n",
      "tensor(7.3803, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [56320/111899 (50%)]\tLoss: 7.380281\n",
      "tensor(7.4635, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [61440/111899 (55%)]\tLoss: 7.463535\n",
      "tensor(7.3541, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [66560/111899 (59%)]\tLoss: 7.354135\n",
      "tensor(7.2904, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [71680/111899 (64%)]\tLoss: 7.290445\n",
      "tensor(7.3011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [76800/111899 (68%)]\tLoss: 7.301107\n",
      "tensor(7.3099, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [81920/111899 (73%)]\tLoss: 7.309941\n",
      "tensor(7.4294, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [87040/111899 (78%)]\tLoss: 7.429352\n",
      "tensor(7.3561, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [92160/111899 (82%)]\tLoss: 7.356133\n",
      "tensor(7.3708, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [97280/111899 (87%)]\tLoss: 7.370799\n",
      "tensor(7.3789, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [102400/111899 (91%)]\tLoss: 7.378939\n",
      "tensor(7.3852, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 56 [107520/111899 (96%)]\tLoss: 7.385155\n",
      "\n",
      "Test Epoch: 56\tAttack_Accuracy: 67/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 56\tmaintain_Accuracy: 1705/10593 (16%)\n",
      "\n",
      "tensor(7.2784, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [0/111899 (0%)]\tLoss: 7.278428\n",
      "tensor(7.3043, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [5120/111899 (5%)]\tLoss: 7.304296\n",
      "tensor(7.3605, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [10240/111899 (9%)]\tLoss: 7.360496\n",
      "tensor(7.2838, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [15360/111899 (14%)]\tLoss: 7.283827\n",
      "tensor(7.3259, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [20480/111899 (18%)]\tLoss: 7.325873\n",
      "tensor(7.3241, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [25600/111899 (23%)]\tLoss: 7.324087\n",
      "tensor(7.3078, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [30720/111899 (27%)]\tLoss: 7.307839\n",
      "tensor(7.3162, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [35840/111899 (32%)]\tLoss: 7.316182\n",
      "tensor(7.3139, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [40960/111899 (37%)]\tLoss: 7.313890\n",
      "tensor(7.3471, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [46080/111899 (41%)]\tLoss: 7.347124\n",
      "tensor(7.2583, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [51200/111899 (46%)]\tLoss: 7.258305\n",
      "tensor(7.3431, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [56320/111899 (50%)]\tLoss: 7.343103\n",
      "tensor(7.2904, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [61440/111899 (55%)]\tLoss: 7.290371\n",
      "tensor(7.2712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [66560/111899 (59%)]\tLoss: 7.271235\n",
      "tensor(7.3049, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [71680/111899 (64%)]\tLoss: 7.304856\n",
      "tensor(7.2075, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [76800/111899 (68%)]\tLoss: 7.207534\n",
      "tensor(7.3421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [81920/111899 (73%)]\tLoss: 7.342138\n",
      "tensor(7.2467, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [87040/111899 (78%)]\tLoss: 7.246655\n",
      "tensor(7.3938, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [92160/111899 (82%)]\tLoss: 7.393764\n",
      "tensor(7.2034, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [97280/111899 (87%)]\tLoss: 7.203352\n",
      "tensor(7.3027, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [102400/111899 (91%)]\tLoss: 7.302685\n",
      "tensor(7.3251, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 57 [107520/111899 (96%)]\tLoss: 7.325056\n",
      "\n",
      "Test Epoch: 57\tAttack_Accuracy: 76/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 57\tmaintain_Accuracy: 1706/10593 (16%)\n",
      "\n",
      "tensor(7.2338, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [0/111899 (0%)]\tLoss: 7.233829\n",
      "tensor(7.2915, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [5120/111899 (5%)]\tLoss: 7.291450\n",
      "tensor(7.2971, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [10240/111899 (9%)]\tLoss: 7.297102\n",
      "tensor(7.2220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [15360/111899 (14%)]\tLoss: 7.222047\n",
      "tensor(7.2464, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [20480/111899 (18%)]\tLoss: 7.246397\n",
      "tensor(7.2574, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [25600/111899 (23%)]\tLoss: 7.257446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.1929, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [30720/111899 (27%)]\tLoss: 7.192912\n",
      "tensor(7.3664, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [35840/111899 (32%)]\tLoss: 7.366418\n",
      "tensor(7.2767, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [40960/111899 (37%)]\tLoss: 7.276669\n",
      "tensor(7.2526, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [46080/111899 (41%)]\tLoss: 7.252633\n",
      "tensor(7.2960, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [51200/111899 (46%)]\tLoss: 7.295968\n",
      "tensor(7.1988, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [56320/111899 (50%)]\tLoss: 7.198847\n",
      "tensor(7.3156, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [61440/111899 (55%)]\tLoss: 7.315636\n",
      "tensor(7.1263, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [66560/111899 (59%)]\tLoss: 7.126301\n",
      "tensor(7.3021, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [71680/111899 (64%)]\tLoss: 7.302081\n",
      "tensor(7.2454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [76800/111899 (68%)]\tLoss: 7.245393\n",
      "tensor(7.2516, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [81920/111899 (73%)]\tLoss: 7.251602\n",
      "tensor(7.2657, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [87040/111899 (78%)]\tLoss: 7.265691\n",
      "tensor(7.1564, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [92160/111899 (82%)]\tLoss: 7.156373\n",
      "tensor(7.2682, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [97280/111899 (87%)]\tLoss: 7.268206\n",
      "tensor(7.1650, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [102400/111899 (91%)]\tLoss: 7.164975\n",
      "tensor(7.1830, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 58 [107520/111899 (96%)]\tLoss: 7.182963\n",
      "\n",
      "Test Epoch: 58\tAttack_Accuracy: 57/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 58\tmaintain_Accuracy: 1732/10593 (16%)\n",
      "\n",
      "tensor(7.2295, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [0/111899 (0%)]\tLoss: 7.229472\n",
      "tensor(7.1717, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [5120/111899 (5%)]\tLoss: 7.171675\n",
      "tensor(7.2827, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [10240/111899 (9%)]\tLoss: 7.282703\n",
      "tensor(7.0866, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [15360/111899 (14%)]\tLoss: 7.086632\n",
      "tensor(7.1146, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [20480/111899 (18%)]\tLoss: 7.114622\n",
      "tensor(7.2250, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [25600/111899 (23%)]\tLoss: 7.225013\n",
      "tensor(7.2202, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [30720/111899 (27%)]\tLoss: 7.220231\n",
      "tensor(7.1696, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [35840/111899 (32%)]\tLoss: 7.169564\n",
      "tensor(7.1475, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [40960/111899 (37%)]\tLoss: 7.147524\n",
      "tensor(7.1742, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [46080/111899 (41%)]\tLoss: 7.174167\n",
      "tensor(7.1960, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [51200/111899 (46%)]\tLoss: 7.196033\n",
      "tensor(7.1855, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [56320/111899 (50%)]\tLoss: 7.185515\n",
      "tensor(7.1400, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [61440/111899 (55%)]\tLoss: 7.140004\n",
      "tensor(7.2547, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [66560/111899 (59%)]\tLoss: 7.254668\n",
      "tensor(7.0949, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [71680/111899 (64%)]\tLoss: 7.094946\n",
      "tensor(7.1625, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [76800/111899 (68%)]\tLoss: 7.162497\n",
      "tensor(7.1361, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [81920/111899 (73%)]\tLoss: 7.136053\n",
      "tensor(7.2041, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [87040/111899 (78%)]\tLoss: 7.204085\n",
      "tensor(7.2363, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [92160/111899 (82%)]\tLoss: 7.236333\n",
      "tensor(7.1537, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [97280/111899 (87%)]\tLoss: 7.153684\n",
      "tensor(7.1304, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [102400/111899 (91%)]\tLoss: 7.130422\n",
      "tensor(7.0885, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 59 [107520/111899 (96%)]\tLoss: 7.088520\n",
      "\n",
      "Test Epoch: 59\tAttack_Accuracy: 49/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 59\tmaintain_Accuracy: 1727/10593 (16%)\n",
      "\n",
      "tensor(7.0413, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [0/111899 (0%)]\tLoss: 7.041346\n",
      "tensor(7.2089, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [5120/111899 (5%)]\tLoss: 7.208901\n",
      "tensor(7.1923, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [10240/111899 (9%)]\tLoss: 7.192260\n",
      "tensor(7.0839, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [15360/111899 (14%)]\tLoss: 7.083929\n",
      "tensor(7.1085, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [20480/111899 (18%)]\tLoss: 7.108521\n",
      "tensor(7.1427, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [25600/111899 (23%)]\tLoss: 7.142717\n",
      "tensor(7.1540, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [30720/111899 (27%)]\tLoss: 7.153955\n",
      "tensor(7.1260, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [35840/111899 (32%)]\tLoss: 7.126033\n",
      "tensor(7.1419, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [40960/111899 (37%)]\tLoss: 7.141900\n",
      "tensor(7.1802, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [46080/111899 (41%)]\tLoss: 7.180164\n",
      "tensor(7.0679, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [51200/111899 (46%)]\tLoss: 7.067864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0093, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [56320/111899 (50%)]\tLoss: 7.009300\n",
      "tensor(7.0237, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [61440/111899 (55%)]\tLoss: 7.023674\n",
      "tensor(7.0993, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [66560/111899 (59%)]\tLoss: 7.099319\n",
      "tensor(7.0743, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [71680/111899 (64%)]\tLoss: 7.074327\n",
      "tensor(7.1311, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [76800/111899 (68%)]\tLoss: 7.131082\n",
      "tensor(7.0773, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [81920/111899 (73%)]\tLoss: 7.077312\n",
      "tensor(7.2329, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [87040/111899 (78%)]\tLoss: 7.232865\n",
      "tensor(7.0990, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [92160/111899 (82%)]\tLoss: 7.099023\n",
      "tensor(7.1905, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [97280/111899 (87%)]\tLoss: 7.190525\n",
      "tensor(7.0303, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [102400/111899 (91%)]\tLoss: 7.030275\n",
      "tensor(7.0913, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 60 [107520/111899 (96%)]\tLoss: 7.091274\n",
      "\n",
      "Test Epoch: 60\tAttack_Accuracy: 64/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 60\tmaintain_Accuracy: 1717/10593 (16%)\n",
      "\n",
      "tensor(7.0646, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [0/111899 (0%)]\tLoss: 7.064598\n",
      "tensor(7.0379, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [5120/111899 (5%)]\tLoss: 7.037881\n",
      "tensor(7.1790, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [10240/111899 (9%)]\tLoss: 7.179050\n",
      "tensor(7.1067, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [15360/111899 (14%)]\tLoss: 7.106678\n",
      "tensor(7.1802, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [20480/111899 (18%)]\tLoss: 7.180203\n",
      "tensor(6.9802, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [25600/111899 (23%)]\tLoss: 6.980160\n",
      "tensor(7.1330, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [30720/111899 (27%)]\tLoss: 7.133017\n",
      "tensor(7.0779, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [35840/111899 (32%)]\tLoss: 7.077911\n",
      "tensor(7.1755, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [40960/111899 (37%)]\tLoss: 7.175486\n",
      "tensor(7.1223, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [46080/111899 (41%)]\tLoss: 7.122349\n",
      "tensor(7.0656, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [51200/111899 (46%)]\tLoss: 7.065591\n",
      "tensor(7.0961, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [56320/111899 (50%)]\tLoss: 7.096091\n",
      "tensor(7.0200, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [61440/111899 (55%)]\tLoss: 7.019969\n",
      "tensor(7.0910, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [66560/111899 (59%)]\tLoss: 7.091006\n",
      "tensor(7.0939, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [71680/111899 (64%)]\tLoss: 7.093892\n",
      "tensor(7.0136, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [76800/111899 (68%)]\tLoss: 7.013555\n",
      "tensor(7.1482, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [81920/111899 (73%)]\tLoss: 7.148203\n",
      "tensor(7.0346, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [87040/111899 (78%)]\tLoss: 7.034563\n",
      "tensor(7.1150, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [92160/111899 (82%)]\tLoss: 7.114978\n",
      "tensor(7.0387, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [97280/111899 (87%)]\tLoss: 7.038661\n",
      "tensor(6.9927, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [102400/111899 (91%)]\tLoss: 6.992659\n",
      "tensor(7.1044, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 61 [107520/111899 (96%)]\tLoss: 7.104408\n",
      "\n",
      "Test Epoch: 61\tAttack_Accuracy: 54/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 61\tmaintain_Accuracy: 1773/10593 (17%)\n",
      "\n",
      "tensor(7.0633, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [0/111899 (0%)]\tLoss: 7.063272\n",
      "tensor(7.0181, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [5120/111899 (5%)]\tLoss: 7.018087\n",
      "tensor(7.0757, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [10240/111899 (9%)]\tLoss: 7.075728\n",
      "tensor(7.1289, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [15360/111899 (14%)]\tLoss: 7.128893\n",
      "tensor(7.1043, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [20480/111899 (18%)]\tLoss: 7.104277\n",
      "tensor(7.0325, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [25600/111899 (23%)]\tLoss: 7.032511\n",
      "tensor(7.0044, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [30720/111899 (27%)]\tLoss: 7.004355\n",
      "tensor(7.0662, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [35840/111899 (32%)]\tLoss: 7.066214\n",
      "tensor(6.9755, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [40960/111899 (37%)]\tLoss: 6.975497\n",
      "tensor(7.0499, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [46080/111899 (41%)]\tLoss: 7.049950\n",
      "tensor(7.0503, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [51200/111899 (46%)]\tLoss: 7.050290\n",
      "tensor(7.1067, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [56320/111899 (50%)]\tLoss: 7.106702\n",
      "tensor(7.0780, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [61440/111899 (55%)]\tLoss: 7.077953\n",
      "tensor(7.0734, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [66560/111899 (59%)]\tLoss: 7.073415\n",
      "tensor(7.1356, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [71680/111899 (64%)]\tLoss: 7.135640\n",
      "tensor(7.0377, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [76800/111899 (68%)]\tLoss: 7.037690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.1011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [81920/111899 (73%)]\tLoss: 7.101055\n",
      "tensor(7.0897, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [87040/111899 (78%)]\tLoss: 7.089654\n",
      "tensor(7.0580, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [92160/111899 (82%)]\tLoss: 7.057985\n",
      "tensor(7.0021, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [97280/111899 (87%)]\tLoss: 7.002145\n",
      "tensor(6.9977, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [102400/111899 (91%)]\tLoss: 6.997730\n",
      "tensor(7.0140, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 62 [107520/111899 (96%)]\tLoss: 7.013971\n",
      "\n",
      "Test Epoch: 62\tAttack_Accuracy: 48/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 62\tmaintain_Accuracy: 1786/10593 (17%)\n",
      "\n",
      "tensor(7.0420, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [0/111899 (0%)]\tLoss: 7.041973\n",
      "tensor(7.0652, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [5120/111899 (5%)]\tLoss: 7.065213\n",
      "tensor(7.0516, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [10240/111899 (9%)]\tLoss: 7.051611\n",
      "tensor(7.0544, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [15360/111899 (14%)]\tLoss: 7.054366\n",
      "tensor(7.0999, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [20480/111899 (18%)]\tLoss: 7.099924\n",
      "tensor(7.0670, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [25600/111899 (23%)]\tLoss: 7.066984\n",
      "tensor(6.9861, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [30720/111899 (27%)]\tLoss: 6.986146\n",
      "tensor(7.0495, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [35840/111899 (32%)]\tLoss: 7.049498\n",
      "tensor(6.9886, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [40960/111899 (37%)]\tLoss: 6.988572\n",
      "tensor(7.0089, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [46080/111899 (41%)]\tLoss: 7.008859\n",
      "tensor(6.9303, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [51200/111899 (46%)]\tLoss: 6.930272\n",
      "tensor(7.0230, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [56320/111899 (50%)]\tLoss: 7.022977\n",
      "tensor(7.0215, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [61440/111899 (55%)]\tLoss: 7.021511\n",
      "tensor(7.0650, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [66560/111899 (59%)]\tLoss: 7.065016\n",
      "tensor(7.0333, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [71680/111899 (64%)]\tLoss: 7.033297\n",
      "tensor(7.0177, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [76800/111899 (68%)]\tLoss: 7.017727\n",
      "tensor(7.0587, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [81920/111899 (73%)]\tLoss: 7.058663\n",
      "tensor(7.0037, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [87040/111899 (78%)]\tLoss: 7.003709\n",
      "tensor(6.9646, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [92160/111899 (82%)]\tLoss: 6.964645\n",
      "tensor(6.9781, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [97280/111899 (87%)]\tLoss: 6.978132\n",
      "tensor(6.9232, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [102400/111899 (91%)]\tLoss: 6.923185\n",
      "tensor(7.0035, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 63 [107520/111899 (96%)]\tLoss: 7.003484\n",
      "\n",
      "Test Epoch: 63\tAttack_Accuracy: 60/412 (15%)\n",
      "\n",
      "\n",
      "Test Epoch: 63\tmaintain_Accuracy: 1760/10593 (17%)\n",
      "\n",
      "tensor(6.9762, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [0/111899 (0%)]\tLoss: 6.976172\n",
      "tensor(7.0019, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [5120/111899 (5%)]\tLoss: 7.001907\n",
      "tensor(6.9828, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [10240/111899 (9%)]\tLoss: 6.982780\n",
      "tensor(7.0513, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [15360/111899 (14%)]\tLoss: 7.051283\n",
      "tensor(6.9779, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [20480/111899 (18%)]\tLoss: 6.977896\n",
      "tensor(6.9660, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [25600/111899 (23%)]\tLoss: 6.966002\n",
      "tensor(6.9956, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [30720/111899 (27%)]\tLoss: 6.995621\n",
      "tensor(6.9865, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [35840/111899 (32%)]\tLoss: 6.986484\n",
      "tensor(7.0147, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [40960/111899 (37%)]\tLoss: 7.014745\n",
      "tensor(7.0401, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [46080/111899 (41%)]\tLoss: 7.040079\n",
      "tensor(7.0698, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [51200/111899 (46%)]\tLoss: 7.069754\n",
      "tensor(7.0600, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [56320/111899 (50%)]\tLoss: 7.059950\n",
      "tensor(7.0914, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [61440/111899 (55%)]\tLoss: 7.091408\n",
      "tensor(7.0043, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [66560/111899 (59%)]\tLoss: 7.004313\n",
      "tensor(7.0414, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [71680/111899 (64%)]\tLoss: 7.041362\n",
      "tensor(6.9541, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [76800/111899 (68%)]\tLoss: 6.954103\n",
      "tensor(6.9415, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [81920/111899 (73%)]\tLoss: 6.941525\n",
      "tensor(7.0154, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [87040/111899 (78%)]\tLoss: 7.015353\n",
      "tensor(7.1046, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [92160/111899 (82%)]\tLoss: 7.104617\n",
      "tensor(6.9523, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [97280/111899 (87%)]\tLoss: 6.952278\n",
      "tensor(6.9606, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [102400/111899 (91%)]\tLoss: 6.960567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0203, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 64 [107520/111899 (96%)]\tLoss: 7.020277\n",
      "\n",
      "Test Epoch: 64\tAttack_Accuracy: 51/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 64\tmaintain_Accuracy: 1785/10593 (17%)\n",
      "\n",
      "tensor(6.9694, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [0/111899 (0%)]\tLoss: 6.969357\n",
      "tensor(7.0157, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [5120/111899 (5%)]\tLoss: 7.015739\n",
      "tensor(6.9084, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [10240/111899 (9%)]\tLoss: 6.908375\n",
      "tensor(6.9669, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [15360/111899 (14%)]\tLoss: 6.966941\n",
      "tensor(6.9548, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [20480/111899 (18%)]\tLoss: 6.954757\n",
      "tensor(7.0603, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [25600/111899 (23%)]\tLoss: 7.060255\n",
      "tensor(7.0258, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [30720/111899 (27%)]\tLoss: 7.025791\n",
      "tensor(6.9161, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [35840/111899 (32%)]\tLoss: 6.916143\n",
      "tensor(6.9922, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [40960/111899 (37%)]\tLoss: 6.992245\n",
      "tensor(7.0273, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [46080/111899 (41%)]\tLoss: 7.027331\n",
      "tensor(6.9445, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [51200/111899 (46%)]\tLoss: 6.944457\n",
      "tensor(7.0005, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [56320/111899 (50%)]\tLoss: 7.000535\n",
      "tensor(6.9983, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [61440/111899 (55%)]\tLoss: 6.998299\n",
      "tensor(6.9996, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [66560/111899 (59%)]\tLoss: 6.999631\n",
      "tensor(6.9474, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [71680/111899 (64%)]\tLoss: 6.947407\n",
      "tensor(6.9490, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [76800/111899 (68%)]\tLoss: 6.948951\n",
      "tensor(6.9632, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [81920/111899 (73%)]\tLoss: 6.963154\n",
      "tensor(6.9964, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [87040/111899 (78%)]\tLoss: 6.996449\n",
      "tensor(6.9339, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [92160/111899 (82%)]\tLoss: 6.933870\n",
      "tensor(6.9507, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [97280/111899 (87%)]\tLoss: 6.950672\n",
      "tensor(6.9950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [102400/111899 (91%)]\tLoss: 6.995034\n",
      "tensor(6.9478, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 65 [107520/111899 (96%)]\tLoss: 6.947840\n",
      "\n",
      "Test Epoch: 65\tAttack_Accuracy: 43/412 (10%)\n",
      "\n",
      "\n",
      "Test Epoch: 65\tmaintain_Accuracy: 1825/10593 (17%)\n",
      "\n",
      "tensor(7.0228, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [0/111899 (0%)]\tLoss: 7.022805\n",
      "tensor(7.1024, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [5120/111899 (5%)]\tLoss: 7.102434\n",
      "tensor(7.0270, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [10240/111899 (9%)]\tLoss: 7.027016\n",
      "tensor(6.8736, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [15360/111899 (14%)]\tLoss: 6.873562\n",
      "tensor(6.9551, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [20480/111899 (18%)]\tLoss: 6.955071\n",
      "tensor(6.9344, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [25600/111899 (23%)]\tLoss: 6.934417\n",
      "tensor(6.9526, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [30720/111899 (27%)]\tLoss: 6.952560\n",
      "tensor(6.9663, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [35840/111899 (32%)]\tLoss: 6.966346\n",
      "tensor(6.9423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [40960/111899 (37%)]\tLoss: 6.942315\n",
      "tensor(6.9891, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [46080/111899 (41%)]\tLoss: 6.989062\n",
      "tensor(6.8761, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [51200/111899 (46%)]\tLoss: 6.876082\n",
      "tensor(6.9486, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [56320/111899 (50%)]\tLoss: 6.948641\n",
      "tensor(7.0326, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [61440/111899 (55%)]\tLoss: 7.032639\n",
      "tensor(6.9418, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [66560/111899 (59%)]\tLoss: 6.941817\n",
      "tensor(6.9366, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [71680/111899 (64%)]\tLoss: 6.936596\n",
      "tensor(6.9476, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [76800/111899 (68%)]\tLoss: 6.947605\n",
      "tensor(7.0011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [81920/111899 (73%)]\tLoss: 7.001098\n",
      "tensor(6.9377, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [87040/111899 (78%)]\tLoss: 6.937738\n",
      "tensor(6.9726, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [92160/111899 (82%)]\tLoss: 6.972599\n",
      "tensor(6.9220, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [97280/111899 (87%)]\tLoss: 6.921968\n",
      "tensor(7.0486, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [102400/111899 (91%)]\tLoss: 7.048594\n",
      "tensor(6.9265, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 66 [107520/111899 (96%)]\tLoss: 6.926508\n",
      "\n",
      "Test Epoch: 66\tAttack_Accuracy: 48/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 66\tmaintain_Accuracy: 1821/10593 (17%)\n",
      "\n",
      "tensor(6.9644, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [0/111899 (0%)]\tLoss: 6.964419\n",
      "tensor(6.9991, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [5120/111899 (5%)]\tLoss: 6.999113\n",
      "tensor(6.9588, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [10240/111899 (9%)]\tLoss: 6.958769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9121, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [15360/111899 (14%)]\tLoss: 6.912066\n",
      "tensor(6.9430, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [20480/111899 (18%)]\tLoss: 6.942991\n",
      "tensor(6.9948, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [25600/111899 (23%)]\tLoss: 6.994761\n",
      "tensor(6.9821, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [30720/111899 (27%)]\tLoss: 6.982144\n",
      "tensor(7.0029, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [35840/111899 (32%)]\tLoss: 7.002925\n",
      "tensor(6.9020, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [40960/111899 (37%)]\tLoss: 6.901969\n",
      "tensor(6.8281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [46080/111899 (41%)]\tLoss: 6.828058\n",
      "tensor(6.8476, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [51200/111899 (46%)]\tLoss: 6.847613\n",
      "tensor(6.8867, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [56320/111899 (50%)]\tLoss: 6.886676\n",
      "tensor(6.9042, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [61440/111899 (55%)]\tLoss: 6.904237\n",
      "tensor(6.9007, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [66560/111899 (59%)]\tLoss: 6.900664\n",
      "tensor(6.9615, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [71680/111899 (64%)]\tLoss: 6.961508\n",
      "tensor(7.0064, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [76800/111899 (68%)]\tLoss: 7.006409\n",
      "tensor(6.8827, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [81920/111899 (73%)]\tLoss: 6.882696\n",
      "tensor(6.8613, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [87040/111899 (78%)]\tLoss: 6.861252\n",
      "tensor(7.0236, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [92160/111899 (82%)]\tLoss: 7.023590\n",
      "tensor(6.9348, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [97280/111899 (87%)]\tLoss: 6.934812\n",
      "tensor(6.9152, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [102400/111899 (91%)]\tLoss: 6.915226\n",
      "tensor(6.9019, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 67 [107520/111899 (96%)]\tLoss: 6.901853\n",
      "\n",
      "Test Epoch: 67\tAttack_Accuracy: 52/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 67\tmaintain_Accuracy: 1824/10593 (17%)\n",
      "\n",
      "tensor(6.9012, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [0/111899 (0%)]\tLoss: 6.901173\n",
      "tensor(6.9439, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [5120/111899 (5%)]\tLoss: 6.943851\n",
      "tensor(6.9850, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [10240/111899 (9%)]\tLoss: 6.984984\n",
      "tensor(6.9630, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [15360/111899 (14%)]\tLoss: 6.963018\n",
      "tensor(6.8834, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [20480/111899 (18%)]\tLoss: 6.883360\n",
      "tensor(6.9013, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [25600/111899 (23%)]\tLoss: 6.901344\n",
      "tensor(6.9512, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [30720/111899 (27%)]\tLoss: 6.951193\n",
      "tensor(6.9362, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [35840/111899 (32%)]\tLoss: 6.936227\n",
      "tensor(6.8663, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [40960/111899 (37%)]\tLoss: 6.866263\n",
      "tensor(6.8846, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [46080/111899 (41%)]\tLoss: 6.884623\n",
      "tensor(6.9761, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [51200/111899 (46%)]\tLoss: 6.976093\n",
      "tensor(6.9691, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [56320/111899 (50%)]\tLoss: 6.969141\n",
      "tensor(6.8268, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [61440/111899 (55%)]\tLoss: 6.826806\n",
      "tensor(6.8118, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [66560/111899 (59%)]\tLoss: 6.811806\n",
      "tensor(6.9372, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [71680/111899 (64%)]\tLoss: 6.937199\n",
      "tensor(6.8268, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [76800/111899 (68%)]\tLoss: 6.826822\n",
      "tensor(6.9676, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [81920/111899 (73%)]\tLoss: 6.967645\n",
      "tensor(6.8068, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [87040/111899 (78%)]\tLoss: 6.806770\n",
      "tensor(6.8206, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [92160/111899 (82%)]\tLoss: 6.820570\n",
      "tensor(6.9288, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [97280/111899 (87%)]\tLoss: 6.928809\n",
      "tensor(6.9621, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [102400/111899 (91%)]\tLoss: 6.962066\n",
      "tensor(6.9221, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 68 [107520/111899 (96%)]\tLoss: 6.922079\n",
      "\n",
      "Test Epoch: 68\tAttack_Accuracy: 52/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 68\tmaintain_Accuracy: 1821/10593 (17%)\n",
      "\n",
      "tensor(6.9272, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [0/111899 (0%)]\tLoss: 6.927195\n",
      "tensor(6.8908, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [5120/111899 (5%)]\tLoss: 6.890800\n",
      "tensor(6.9278, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [10240/111899 (9%)]\tLoss: 6.927816\n",
      "tensor(6.9492, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [15360/111899 (14%)]\tLoss: 6.949154\n",
      "tensor(6.8602, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [20480/111899 (18%)]\tLoss: 6.860248\n",
      "tensor(6.9265, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [25600/111899 (23%)]\tLoss: 6.926522\n",
      "tensor(6.8979, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [30720/111899 (27%)]\tLoss: 6.897862\n",
      "tensor(6.9085, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [35840/111899 (32%)]\tLoss: 6.908473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8963, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [40960/111899 (37%)]\tLoss: 6.896311\n",
      "tensor(6.9265, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [46080/111899 (41%)]\tLoss: 6.926550\n",
      "tensor(6.9359, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [51200/111899 (46%)]\tLoss: 6.935869\n",
      "tensor(6.8889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [56320/111899 (50%)]\tLoss: 6.888869\n",
      "tensor(6.8684, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [61440/111899 (55%)]\tLoss: 6.868419\n",
      "tensor(6.8608, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [66560/111899 (59%)]\tLoss: 6.860823\n",
      "tensor(6.8067, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [71680/111899 (64%)]\tLoss: 6.806668\n",
      "tensor(6.8680, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [76800/111899 (68%)]\tLoss: 6.868012\n",
      "tensor(6.8837, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [81920/111899 (73%)]\tLoss: 6.883718\n",
      "tensor(6.8652, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [87040/111899 (78%)]\tLoss: 6.865196\n",
      "tensor(6.8959, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [92160/111899 (82%)]\tLoss: 6.895949\n",
      "tensor(6.8625, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [97280/111899 (87%)]\tLoss: 6.862475\n",
      "tensor(6.8701, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [102400/111899 (91%)]\tLoss: 6.870125\n",
      "tensor(6.8359, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 69 [107520/111899 (96%)]\tLoss: 6.835925\n",
      "\n",
      "Test Epoch: 69\tAttack_Accuracy: 55/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 69\tmaintain_Accuracy: 1855/10593 (18%)\n",
      "\n",
      "tensor(6.9606, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [0/111899 (0%)]\tLoss: 6.960608\n",
      "tensor(6.8945, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [5120/111899 (5%)]\tLoss: 6.894458\n",
      "tensor(6.7717, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [10240/111899 (9%)]\tLoss: 6.771662\n",
      "tensor(6.8331, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [15360/111899 (14%)]\tLoss: 6.833070\n",
      "tensor(6.8927, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [20480/111899 (18%)]\tLoss: 6.892745\n",
      "tensor(6.8443, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [25600/111899 (23%)]\tLoss: 6.844324\n",
      "tensor(6.8352, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [30720/111899 (27%)]\tLoss: 6.835182\n",
      "tensor(6.9567, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [35840/111899 (32%)]\tLoss: 6.956675\n",
      "tensor(6.9657, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [40960/111899 (37%)]\tLoss: 6.965693\n",
      "tensor(6.8529, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [46080/111899 (41%)]\tLoss: 6.852892\n",
      "tensor(6.9119, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [51200/111899 (46%)]\tLoss: 6.911856\n",
      "tensor(6.8153, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [56320/111899 (50%)]\tLoss: 6.815343\n",
      "tensor(6.8767, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [61440/111899 (55%)]\tLoss: 6.876733\n",
      "tensor(6.8251, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [66560/111899 (59%)]\tLoss: 6.825119\n",
      "tensor(6.8909, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [71680/111899 (64%)]\tLoss: 6.890917\n",
      "tensor(6.8376, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [76800/111899 (68%)]\tLoss: 6.837592\n",
      "tensor(6.8657, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [81920/111899 (73%)]\tLoss: 6.865724\n",
      "tensor(6.8596, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [87040/111899 (78%)]\tLoss: 6.859624\n",
      "tensor(6.8316, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [92160/111899 (82%)]\tLoss: 6.831572\n",
      "tensor(6.8125, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [97280/111899 (87%)]\tLoss: 6.812482\n",
      "tensor(6.8325, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [102400/111899 (91%)]\tLoss: 6.832463\n",
      "tensor(6.8778, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 70 [107520/111899 (96%)]\tLoss: 6.877801\n",
      "\n",
      "Test Epoch: 70\tAttack_Accuracy: 48/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 70\tmaintain_Accuracy: 1866/10593 (18%)\n",
      "\n",
      "tensor(6.8744, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [0/111899 (0%)]\tLoss: 6.874431\n",
      "tensor(6.8914, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [5120/111899 (5%)]\tLoss: 6.891362\n",
      "tensor(6.9231, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [10240/111899 (9%)]\tLoss: 6.923111\n",
      "tensor(6.8140, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [15360/111899 (14%)]\tLoss: 6.814027\n",
      "tensor(6.9346, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [20480/111899 (18%)]\tLoss: 6.934619\n",
      "tensor(6.8256, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [25600/111899 (23%)]\tLoss: 6.825589\n",
      "tensor(6.8264, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [30720/111899 (27%)]\tLoss: 6.826374\n",
      "tensor(6.8191, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [35840/111899 (32%)]\tLoss: 6.819129\n",
      "tensor(6.8546, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [40960/111899 (37%)]\tLoss: 6.854622\n",
      "tensor(6.8505, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [46080/111899 (41%)]\tLoss: 6.850491\n",
      "tensor(6.7973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [51200/111899 (46%)]\tLoss: 6.797331\n",
      "tensor(6.8753, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [56320/111899 (50%)]\tLoss: 6.875343\n",
      "tensor(6.8847, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [61440/111899 (55%)]\tLoss: 6.884668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7931, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [66560/111899 (59%)]\tLoss: 6.793068\n",
      "tensor(6.8773, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [71680/111899 (64%)]\tLoss: 6.877313\n",
      "tensor(6.9237, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [76800/111899 (68%)]\tLoss: 6.923674\n",
      "tensor(6.9111, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [81920/111899 (73%)]\tLoss: 6.911105\n",
      "tensor(6.7998, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [87040/111899 (78%)]\tLoss: 6.799807\n",
      "tensor(6.8850, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [92160/111899 (82%)]\tLoss: 6.884961\n",
      "tensor(6.7209, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [97280/111899 (87%)]\tLoss: 6.720869\n",
      "tensor(6.8402, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [102400/111899 (91%)]\tLoss: 6.840206\n",
      "tensor(6.8298, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 71 [107520/111899 (96%)]\tLoss: 6.829785\n",
      "\n",
      "Test Epoch: 71\tAttack_Accuracy: 59/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 71\tmaintain_Accuracy: 1854/10593 (18%)\n",
      "\n",
      "tensor(6.8538, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [0/111899 (0%)]\tLoss: 6.853807\n",
      "tensor(6.8560, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [5120/111899 (5%)]\tLoss: 6.856050\n",
      "tensor(6.8348, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [10240/111899 (9%)]\tLoss: 6.834796\n",
      "tensor(6.7712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [15360/111899 (14%)]\tLoss: 6.771235\n",
      "tensor(6.7814, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [20480/111899 (18%)]\tLoss: 6.781423\n",
      "tensor(6.8979, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [25600/111899 (23%)]\tLoss: 6.897920\n",
      "tensor(6.8067, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [30720/111899 (27%)]\tLoss: 6.806726\n",
      "tensor(6.8732, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [35840/111899 (32%)]\tLoss: 6.873157\n",
      "tensor(6.8309, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [40960/111899 (37%)]\tLoss: 6.830908\n",
      "tensor(6.8127, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [46080/111899 (41%)]\tLoss: 6.812652\n",
      "tensor(6.9175, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [51200/111899 (46%)]\tLoss: 6.917531\n",
      "tensor(6.8382, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [56320/111899 (50%)]\tLoss: 6.838172\n",
      "tensor(6.8283, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [61440/111899 (55%)]\tLoss: 6.828318\n",
      "tensor(6.8242, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [66560/111899 (59%)]\tLoss: 6.824193\n",
      "tensor(6.8491, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [71680/111899 (64%)]\tLoss: 6.849072\n",
      "tensor(6.7713, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [76800/111899 (68%)]\tLoss: 6.771345\n",
      "tensor(6.7384, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [81920/111899 (73%)]\tLoss: 6.738442\n",
      "tensor(6.8309, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [87040/111899 (78%)]\tLoss: 6.830916\n",
      "tensor(6.8087, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [92160/111899 (82%)]\tLoss: 6.808742\n",
      "tensor(6.8022, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [97280/111899 (87%)]\tLoss: 6.802174\n",
      "tensor(6.7544, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [102400/111899 (91%)]\tLoss: 6.754418\n",
      "tensor(6.8712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 72 [107520/111899 (96%)]\tLoss: 6.871169\n",
      "\n",
      "Test Epoch: 72\tAttack_Accuracy: 56/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 72\tmaintain_Accuracy: 1863/10593 (18%)\n",
      "\n",
      "tensor(6.8018, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [0/111899 (0%)]\tLoss: 6.801793\n",
      "tensor(6.8590, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [5120/111899 (5%)]\tLoss: 6.858968\n",
      "tensor(6.8024, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [10240/111899 (9%)]\tLoss: 6.802353\n",
      "tensor(6.8401, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [15360/111899 (14%)]\tLoss: 6.840122\n",
      "tensor(6.8959, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [20480/111899 (18%)]\tLoss: 6.895853\n",
      "tensor(6.7803, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [25600/111899 (23%)]\tLoss: 6.780315\n",
      "tensor(6.8062, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [30720/111899 (27%)]\tLoss: 6.806246\n",
      "tensor(6.7246, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [35840/111899 (32%)]\tLoss: 6.724595\n",
      "tensor(6.8166, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [40960/111899 (37%)]\tLoss: 6.816640\n",
      "tensor(6.7044, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [46080/111899 (41%)]\tLoss: 6.704439\n",
      "tensor(6.8510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [51200/111899 (46%)]\tLoss: 6.851022\n",
      "tensor(6.7524, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [56320/111899 (50%)]\tLoss: 6.752372\n",
      "tensor(6.8244, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [61440/111899 (55%)]\tLoss: 6.824360\n",
      "tensor(6.8435, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [66560/111899 (59%)]\tLoss: 6.843471\n",
      "tensor(6.8205, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [71680/111899 (64%)]\tLoss: 6.820470\n",
      "tensor(6.7345, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [76800/111899 (68%)]\tLoss: 6.734510\n",
      "tensor(6.8264, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [81920/111899 (73%)]\tLoss: 6.826426\n",
      "tensor(6.7876, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [87040/111899 (78%)]\tLoss: 6.787628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8114, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [92160/111899 (82%)]\tLoss: 6.811410\n",
      "tensor(6.7344, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [97280/111899 (87%)]\tLoss: 6.734405\n",
      "tensor(6.7454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [102400/111899 (91%)]\tLoss: 6.745358\n",
      "tensor(6.8059, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 73 [107520/111899 (96%)]\tLoss: 6.805924\n",
      "\n",
      "Test Epoch: 73\tAttack_Accuracy: 58/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 73\tmaintain_Accuracy: 1861/10593 (18%)\n",
      "\n",
      "tensor(6.9411, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [0/111899 (0%)]\tLoss: 6.941070\n",
      "tensor(6.7854, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [5120/111899 (5%)]\tLoss: 6.785371\n",
      "tensor(6.7808, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [10240/111899 (9%)]\tLoss: 6.780828\n",
      "tensor(6.7692, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [15360/111899 (14%)]\tLoss: 6.769240\n",
      "tensor(6.8281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [20480/111899 (18%)]\tLoss: 6.828077\n",
      "tensor(6.7671, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [25600/111899 (23%)]\tLoss: 6.767150\n",
      "tensor(6.7794, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [30720/111899 (27%)]\tLoss: 6.779408\n",
      "tensor(6.7520, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [35840/111899 (32%)]\tLoss: 6.752021\n",
      "tensor(6.7624, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [40960/111899 (37%)]\tLoss: 6.762383\n",
      "tensor(6.8340, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [46080/111899 (41%)]\tLoss: 6.834012\n",
      "tensor(6.8069, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [51200/111899 (46%)]\tLoss: 6.806865\n",
      "tensor(6.7825, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [56320/111899 (50%)]\tLoss: 6.782518\n",
      "tensor(6.8366, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [61440/111899 (55%)]\tLoss: 6.836606\n",
      "tensor(6.8452, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [66560/111899 (59%)]\tLoss: 6.845216\n",
      "tensor(6.6462, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [71680/111899 (64%)]\tLoss: 6.646232\n",
      "tensor(6.9011, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [76800/111899 (68%)]\tLoss: 6.901070\n",
      "tensor(6.7564, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [81920/111899 (73%)]\tLoss: 6.756391\n",
      "tensor(6.6796, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [87040/111899 (78%)]\tLoss: 6.679640\n",
      "tensor(6.8193, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [92160/111899 (82%)]\tLoss: 6.819327\n",
      "tensor(6.7746, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [97280/111899 (87%)]\tLoss: 6.774584\n",
      "tensor(6.7637, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [102400/111899 (91%)]\tLoss: 6.763704\n",
      "tensor(6.7634, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 74 [107520/111899 (96%)]\tLoss: 6.763438\n",
      "\n",
      "Test Epoch: 74\tAttack_Accuracy: 55/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 74\tmaintain_Accuracy: 1854/10593 (18%)\n",
      "\n",
      "tensor(6.7762, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [0/111899 (0%)]\tLoss: 6.776240\n",
      "tensor(6.8718, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [5120/111899 (5%)]\tLoss: 6.871807\n",
      "tensor(6.7493, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [10240/111899 (9%)]\tLoss: 6.749304\n",
      "tensor(6.7957, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [15360/111899 (14%)]\tLoss: 6.795657\n",
      "tensor(6.7392, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [20480/111899 (18%)]\tLoss: 6.739230\n",
      "tensor(6.6806, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [25600/111899 (23%)]\tLoss: 6.680583\n",
      "tensor(6.7865, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [30720/111899 (27%)]\tLoss: 6.786521\n",
      "tensor(6.6741, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [35840/111899 (32%)]\tLoss: 6.674078\n",
      "tensor(6.7918, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [40960/111899 (37%)]\tLoss: 6.791773\n",
      "tensor(6.7541, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [46080/111899 (41%)]\tLoss: 6.754134\n",
      "tensor(6.8144, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [51200/111899 (46%)]\tLoss: 6.814376\n",
      "tensor(6.8206, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [56320/111899 (50%)]\tLoss: 6.820601\n",
      "tensor(6.8692, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [61440/111899 (55%)]\tLoss: 6.869219\n",
      "tensor(6.8031, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [66560/111899 (59%)]\tLoss: 6.803072\n",
      "tensor(6.6902, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [71680/111899 (64%)]\tLoss: 6.690218\n",
      "tensor(6.6546, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [76800/111899 (68%)]\tLoss: 6.654552\n",
      "tensor(6.7476, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [81920/111899 (73%)]\tLoss: 6.747600\n",
      "tensor(6.8240, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [87040/111899 (78%)]\tLoss: 6.823984\n",
      "tensor(6.7216, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [92160/111899 (82%)]\tLoss: 6.721649\n",
      "tensor(6.8147, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [97280/111899 (87%)]\tLoss: 6.814668\n",
      "tensor(6.7539, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [102400/111899 (91%)]\tLoss: 6.753858\n",
      "tensor(6.7735, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 75 [107520/111899 (96%)]\tLoss: 6.773541\n",
      "\n",
      "Test Epoch: 75\tAttack_Accuracy: 55/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 75\tmaintain_Accuracy: 1853/10593 (17%)\n",
      "\n",
      "tensor(6.6936, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [0/111899 (0%)]\tLoss: 6.693638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7583, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [5120/111899 (5%)]\tLoss: 6.758295\n",
      "tensor(6.7800, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [10240/111899 (9%)]\tLoss: 6.780001\n",
      "tensor(6.7561, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [15360/111899 (14%)]\tLoss: 6.756081\n",
      "tensor(6.6712, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [20480/111899 (18%)]\tLoss: 6.671249\n",
      "tensor(6.7624, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [25600/111899 (23%)]\tLoss: 6.762415\n",
      "tensor(6.7246, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [30720/111899 (27%)]\tLoss: 6.724574\n",
      "tensor(6.7717, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [35840/111899 (32%)]\tLoss: 6.771666\n",
      "tensor(6.7504, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [40960/111899 (37%)]\tLoss: 6.750421\n",
      "tensor(6.7111, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [46080/111899 (41%)]\tLoss: 6.711105\n",
      "tensor(6.7363, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [51200/111899 (46%)]\tLoss: 6.736275\n",
      "tensor(6.7602, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [56320/111899 (50%)]\tLoss: 6.760191\n",
      "tensor(6.6889, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [61440/111899 (55%)]\tLoss: 6.688868\n",
      "tensor(6.7092, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [66560/111899 (59%)]\tLoss: 6.709206\n",
      "tensor(6.7421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [71680/111899 (64%)]\tLoss: 6.742081\n",
      "tensor(6.7032, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [76800/111899 (68%)]\tLoss: 6.703156\n",
      "tensor(6.7202, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [81920/111899 (73%)]\tLoss: 6.720250\n",
      "tensor(6.7378, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [87040/111899 (78%)]\tLoss: 6.737782\n",
      "tensor(6.7756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [92160/111899 (82%)]\tLoss: 6.775599\n",
      "tensor(6.7351, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [97280/111899 (87%)]\tLoss: 6.735111\n",
      "tensor(6.7628, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [102400/111899 (91%)]\tLoss: 6.762821\n",
      "tensor(6.7842, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 76 [107520/111899 (96%)]\tLoss: 6.784250\n",
      "\n",
      "Test Epoch: 76\tAttack_Accuracy: 59/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 76\tmaintain_Accuracy: 1864/10593 (18%)\n",
      "\n",
      "tensor(6.6955, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [0/111899 (0%)]\tLoss: 6.695483\n",
      "tensor(6.8773, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [5120/111899 (5%)]\tLoss: 6.877319\n",
      "tensor(6.7447, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [10240/111899 (9%)]\tLoss: 6.744654\n",
      "tensor(6.7021, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [15360/111899 (14%)]\tLoss: 6.702072\n",
      "tensor(6.6437, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [20480/111899 (18%)]\tLoss: 6.643724\n",
      "tensor(6.8169, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [25600/111899 (23%)]\tLoss: 6.816903\n",
      "tensor(6.8305, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [30720/111899 (27%)]\tLoss: 6.830459\n",
      "tensor(6.7869, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [35840/111899 (32%)]\tLoss: 6.786897\n",
      "tensor(6.6861, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [40960/111899 (37%)]\tLoss: 6.686112\n",
      "tensor(6.8323, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [46080/111899 (41%)]\tLoss: 6.832350\n",
      "tensor(6.9016, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [51200/111899 (46%)]\tLoss: 6.901625\n",
      "tensor(6.8058, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [56320/111899 (50%)]\tLoss: 6.805824\n",
      "tensor(6.7370, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [61440/111899 (55%)]\tLoss: 6.737020\n",
      "tensor(6.7448, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [66560/111899 (59%)]\tLoss: 6.744792\n",
      "tensor(6.7249, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [71680/111899 (64%)]\tLoss: 6.724950\n",
      "tensor(6.8030, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [76800/111899 (68%)]\tLoss: 6.803022\n",
      "tensor(6.7049, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [81920/111899 (73%)]\tLoss: 6.704889\n",
      "tensor(6.6349, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [87040/111899 (78%)]\tLoss: 6.634919\n",
      "tensor(6.6605, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [92160/111899 (82%)]\tLoss: 6.660501\n",
      "tensor(6.7494, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [97280/111899 (87%)]\tLoss: 6.749444\n",
      "tensor(6.7726, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [102400/111899 (91%)]\tLoss: 6.772606\n",
      "tensor(6.7191, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 77 [107520/111899 (96%)]\tLoss: 6.719105\n",
      "\n",
      "Test Epoch: 77\tAttack_Accuracy: 63/412 (15%)\n",
      "\n",
      "\n",
      "Test Epoch: 77\tmaintain_Accuracy: 1871/10593 (18%)\n",
      "\n",
      "tensor(6.7276, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [0/111899 (0%)]\tLoss: 6.727630\n",
      "tensor(6.7214, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [5120/111899 (5%)]\tLoss: 6.721443\n",
      "tensor(6.7332, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [10240/111899 (9%)]\tLoss: 6.733204\n",
      "tensor(6.7154, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [15360/111899 (14%)]\tLoss: 6.715430\n",
      "tensor(6.7183, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [20480/111899 (18%)]\tLoss: 6.718295\n",
      "tensor(6.7642, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [25600/111899 (23%)]\tLoss: 6.764178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7035, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [30720/111899 (27%)]\tLoss: 6.703465\n",
      "tensor(6.7536, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [35840/111899 (32%)]\tLoss: 6.753579\n",
      "tensor(6.6922, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [40960/111899 (37%)]\tLoss: 6.692224\n",
      "tensor(6.6961, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [46080/111899 (41%)]\tLoss: 6.696077\n",
      "tensor(6.7086, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [51200/111899 (46%)]\tLoss: 6.708613\n",
      "tensor(6.6823, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [56320/111899 (50%)]\tLoss: 6.682251\n",
      "tensor(6.7700, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [61440/111899 (55%)]\tLoss: 6.770043\n",
      "tensor(6.7539, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [66560/111899 (59%)]\tLoss: 6.753901\n",
      "tensor(6.7401, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [71680/111899 (64%)]\tLoss: 6.740136\n",
      "tensor(6.7288, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [76800/111899 (68%)]\tLoss: 6.728791\n",
      "tensor(6.7461, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [81920/111899 (73%)]\tLoss: 6.746051\n",
      "tensor(6.7461, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [87040/111899 (78%)]\tLoss: 6.746063\n",
      "tensor(6.7009, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [92160/111899 (82%)]\tLoss: 6.700928\n",
      "tensor(6.7673, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [97280/111899 (87%)]\tLoss: 6.767348\n",
      "tensor(6.7191, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [102400/111899 (91%)]\tLoss: 6.719071\n",
      "tensor(6.6767, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 78 [107520/111899 (96%)]\tLoss: 6.676675\n",
      "\n",
      "Test Epoch: 78\tAttack_Accuracy: 56/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 78\tmaintain_Accuracy: 1849/10593 (17%)\n",
      "\n",
      "tensor(6.6921, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [0/111899 (0%)]\tLoss: 6.692062\n",
      "tensor(6.7313, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [5120/111899 (5%)]\tLoss: 6.731285\n",
      "tensor(6.7516, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [10240/111899 (9%)]\tLoss: 6.751567\n",
      "tensor(6.7048, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [15360/111899 (14%)]\tLoss: 6.704753\n",
      "tensor(6.6046, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [20480/111899 (18%)]\tLoss: 6.604644\n",
      "tensor(6.7208, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [25600/111899 (23%)]\tLoss: 6.720804\n",
      "tensor(6.7289, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [30720/111899 (27%)]\tLoss: 6.728946\n",
      "tensor(6.8293, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [35840/111899 (32%)]\tLoss: 6.829309\n",
      "tensor(6.7019, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [40960/111899 (37%)]\tLoss: 6.701862\n",
      "tensor(6.7860, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [46080/111899 (41%)]\tLoss: 6.785964\n",
      "tensor(6.7312, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [51200/111899 (46%)]\tLoss: 6.731181\n",
      "tensor(6.6804, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [56320/111899 (50%)]\tLoss: 6.680402\n",
      "tensor(6.6758, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [61440/111899 (55%)]\tLoss: 6.675837\n",
      "tensor(6.7514, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [66560/111899 (59%)]\tLoss: 6.751379\n",
      "tensor(6.6631, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [71680/111899 (64%)]\tLoss: 6.663116\n",
      "tensor(6.8241, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [76800/111899 (68%)]\tLoss: 6.824050\n",
      "tensor(6.7715, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [81920/111899 (73%)]\tLoss: 6.771513\n",
      "tensor(6.6350, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [87040/111899 (78%)]\tLoss: 6.634980\n",
      "tensor(6.7666, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [92160/111899 (82%)]\tLoss: 6.766635\n",
      "tensor(6.7197, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [97280/111899 (87%)]\tLoss: 6.719703\n",
      "tensor(6.7749, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [102400/111899 (91%)]\tLoss: 6.774943\n",
      "tensor(6.7813, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 79 [107520/111899 (96%)]\tLoss: 6.781307\n",
      "\n",
      "Test Epoch: 79\tAttack_Accuracy: 56/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 79\tmaintain_Accuracy: 1881/10593 (18%)\n",
      "\n",
      "tensor(6.7251, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [0/111899 (0%)]\tLoss: 6.725057\n",
      "tensor(6.7942, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [5120/111899 (5%)]\tLoss: 6.794249\n",
      "tensor(6.7499, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [10240/111899 (9%)]\tLoss: 6.749924\n",
      "tensor(6.7969, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [15360/111899 (14%)]\tLoss: 6.796906\n",
      "tensor(6.6781, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [20480/111899 (18%)]\tLoss: 6.678051\n",
      "tensor(6.7510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [25600/111899 (23%)]\tLoss: 6.750980\n",
      "tensor(6.7177, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [30720/111899 (27%)]\tLoss: 6.717721\n",
      "tensor(6.8046, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [35840/111899 (32%)]\tLoss: 6.804564\n",
      "tensor(6.7151, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [40960/111899 (37%)]\tLoss: 6.715149\n",
      "tensor(6.7417, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [46080/111899 (41%)]\tLoss: 6.741696\n",
      "tensor(6.7921, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [51200/111899 (46%)]\tLoss: 6.792061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [56320/111899 (50%)]\tLoss: 6.745431\n",
      "tensor(6.7229, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [61440/111899 (55%)]\tLoss: 6.722900\n",
      "tensor(6.7685, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [66560/111899 (59%)]\tLoss: 6.768541\n",
      "tensor(6.7848, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [71680/111899 (64%)]\tLoss: 6.784823\n",
      "tensor(6.7500, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [76800/111899 (68%)]\tLoss: 6.749999\n",
      "tensor(6.7436, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [81920/111899 (73%)]\tLoss: 6.743626\n",
      "tensor(6.7264, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [87040/111899 (78%)]\tLoss: 6.726428\n",
      "tensor(6.7107, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [92160/111899 (82%)]\tLoss: 6.710700\n",
      "tensor(6.7140, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [97280/111899 (87%)]\tLoss: 6.714010\n",
      "tensor(6.6742, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [102400/111899 (91%)]\tLoss: 6.674164\n",
      "tensor(6.6245, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 80 [107520/111899 (96%)]\tLoss: 6.624485\n",
      "\n",
      "Test Epoch: 80\tAttack_Accuracy: 54/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 80\tmaintain_Accuracy: 1872/10593 (18%)\n",
      "\n",
      "tensor(6.7104, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [0/111899 (0%)]\tLoss: 6.710366\n",
      "tensor(6.7655, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [5120/111899 (5%)]\tLoss: 6.765510\n",
      "tensor(6.6809, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [10240/111899 (9%)]\tLoss: 6.680899\n",
      "tensor(6.7831, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [15360/111899 (14%)]\tLoss: 6.783068\n",
      "tensor(6.7288, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [20480/111899 (18%)]\tLoss: 6.728849\n",
      "tensor(6.6978, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [25600/111899 (23%)]\tLoss: 6.697795\n",
      "tensor(6.7756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [30720/111899 (27%)]\tLoss: 6.775615\n",
      "tensor(6.6684, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [35840/111899 (32%)]\tLoss: 6.668363\n",
      "tensor(6.7106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [40960/111899 (37%)]\tLoss: 6.710622\n",
      "tensor(6.6910, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [46080/111899 (41%)]\tLoss: 6.690970\n",
      "tensor(6.7228, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [51200/111899 (46%)]\tLoss: 6.722819\n",
      "tensor(6.6632, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [56320/111899 (50%)]\tLoss: 6.663210\n",
      "tensor(6.7061, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [61440/111899 (55%)]\tLoss: 6.706126\n",
      "tensor(6.8075, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [66560/111899 (59%)]\tLoss: 6.807467\n",
      "tensor(6.6808, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [71680/111899 (64%)]\tLoss: 6.680780\n",
      "tensor(6.7679, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [76800/111899 (68%)]\tLoss: 6.767889\n",
      "tensor(6.7136, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [81920/111899 (73%)]\tLoss: 6.713634\n",
      "tensor(6.7332, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [87040/111899 (78%)]\tLoss: 6.733179\n",
      "tensor(6.8122, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [92160/111899 (82%)]\tLoss: 6.812160\n",
      "tensor(6.6997, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [97280/111899 (87%)]\tLoss: 6.699713\n",
      "tensor(6.7141, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [102400/111899 (91%)]\tLoss: 6.714133\n",
      "tensor(6.7336, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 81 [107520/111899 (96%)]\tLoss: 6.733647\n",
      "\n",
      "Test Epoch: 81\tAttack_Accuracy: 73/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 81\tmaintain_Accuracy: 1885/10593 (18%)\n",
      "\n",
      "tensor(6.6452, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [0/111899 (0%)]\tLoss: 6.645165\n",
      "tensor(6.6969, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [5120/111899 (5%)]\tLoss: 6.696929\n",
      "tensor(6.6671, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [10240/111899 (9%)]\tLoss: 6.667054\n",
      "tensor(6.6511, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [15360/111899 (14%)]\tLoss: 6.651063\n",
      "tensor(6.7678, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [20480/111899 (18%)]\tLoss: 6.767752\n",
      "tensor(6.7571, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [25600/111899 (23%)]\tLoss: 6.757132\n",
      "tensor(6.6599, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [30720/111899 (27%)]\tLoss: 6.659941\n",
      "tensor(6.7348, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [35840/111899 (32%)]\tLoss: 6.734827\n",
      "tensor(6.6874, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [40960/111899 (37%)]\tLoss: 6.687449\n",
      "tensor(6.7280, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [46080/111899 (41%)]\tLoss: 6.728011\n",
      "tensor(6.7580, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [51200/111899 (46%)]\tLoss: 6.758038\n",
      "tensor(6.7333, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [56320/111899 (50%)]\tLoss: 6.733316\n",
      "tensor(6.5239, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [61440/111899 (55%)]\tLoss: 6.523942\n",
      "tensor(6.7567, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [66560/111899 (59%)]\tLoss: 6.756670\n",
      "tensor(6.7194, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [71680/111899 (64%)]\tLoss: 6.719399\n",
      "tensor(6.6883, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [76800/111899 (68%)]\tLoss: 6.688289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6381, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [81920/111899 (73%)]\tLoss: 6.638127\n",
      "tensor(6.6839, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [87040/111899 (78%)]\tLoss: 6.683899\n",
      "tensor(6.6626, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [92160/111899 (82%)]\tLoss: 6.662561\n",
      "tensor(6.6959, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [97280/111899 (87%)]\tLoss: 6.695904\n",
      "tensor(6.7278, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [102400/111899 (91%)]\tLoss: 6.727845\n",
      "tensor(6.7348, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 82 [107520/111899 (96%)]\tLoss: 6.734758\n",
      "\n",
      "Test Epoch: 82\tAttack_Accuracy: 65/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 82\tmaintain_Accuracy: 1860/10593 (18%)\n",
      "\n",
      "tensor(6.7385, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [0/111899 (0%)]\tLoss: 6.738476\n",
      "tensor(6.6721, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [5120/111899 (5%)]\tLoss: 6.672056\n",
      "tensor(6.6794, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [10240/111899 (9%)]\tLoss: 6.679422\n",
      "tensor(6.6913, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [15360/111899 (14%)]\tLoss: 6.691281\n",
      "tensor(6.7301, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [20480/111899 (18%)]\tLoss: 6.730121\n",
      "tensor(6.6773, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [25600/111899 (23%)]\tLoss: 6.677281\n",
      "tensor(6.5649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [30720/111899 (27%)]\tLoss: 6.564876\n",
      "tensor(6.6291, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [35840/111899 (32%)]\tLoss: 6.629107\n",
      "tensor(6.6618, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [40960/111899 (37%)]\tLoss: 6.661774\n",
      "tensor(6.7534, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [46080/111899 (41%)]\tLoss: 6.753420\n",
      "tensor(6.7376, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [51200/111899 (46%)]\tLoss: 6.737555\n",
      "tensor(6.6182, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [56320/111899 (50%)]\tLoss: 6.618239\n",
      "tensor(6.6906, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [61440/111899 (55%)]\tLoss: 6.690550\n",
      "tensor(6.6296, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [66560/111899 (59%)]\tLoss: 6.629649\n",
      "tensor(6.7396, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [71680/111899 (64%)]\tLoss: 6.739594\n",
      "tensor(6.6492, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [76800/111899 (68%)]\tLoss: 6.649199\n",
      "tensor(6.6691, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [81920/111899 (73%)]\tLoss: 6.669080\n",
      "tensor(6.7590, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [87040/111899 (78%)]\tLoss: 6.758965\n",
      "tensor(6.6924, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [92160/111899 (82%)]\tLoss: 6.692389\n",
      "tensor(6.6992, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [97280/111899 (87%)]\tLoss: 6.699204\n",
      "tensor(6.7764, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [102400/111899 (91%)]\tLoss: 6.776442\n",
      "tensor(6.7590, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 83 [107520/111899 (96%)]\tLoss: 6.759042\n",
      "\n",
      "Test Epoch: 83\tAttack_Accuracy: 60/412 (15%)\n",
      "\n",
      "\n",
      "Test Epoch: 83\tmaintain_Accuracy: 1866/10593 (18%)\n",
      "\n",
      "tensor(6.6950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [0/111899 (0%)]\tLoss: 6.695038\n",
      "tensor(6.6754, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [5120/111899 (5%)]\tLoss: 6.675375\n",
      "tensor(6.8090, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [10240/111899 (9%)]\tLoss: 6.809020\n",
      "tensor(6.7187, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [15360/111899 (14%)]\tLoss: 6.718691\n",
      "tensor(6.8020, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [20480/111899 (18%)]\tLoss: 6.801996\n",
      "tensor(6.7019, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [25600/111899 (23%)]\tLoss: 6.701918\n",
      "tensor(6.7396, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [30720/111899 (27%)]\tLoss: 6.739593\n",
      "tensor(6.6327, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [35840/111899 (32%)]\tLoss: 6.632701\n",
      "tensor(6.6890, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [40960/111899 (37%)]\tLoss: 6.689015\n",
      "tensor(6.7232, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [46080/111899 (41%)]\tLoss: 6.723175\n",
      "tensor(6.7205, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [51200/111899 (46%)]\tLoss: 6.720526\n",
      "tensor(6.7574, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [56320/111899 (50%)]\tLoss: 6.757401\n",
      "tensor(6.7396, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [61440/111899 (55%)]\tLoss: 6.739647\n",
      "tensor(6.5992, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [66560/111899 (59%)]\tLoss: 6.599223\n",
      "tensor(6.6859, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [71680/111899 (64%)]\tLoss: 6.685858\n",
      "tensor(6.6147, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [76800/111899 (68%)]\tLoss: 6.614696\n",
      "tensor(6.6594, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [81920/111899 (73%)]\tLoss: 6.659392\n",
      "tensor(6.8575, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [87040/111899 (78%)]\tLoss: 6.857457\n",
      "tensor(6.7253, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [92160/111899 (82%)]\tLoss: 6.725285\n",
      "tensor(6.7072, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [97280/111899 (87%)]\tLoss: 6.707179\n",
      "tensor(6.6379, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [102400/111899 (91%)]\tLoss: 6.637940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7270, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 84 [107520/111899 (96%)]\tLoss: 6.727030\n",
      "\n",
      "Test Epoch: 84\tAttack_Accuracy: 66/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 84\tmaintain_Accuracy: 1863/10593 (18%)\n",
      "\n",
      "tensor(6.6776, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [0/111899 (0%)]\tLoss: 6.677598\n",
      "tensor(6.7454, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [5120/111899 (5%)]\tLoss: 6.745450\n",
      "tensor(6.6835, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [10240/111899 (9%)]\tLoss: 6.683528\n",
      "tensor(6.6886, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [15360/111899 (14%)]\tLoss: 6.688574\n",
      "tensor(6.7020, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [20480/111899 (18%)]\tLoss: 6.701972\n",
      "tensor(6.7362, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [25600/111899 (23%)]\tLoss: 6.736166\n",
      "tensor(6.6734, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [30720/111899 (27%)]\tLoss: 6.673371\n",
      "tensor(6.6551, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [35840/111899 (32%)]\tLoss: 6.655056\n",
      "tensor(6.6076, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [40960/111899 (37%)]\tLoss: 6.607628\n",
      "tensor(6.7268, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [46080/111899 (41%)]\tLoss: 6.726793\n",
      "tensor(6.7064, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [51200/111899 (46%)]\tLoss: 6.706366\n",
      "tensor(6.7413, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [56320/111899 (50%)]\tLoss: 6.741324\n",
      "tensor(6.6552, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [61440/111899 (55%)]\tLoss: 6.655190\n",
      "tensor(6.7113, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [66560/111899 (59%)]\tLoss: 6.711312\n",
      "tensor(6.7538, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [71680/111899 (64%)]\tLoss: 6.753794\n",
      "tensor(6.6803, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [76800/111899 (68%)]\tLoss: 6.680318\n",
      "tensor(6.6787, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [81920/111899 (73%)]\tLoss: 6.678690\n",
      "tensor(6.7421, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [87040/111899 (78%)]\tLoss: 6.742071\n",
      "tensor(6.6777, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [92160/111899 (82%)]\tLoss: 6.677696\n",
      "tensor(6.6843, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [97280/111899 (87%)]\tLoss: 6.684293\n",
      "tensor(6.6947, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [102400/111899 (91%)]\tLoss: 6.694742\n",
      "tensor(6.6410, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 85 [107520/111899 (96%)]\tLoss: 6.640992\n",
      "\n",
      "Test Epoch: 85\tAttack_Accuracy: 66/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 85\tmaintain_Accuracy: 1848/10593 (17%)\n",
      "\n",
      "tensor(6.7722, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [0/111899 (0%)]\tLoss: 6.772215\n",
      "tensor(6.7887, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [5120/111899 (5%)]\tLoss: 6.788721\n",
      "tensor(6.6566, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [10240/111899 (9%)]\tLoss: 6.656630\n",
      "tensor(6.6211, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [15360/111899 (14%)]\tLoss: 6.621068\n",
      "tensor(6.6687, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [20480/111899 (18%)]\tLoss: 6.668665\n",
      "tensor(6.7022, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [25600/111899 (23%)]\tLoss: 6.702187\n",
      "tensor(6.7145, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [30720/111899 (27%)]\tLoss: 6.714485\n",
      "tensor(6.6485, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [35840/111899 (32%)]\tLoss: 6.648542\n",
      "tensor(6.6481, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [40960/111899 (37%)]\tLoss: 6.648130\n",
      "tensor(6.6826, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [46080/111899 (41%)]\tLoss: 6.682591\n",
      "tensor(6.6095, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [51200/111899 (46%)]\tLoss: 6.609477\n",
      "tensor(6.6161, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [56320/111899 (50%)]\tLoss: 6.616074\n",
      "tensor(6.7045, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [61440/111899 (55%)]\tLoss: 6.704479\n",
      "tensor(6.6509, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [66560/111899 (59%)]\tLoss: 6.650863\n",
      "tensor(6.6456, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [71680/111899 (64%)]\tLoss: 6.645615\n",
      "tensor(6.6942, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [76800/111899 (68%)]\tLoss: 6.694242\n",
      "tensor(6.6800, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [81920/111899 (73%)]\tLoss: 6.680025\n",
      "tensor(6.6585, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [87040/111899 (78%)]\tLoss: 6.658475\n",
      "tensor(6.6554, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [92160/111899 (82%)]\tLoss: 6.655365\n",
      "tensor(6.6525, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [97280/111899 (87%)]\tLoss: 6.652461\n",
      "tensor(6.7055, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [102400/111899 (91%)]\tLoss: 6.705477\n",
      "tensor(6.6885, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 86 [107520/111899 (96%)]\tLoss: 6.688542\n",
      "\n",
      "Test Epoch: 86\tAttack_Accuracy: 52/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 86\tmaintain_Accuracy: 1874/10593 (18%)\n",
      "\n",
      "tensor(6.6669, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [0/111899 (0%)]\tLoss: 6.666898\n",
      "tensor(6.6812, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [5120/111899 (5%)]\tLoss: 6.681233\n",
      "tensor(6.7111, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [10240/111899 (9%)]\tLoss: 6.711094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6809, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [15360/111899 (14%)]\tLoss: 6.680871\n",
      "tensor(6.6721, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [20480/111899 (18%)]\tLoss: 6.672054\n",
      "tensor(6.6335, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [25600/111899 (23%)]\tLoss: 6.633521\n",
      "tensor(6.7689, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [30720/111899 (27%)]\tLoss: 6.768928\n",
      "tensor(6.7173, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [35840/111899 (32%)]\tLoss: 6.717330\n",
      "tensor(6.6758, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [40960/111899 (37%)]\tLoss: 6.675792\n",
      "tensor(6.6932, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [46080/111899 (41%)]\tLoss: 6.693203\n",
      "tensor(6.7120, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [51200/111899 (46%)]\tLoss: 6.712021\n",
      "tensor(6.6299, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [56320/111899 (50%)]\tLoss: 6.629908\n",
      "tensor(6.6697, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [61440/111899 (55%)]\tLoss: 6.669731\n",
      "tensor(6.7468, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [66560/111899 (59%)]\tLoss: 6.746835\n",
      "tensor(6.5890, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [71680/111899 (64%)]\tLoss: 6.588954\n",
      "tensor(6.6402, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [76800/111899 (68%)]\tLoss: 6.640175\n",
      "tensor(6.7549, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [81920/111899 (73%)]\tLoss: 6.754935\n",
      "tensor(6.7313, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [87040/111899 (78%)]\tLoss: 6.731281\n",
      "tensor(6.5757, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [92160/111899 (82%)]\tLoss: 6.575697\n",
      "tensor(6.7672, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [97280/111899 (87%)]\tLoss: 6.767180\n",
      "tensor(6.6551, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [102400/111899 (91%)]\tLoss: 6.655127\n",
      "tensor(6.7222, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 87 [107520/111899 (96%)]\tLoss: 6.722223\n",
      "\n",
      "Test Epoch: 87\tAttack_Accuracy: 52/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 87\tmaintain_Accuracy: 1850/10593 (17%)\n",
      "\n",
      "tensor(6.6518, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [0/111899 (0%)]\tLoss: 6.651790\n",
      "tensor(6.6334, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [5120/111899 (5%)]\tLoss: 6.633406\n",
      "tensor(6.6416, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [10240/111899 (9%)]\tLoss: 6.641597\n",
      "tensor(6.6465, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [15360/111899 (14%)]\tLoss: 6.646454\n",
      "tensor(6.6490, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [20480/111899 (18%)]\tLoss: 6.648953\n",
      "tensor(6.7951, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [25600/111899 (23%)]\tLoss: 6.795074\n",
      "tensor(6.6369, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [30720/111899 (27%)]\tLoss: 6.636862\n",
      "tensor(6.6298, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [35840/111899 (32%)]\tLoss: 6.629846\n",
      "tensor(6.7524, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [40960/111899 (37%)]\tLoss: 6.752438\n",
      "tensor(6.7213, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [46080/111899 (41%)]\tLoss: 6.721285\n",
      "tensor(6.6032, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [51200/111899 (46%)]\tLoss: 6.603246\n",
      "tensor(6.7133, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [56320/111899 (50%)]\tLoss: 6.713259\n",
      "tensor(6.6730, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [61440/111899 (55%)]\tLoss: 6.673037\n",
      "tensor(6.6441, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [66560/111899 (59%)]\tLoss: 6.644114\n",
      "tensor(6.6241, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [71680/111899 (64%)]\tLoss: 6.624084\n",
      "tensor(6.6451, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [76800/111899 (68%)]\tLoss: 6.645133\n",
      "tensor(6.7470, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [81920/111899 (73%)]\tLoss: 6.746974\n",
      "tensor(6.6569, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [87040/111899 (78%)]\tLoss: 6.656909\n",
      "tensor(6.6917, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [92160/111899 (82%)]\tLoss: 6.691684\n",
      "tensor(6.6676, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [97280/111899 (87%)]\tLoss: 6.667604\n",
      "tensor(6.6595, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [102400/111899 (91%)]\tLoss: 6.659522\n",
      "tensor(6.6800, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 88 [107520/111899 (96%)]\tLoss: 6.680029\n",
      "\n",
      "Test Epoch: 88\tAttack_Accuracy: 49/412 (12%)\n",
      "\n",
      "\n",
      "Test Epoch: 88\tmaintain_Accuracy: 1890/10593 (18%)\n",
      "\n",
      "tensor(6.6430, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [0/111899 (0%)]\tLoss: 6.642956\n",
      "tensor(6.6823, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [5120/111899 (5%)]\tLoss: 6.682306\n",
      "tensor(6.5937, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [10240/111899 (9%)]\tLoss: 6.593653\n",
      "tensor(6.5993, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [15360/111899 (14%)]\tLoss: 6.599342\n",
      "tensor(6.6896, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [20480/111899 (18%)]\tLoss: 6.689610\n",
      "tensor(6.6667, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [25600/111899 (23%)]\tLoss: 6.666667\n",
      "tensor(6.7756, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [30720/111899 (27%)]\tLoss: 6.775623\n",
      "tensor(6.7447, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [35840/111899 (32%)]\tLoss: 6.744683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5942, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [40960/111899 (37%)]\tLoss: 6.594189\n",
      "tensor(6.6082, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [46080/111899 (41%)]\tLoss: 6.608156\n",
      "tensor(6.7244, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [51200/111899 (46%)]\tLoss: 6.724405\n",
      "tensor(6.6765, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [56320/111899 (50%)]\tLoss: 6.676461\n",
      "tensor(6.6747, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [61440/111899 (55%)]\tLoss: 6.674718\n",
      "tensor(6.7416, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [66560/111899 (59%)]\tLoss: 6.741626\n",
      "tensor(6.6782, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [71680/111899 (64%)]\tLoss: 6.678157\n",
      "tensor(6.6625, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [76800/111899 (68%)]\tLoss: 6.662545\n",
      "tensor(6.6533, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [81920/111899 (73%)]\tLoss: 6.653290\n",
      "tensor(6.6649, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [87040/111899 (78%)]\tLoss: 6.664928\n",
      "tensor(6.6184, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [92160/111899 (82%)]\tLoss: 6.618437\n",
      "tensor(6.6355, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [97280/111899 (87%)]\tLoss: 6.635515\n",
      "tensor(6.7115, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [102400/111899 (91%)]\tLoss: 6.711496\n",
      "tensor(6.6749, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 89 [107520/111899 (96%)]\tLoss: 6.674938\n",
      "\n",
      "Test Epoch: 89\tAttack_Accuracy: 53/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 89\tmaintain_Accuracy: 1860/10593 (18%)\n",
      "\n",
      "tensor(6.6666, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [0/111899 (0%)]\tLoss: 6.666564\n",
      "tensor(6.6376, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [5120/111899 (5%)]\tLoss: 6.637551\n",
      "tensor(6.6206, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [10240/111899 (9%)]\tLoss: 6.620618\n",
      "tensor(6.7215, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [15360/111899 (14%)]\tLoss: 6.721484\n",
      "tensor(6.7112, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [20480/111899 (18%)]\tLoss: 6.711218\n",
      "tensor(6.5816, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [25600/111899 (23%)]\tLoss: 6.581561\n",
      "tensor(6.5617, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [30720/111899 (27%)]\tLoss: 6.561706\n",
      "tensor(6.6723, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [35840/111899 (32%)]\tLoss: 6.672318\n",
      "tensor(6.7237, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [40960/111899 (37%)]\tLoss: 6.723706\n",
      "tensor(6.5863, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [46080/111899 (41%)]\tLoss: 6.586269\n",
      "tensor(6.7388, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [51200/111899 (46%)]\tLoss: 6.738835\n",
      "tensor(6.6719, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [56320/111899 (50%)]\tLoss: 6.671850\n",
      "tensor(6.7119, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [61440/111899 (55%)]\tLoss: 6.711928\n",
      "tensor(6.6596, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [66560/111899 (59%)]\tLoss: 6.659570\n",
      "tensor(6.7224, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [71680/111899 (64%)]\tLoss: 6.722370\n",
      "tensor(6.7368, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [76800/111899 (68%)]\tLoss: 6.736773\n",
      "tensor(6.6304, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [81920/111899 (73%)]\tLoss: 6.630388\n",
      "tensor(6.6522, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [87040/111899 (78%)]\tLoss: 6.652175\n",
      "tensor(6.6636, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [92160/111899 (82%)]\tLoss: 6.663629\n",
      "tensor(6.6659, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [97280/111899 (87%)]\tLoss: 6.665902\n",
      "tensor(6.6683, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [102400/111899 (91%)]\tLoss: 6.668258\n",
      "tensor(6.6109, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 90 [107520/111899 (96%)]\tLoss: 6.610938\n",
      "\n",
      "Test Epoch: 90\tAttack_Accuracy: 57/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 90\tmaintain_Accuracy: 1872/10593 (18%)\n",
      "\n",
      "tensor(6.6282, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [0/111899 (0%)]\tLoss: 6.628175\n",
      "tensor(6.6444, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [5120/111899 (5%)]\tLoss: 6.644445\n",
      "tensor(6.6680, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [10240/111899 (9%)]\tLoss: 6.667992\n",
      "tensor(6.5094, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [15360/111899 (14%)]\tLoss: 6.509432\n",
      "tensor(6.6316, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [20480/111899 (18%)]\tLoss: 6.631605\n",
      "tensor(6.5950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [25600/111899 (23%)]\tLoss: 6.594990\n",
      "tensor(6.6690, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [30720/111899 (27%)]\tLoss: 6.668979\n",
      "tensor(6.6033, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [35840/111899 (32%)]\tLoss: 6.603308\n",
      "tensor(6.6293, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [40960/111899 (37%)]\tLoss: 6.629295\n",
      "tensor(6.6118, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [46080/111899 (41%)]\tLoss: 6.611790\n",
      "tensor(6.7034, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [51200/111899 (46%)]\tLoss: 6.703442\n",
      "tensor(6.7107, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [56320/111899 (50%)]\tLoss: 6.710727\n",
      "tensor(6.7071, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [61440/111899 (55%)]\tLoss: 6.707129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5836, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [66560/111899 (59%)]\tLoss: 6.583629\n",
      "tensor(6.7796, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [71680/111899 (64%)]\tLoss: 6.779586\n",
      "tensor(6.6694, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [76800/111899 (68%)]\tLoss: 6.669440\n",
      "tensor(6.6327, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [81920/111899 (73%)]\tLoss: 6.632738\n",
      "tensor(6.6788, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [87040/111899 (78%)]\tLoss: 6.678805\n",
      "tensor(6.7170, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [92160/111899 (82%)]\tLoss: 6.716995\n",
      "tensor(6.6862, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [97280/111899 (87%)]\tLoss: 6.686241\n",
      "tensor(6.6736, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [102400/111899 (91%)]\tLoss: 6.673648\n",
      "tensor(6.6863, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 91 [107520/111899 (96%)]\tLoss: 6.686259\n",
      "\n",
      "Test Epoch: 91\tAttack_Accuracy: 67/412 (16%)\n",
      "\n",
      "\n",
      "Test Epoch: 91\tmaintain_Accuracy: 1874/10593 (18%)\n",
      "\n",
      "tensor(6.6772, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [0/111899 (0%)]\tLoss: 6.677232\n",
      "tensor(6.7077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [5120/111899 (5%)]\tLoss: 6.707735\n",
      "tensor(6.6658, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [10240/111899 (9%)]\tLoss: 6.665831\n",
      "tensor(6.6636, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [15360/111899 (14%)]\tLoss: 6.663603\n",
      "tensor(6.6882, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [20480/111899 (18%)]\tLoss: 6.688248\n",
      "tensor(6.6956, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [25600/111899 (23%)]\tLoss: 6.695575\n",
      "tensor(6.6973, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [30720/111899 (27%)]\tLoss: 6.697294\n",
      "tensor(6.6697, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [35840/111899 (32%)]\tLoss: 6.669703\n",
      "tensor(6.6338, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [40960/111899 (37%)]\tLoss: 6.633835\n",
      "tensor(6.6106, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [46080/111899 (41%)]\tLoss: 6.610560\n",
      "tensor(6.6472, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [51200/111899 (46%)]\tLoss: 6.647182\n",
      "tensor(6.6482, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [56320/111899 (50%)]\tLoss: 6.648159\n",
      "tensor(6.5722, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [61440/111899 (55%)]\tLoss: 6.572166\n",
      "tensor(6.7267, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [66560/111899 (59%)]\tLoss: 6.726657\n",
      "tensor(6.7037, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [71680/111899 (64%)]\tLoss: 6.703684\n",
      "tensor(6.6596, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [76800/111899 (68%)]\tLoss: 6.659637\n",
      "tensor(6.7099, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [81920/111899 (73%)]\tLoss: 6.709856\n",
      "tensor(6.7007, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [87040/111899 (78%)]\tLoss: 6.700720\n",
      "tensor(6.6420, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [92160/111899 (82%)]\tLoss: 6.641959\n",
      "tensor(6.7347, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [97280/111899 (87%)]\tLoss: 6.734741\n",
      "tensor(6.7518, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [102400/111899 (91%)]\tLoss: 6.751823\n",
      "tensor(6.5662, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 92 [107520/111899 (96%)]\tLoss: 6.566231\n",
      "\n",
      "Test Epoch: 92\tAttack_Accuracy: 75/412 (18%)\n",
      "\n",
      "\n",
      "Test Epoch: 92\tmaintain_Accuracy: 1881/10593 (18%)\n",
      "\n",
      "tensor(6.6428, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [0/111899 (0%)]\tLoss: 6.642828\n",
      "tensor(6.5844, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [5120/111899 (5%)]\tLoss: 6.584444\n",
      "tensor(6.6860, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [10240/111899 (9%)]\tLoss: 6.685960\n",
      "tensor(6.6433, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [15360/111899 (14%)]\tLoss: 6.643300\n",
      "tensor(6.6230, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [20480/111899 (18%)]\tLoss: 6.623022\n",
      "tensor(6.7429, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [25600/111899 (23%)]\tLoss: 6.742919\n",
      "tensor(6.5902, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [30720/111899 (27%)]\tLoss: 6.590178\n",
      "tensor(6.7432, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [35840/111899 (32%)]\tLoss: 6.743220\n",
      "tensor(6.6426, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [40960/111899 (37%)]\tLoss: 6.642624\n",
      "tensor(6.6686, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [46080/111899 (41%)]\tLoss: 6.668648\n",
      "tensor(6.5547, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [51200/111899 (46%)]\tLoss: 6.554668\n",
      "tensor(6.6913, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [56320/111899 (50%)]\tLoss: 6.691297\n",
      "tensor(6.6466, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [61440/111899 (55%)]\tLoss: 6.646606\n",
      "tensor(6.6864, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [66560/111899 (59%)]\tLoss: 6.686355\n",
      "tensor(6.6853, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [71680/111899 (64%)]\tLoss: 6.685275\n",
      "tensor(6.6730, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [76800/111899 (68%)]\tLoss: 6.673038\n",
      "tensor(6.7141, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [81920/111899 (73%)]\tLoss: 6.714058\n",
      "tensor(6.7446, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [87040/111899 (78%)]\tLoss: 6.744582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7236, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [92160/111899 (82%)]\tLoss: 6.723570\n",
      "tensor(6.6795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [97280/111899 (87%)]\tLoss: 6.679450\n",
      "tensor(6.7040, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [102400/111899 (91%)]\tLoss: 6.703990\n",
      "tensor(6.6178, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 93 [107520/111899 (96%)]\tLoss: 6.617803\n",
      "\n",
      "Test Epoch: 93\tAttack_Accuracy: 62/412 (15%)\n",
      "\n",
      "\n",
      "Test Epoch: 93\tmaintain_Accuracy: 1881/10593 (18%)\n",
      "\n",
      "tensor(6.7182, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [0/111899 (0%)]\tLoss: 6.718218\n",
      "tensor(6.6936, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [5120/111899 (5%)]\tLoss: 6.693640\n",
      "tensor(6.6718, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [10240/111899 (9%)]\tLoss: 6.671786\n",
      "tensor(6.7237, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [15360/111899 (14%)]\tLoss: 6.723703\n",
      "tensor(6.6885, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [20480/111899 (18%)]\tLoss: 6.688509\n",
      "tensor(6.7354, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [25600/111899 (23%)]\tLoss: 6.735359\n",
      "tensor(6.6318, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [30720/111899 (27%)]\tLoss: 6.631771\n",
      "tensor(6.8831, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [35840/111899 (32%)]\tLoss: 6.883118\n",
      "tensor(6.6592, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [40960/111899 (37%)]\tLoss: 6.659206\n",
      "tensor(6.5884, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [46080/111899 (41%)]\tLoss: 6.588425\n",
      "tensor(6.6636, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [51200/111899 (46%)]\tLoss: 6.663580\n",
      "tensor(6.7160, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [56320/111899 (50%)]\tLoss: 6.716049\n",
      "tensor(6.7141, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [61440/111899 (55%)]\tLoss: 6.714095\n",
      "tensor(6.6948, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [66560/111899 (59%)]\tLoss: 6.694811\n",
      "tensor(6.6358, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [71680/111899 (64%)]\tLoss: 6.635783\n",
      "tensor(6.6975, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [76800/111899 (68%)]\tLoss: 6.697461\n",
      "tensor(6.5888, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [81920/111899 (73%)]\tLoss: 6.588758\n",
      "tensor(6.6705, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [87040/111899 (78%)]\tLoss: 6.670542\n",
      "tensor(6.5906, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [92160/111899 (82%)]\tLoss: 6.590582\n",
      "tensor(6.7026, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [97280/111899 (87%)]\tLoss: 6.702647\n",
      "tensor(6.6119, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [102400/111899 (91%)]\tLoss: 6.611872\n",
      "tensor(6.6397, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 94 [107520/111899 (96%)]\tLoss: 6.639723\n",
      "\n",
      "Test Epoch: 94\tAttack_Accuracy: 69/412 (17%)\n",
      "\n",
      "\n",
      "Test Epoch: 94\tmaintain_Accuracy: 1854/10593 (18%)\n",
      "\n",
      "tensor(6.6164, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [0/111899 (0%)]\tLoss: 6.616432\n",
      "tensor(6.6824, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [5120/111899 (5%)]\tLoss: 6.682430\n",
      "tensor(6.6949, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [10240/111899 (9%)]\tLoss: 6.694884\n",
      "tensor(6.5981, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [15360/111899 (14%)]\tLoss: 6.598076\n",
      "tensor(6.6519, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [20480/111899 (18%)]\tLoss: 6.651851\n",
      "tensor(6.6766, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [25600/111899 (23%)]\tLoss: 6.676555\n",
      "tensor(6.6635, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [30720/111899 (27%)]\tLoss: 6.663499\n",
      "tensor(6.7089, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [35840/111899 (32%)]\tLoss: 6.708914\n",
      "tensor(6.6544, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [40960/111899 (37%)]\tLoss: 6.654402\n",
      "tensor(6.6967, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [46080/111899 (41%)]\tLoss: 6.696680\n",
      "tensor(6.6282, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [51200/111899 (46%)]\tLoss: 6.628241\n",
      "tensor(6.6246, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [56320/111899 (50%)]\tLoss: 6.624563\n",
      "tensor(6.6212, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [61440/111899 (55%)]\tLoss: 6.621166\n",
      "tensor(6.6881, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [66560/111899 (59%)]\tLoss: 6.688139\n",
      "tensor(6.6469, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [71680/111899 (64%)]\tLoss: 6.646872\n",
      "tensor(6.5850, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [76800/111899 (68%)]\tLoss: 6.585027\n",
      "tensor(6.5525, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [81920/111899 (73%)]\tLoss: 6.552473\n",
      "tensor(6.7089, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [87040/111899 (78%)]\tLoss: 6.708899\n",
      "tensor(6.7620, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [92160/111899 (82%)]\tLoss: 6.762029\n",
      "tensor(6.6617, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [97280/111899 (87%)]\tLoss: 6.661727\n",
      "tensor(6.6304, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [102400/111899 (91%)]\tLoss: 6.630401\n",
      "tensor(6.7270, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 95 [107520/111899 (96%)]\tLoss: 6.726978\n",
      "\n",
      "Test Epoch: 95\tAttack_Accuracy: 68/412 (17%)\n",
      "\n",
      "\n",
      "Test Epoch: 95\tmaintain_Accuracy: 1876/10593 (18%)\n",
      "\n",
      "tensor(6.6423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [0/111899 (0%)]\tLoss: 6.642342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5569, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [5120/111899 (5%)]\tLoss: 6.556947\n",
      "tensor(6.6174, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [10240/111899 (9%)]\tLoss: 6.617446\n",
      "tensor(6.5637, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [15360/111899 (14%)]\tLoss: 6.563692\n",
      "tensor(6.7210, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [20480/111899 (18%)]\tLoss: 6.720991\n",
      "tensor(6.6449, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [25600/111899 (23%)]\tLoss: 6.644887\n",
      "tensor(6.6703, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [30720/111899 (27%)]\tLoss: 6.670341\n",
      "tensor(6.5774, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [35840/111899 (32%)]\tLoss: 6.577386\n",
      "tensor(6.6689, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [40960/111899 (37%)]\tLoss: 6.668909\n",
      "tensor(6.6772, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [46080/111899 (41%)]\tLoss: 6.677172\n",
      "tensor(6.6804, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [51200/111899 (46%)]\tLoss: 6.680401\n",
      "tensor(6.6109, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [56320/111899 (50%)]\tLoss: 6.610919\n",
      "tensor(6.7597, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [61440/111899 (55%)]\tLoss: 6.759659\n",
      "tensor(6.6266, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [66560/111899 (59%)]\tLoss: 6.626634\n",
      "tensor(6.6460, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [71680/111899 (64%)]\tLoss: 6.645989\n",
      "tensor(6.6720, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [76800/111899 (68%)]\tLoss: 6.672039\n",
      "tensor(6.6077, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [81920/111899 (73%)]\tLoss: 6.607690\n",
      "tensor(6.6754, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [87040/111899 (78%)]\tLoss: 6.675383\n",
      "tensor(6.6849, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [92160/111899 (82%)]\tLoss: 6.684926\n",
      "tensor(6.6689, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [97280/111899 (87%)]\tLoss: 6.668897\n",
      "tensor(6.6850, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [102400/111899 (91%)]\tLoss: 6.684975\n",
      "tensor(6.6038, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 96 [107520/111899 (96%)]\tLoss: 6.603828\n",
      "\n",
      "Test Epoch: 96\tAttack_Accuracy: 57/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 96\tmaintain_Accuracy: 1878/10593 (18%)\n",
      "\n",
      "tensor(6.7307, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [0/111899 (0%)]\tLoss: 6.730734\n",
      "tensor(6.6524, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [5120/111899 (5%)]\tLoss: 6.652436\n",
      "tensor(6.5891, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [10240/111899 (9%)]\tLoss: 6.589071\n",
      "tensor(6.6414, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [15360/111899 (14%)]\tLoss: 6.641368\n",
      "tensor(6.5865, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [20480/111899 (18%)]\tLoss: 6.586503\n",
      "tensor(6.6447, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [25600/111899 (23%)]\tLoss: 6.644726\n",
      "tensor(6.6836, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [30720/111899 (27%)]\tLoss: 6.683637\n",
      "tensor(6.6376, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [35840/111899 (32%)]\tLoss: 6.637638\n",
      "tensor(6.6782, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [40960/111899 (37%)]\tLoss: 6.678194\n",
      "tensor(6.6384, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [46080/111899 (41%)]\tLoss: 6.638391\n",
      "tensor(6.6331, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [51200/111899 (46%)]\tLoss: 6.633084\n",
      "tensor(6.6434, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [56320/111899 (50%)]\tLoss: 6.643424\n",
      "tensor(6.5880, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [61440/111899 (55%)]\tLoss: 6.588037\n",
      "tensor(6.6337, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [66560/111899 (59%)]\tLoss: 6.633734\n",
      "tensor(6.6989, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [71680/111899 (64%)]\tLoss: 6.698922\n",
      "tensor(6.6005, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [76800/111899 (68%)]\tLoss: 6.600482\n",
      "tensor(6.7007, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [81920/111899 (73%)]\tLoss: 6.700664\n",
      "tensor(6.6184, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [87040/111899 (78%)]\tLoss: 6.618366\n",
      "tensor(6.5808, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [92160/111899 (82%)]\tLoss: 6.580821\n",
      "tensor(6.6746, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [97280/111899 (87%)]\tLoss: 6.674590\n",
      "tensor(6.7525, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [102400/111899 (91%)]\tLoss: 6.752512\n",
      "tensor(6.6308, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 97 [107520/111899 (96%)]\tLoss: 6.630782\n",
      "\n",
      "Test Epoch: 97\tAttack_Accuracy: 60/412 (15%)\n",
      "\n",
      "\n",
      "Test Epoch: 97\tmaintain_Accuracy: 1879/10593 (18%)\n",
      "\n",
      "tensor(6.6907, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [0/111899 (0%)]\tLoss: 6.690688\n",
      "tensor(6.6369, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [5120/111899 (5%)]\tLoss: 6.636923\n",
      "tensor(6.6452, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [10240/111899 (9%)]\tLoss: 6.645208\n",
      "tensor(6.6638, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [15360/111899 (14%)]\tLoss: 6.663823\n",
      "tensor(6.6086, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [20480/111899 (18%)]\tLoss: 6.608574\n",
      "tensor(6.6272, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [25600/111899 (23%)]\tLoss: 6.627243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6538, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [30720/111899 (27%)]\tLoss: 6.653773\n",
      "tensor(6.6645, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [35840/111899 (32%)]\tLoss: 6.664544\n",
      "tensor(6.6698, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [40960/111899 (37%)]\tLoss: 6.669813\n",
      "tensor(6.7300, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [46080/111899 (41%)]\tLoss: 6.730033\n",
      "tensor(6.6163, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [51200/111899 (46%)]\tLoss: 6.616319\n",
      "tensor(6.6310, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [56320/111899 (50%)]\tLoss: 6.630996\n",
      "tensor(6.6927, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [61440/111899 (55%)]\tLoss: 6.692697\n",
      "tensor(6.6792, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [66560/111899 (59%)]\tLoss: 6.679173\n",
      "tensor(6.6551, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [71680/111899 (64%)]\tLoss: 6.655141\n",
      "tensor(6.6603, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [76800/111899 (68%)]\tLoss: 6.660256\n",
      "tensor(6.6121, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [81920/111899 (73%)]\tLoss: 6.612114\n",
      "tensor(6.6170, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [87040/111899 (78%)]\tLoss: 6.616994\n",
      "tensor(6.5777, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [92160/111899 (82%)]\tLoss: 6.577700\n",
      "tensor(6.5839, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [97280/111899 (87%)]\tLoss: 6.583880\n",
      "tensor(6.7423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [102400/111899 (91%)]\tLoss: 6.742338\n",
      "tensor(6.6364, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 98 [107520/111899 (96%)]\tLoss: 6.636393\n",
      "\n",
      "Test Epoch: 98\tAttack_Accuracy: 56/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 98\tmaintain_Accuracy: 1876/10593 (18%)\n",
      "\n",
      "tensor(6.7982, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [0/111899 (0%)]\tLoss: 6.798222\n",
      "tensor(6.6064, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [5120/111899 (5%)]\tLoss: 6.606399\n",
      "tensor(6.5541, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [10240/111899 (9%)]\tLoss: 6.554103\n",
      "tensor(6.7286, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [15360/111899 (14%)]\tLoss: 6.728624\n",
      "tensor(6.6578, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [20480/111899 (18%)]\tLoss: 6.657847\n",
      "tensor(6.6527, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [25600/111899 (23%)]\tLoss: 6.652730\n",
      "tensor(6.6510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [30720/111899 (27%)]\tLoss: 6.651008\n",
      "tensor(6.7051, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [35840/111899 (32%)]\tLoss: 6.705107\n",
      "tensor(6.6237, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [40960/111899 (37%)]\tLoss: 6.623679\n",
      "tensor(6.6490, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [46080/111899 (41%)]\tLoss: 6.648958\n",
      "tensor(6.5920, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [51200/111899 (46%)]\tLoss: 6.591987\n",
      "tensor(6.6513, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [56320/111899 (50%)]\tLoss: 6.651331\n",
      "tensor(6.6149, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [61440/111899 (55%)]\tLoss: 6.614910\n",
      "tensor(6.6126, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [66560/111899 (59%)]\tLoss: 6.612563\n",
      "tensor(6.5829, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [71680/111899 (64%)]\tLoss: 6.582916\n",
      "tensor(6.6570, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [76800/111899 (68%)]\tLoss: 6.657022\n",
      "tensor(6.7008, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [81920/111899 (73%)]\tLoss: 6.700758\n",
      "tensor(6.4911, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [87040/111899 (78%)]\tLoss: 6.491150\n",
      "tensor(6.6189, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [92160/111899 (82%)]\tLoss: 6.618935\n",
      "tensor(6.6844, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [97280/111899 (87%)]\tLoss: 6.684381\n",
      "tensor(6.7088, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [102400/111899 (91%)]\tLoss: 6.708783\n",
      "tensor(6.6839, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 99 [107520/111899 (96%)]\tLoss: 6.683882\n",
      "\n",
      "Test Epoch: 99\tAttack_Accuracy: 57/412 (14%)\n",
      "\n",
      "\n",
      "Test Epoch: 99\tmaintain_Accuracy: 1866/10593 (18%)\n",
      "\n",
      "tensor(6.6017, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [0/111899 (0%)]\tLoss: 6.601724\n",
      "tensor(6.6742, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [5120/111899 (5%)]\tLoss: 6.674181\n",
      "tensor(6.6531, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [10240/111899 (9%)]\tLoss: 6.653101\n",
      "tensor(6.6453, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [15360/111899 (14%)]\tLoss: 6.645336\n",
      "tensor(6.5572, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [20480/111899 (18%)]\tLoss: 6.557155\n",
      "tensor(6.4987, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [25600/111899 (23%)]\tLoss: 6.498749\n",
      "tensor(6.6212, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [30720/111899 (27%)]\tLoss: 6.621192\n",
      "tensor(6.6711, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [35840/111899 (32%)]\tLoss: 6.671114\n",
      "tensor(6.5810, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [40960/111899 (37%)]\tLoss: 6.581045\n",
      "tensor(6.7537, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [46080/111899 (41%)]\tLoss: 6.753720\n",
      "tensor(6.6943, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [51200/111899 (46%)]\tLoss: 6.694347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7391, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [56320/111899 (50%)]\tLoss: 6.739058\n",
      "tensor(6.7040, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [61440/111899 (55%)]\tLoss: 6.704048\n",
      "tensor(6.5942, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [66560/111899 (59%)]\tLoss: 6.594237\n",
      "tensor(6.5950, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [71680/111899 (64%)]\tLoss: 6.594972\n",
      "tensor(6.5997, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [76800/111899 (68%)]\tLoss: 6.599691\n",
      "tensor(6.6375, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [81920/111899 (73%)]\tLoss: 6.637466\n",
      "tensor(6.7344, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [87040/111899 (78%)]\tLoss: 6.734443\n",
      "tensor(6.6860, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [92160/111899 (82%)]\tLoss: 6.685978\n",
      "tensor(6.6738, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [97280/111899 (87%)]\tLoss: 6.673844\n",
      "tensor(6.8003, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [102400/111899 (91%)]\tLoss: 6.800341\n",
      "tensor(6.7098, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Train Epoch: 100 [107520/111899 (96%)]\tLoss: 6.709770\n",
      "\n",
      "Test Epoch: 100\tAttack_Accuracy: 55/412 (13%)\n",
      "\n",
      "\n",
      "Test Epoch: 100\tmaintain_Accuracy: 1872/10593 (18%)\n",
      "\n",
      "tensor([[[-0.0002, -0.0015, -0.0001,  ...,  0.0001,  0.0006,  0.0002]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 100\n",
    "model = torch.load('model.pkl')\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data_size = data.size(1)\n",
    "delta = (torch.rand(1,data_size, 16000)-0.5) * 0.2\n",
    "print(delta)\n",
    "delta = delta.to(device)\n",
    "delta.requires_grad = True\n",
    "optimizer = optim.Adam([delta],lr = 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.25)  # reduce the learning after 20 epochs by a factor of 10\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        delta = train_attack(model, epoch, log_interval, delta)\n",
    "        test_attack(model, epoch, delta)\n",
    "        scheduler.step()\n",
    "        \n",
    "print(delta)\n",
    "plt.plot(delta.squeeze().detach().to('cpu').numpy())\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try with one of your own recordings of one of the labels!\n",
    "For example, using Colab, say “Go” while executing the cell below. This\n",
    "will record one second of audio and try to classify it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "In this tutorial, we used torchaudio to load a dataset and resample the\n",
    "signal. We have then defined a neural network that we trained to\n",
    "recognize a given command. There are also other data preprocessing\n",
    "methods, such as finding the mel frequency cepstral coefficients (MFCC),\n",
    "that can reduce the size of the dataset. This transform is also\n",
    "available in torchaudio as ``torchaudio.transforms.MFCC``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
